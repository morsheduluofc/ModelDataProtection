{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of this program\n",
    "## Read the oversampled data and then project the data in a new dimension. Used the projected data to train a ML model\n",
    "## Verify security of the trained model by test data (project the test data by using different random matrices)\n",
    "## #Test the model accuracy by attack data (user data whose profile are not used to train ML model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052478</td>\n",
       "      <td>0.277631</td>\n",
       "      <td>0.451259</td>\n",
       "      <td>0.718039</td>\n",
       "      <td>0.462785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>0.167556</td>\n",
       "      <td>-0.021220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.175416</td>\n",
       "      <td>0.183196</td>\n",
       "      <td>0.083857</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.183644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052112</td>\n",
       "      <td>0.329417</td>\n",
       "      <td>0.456423</td>\n",
       "      <td>0.707689</td>\n",
       "      <td>0.431129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018824</td>\n",
       "      <td>0.188321</td>\n",
       "      <td>-0.046427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.172549</td>\n",
       "      <td>0.161923</td>\n",
       "      <td>0.174437</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.172166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044798</td>\n",
       "      <td>0.246703</td>\n",
       "      <td>0.482247</td>\n",
       "      <td>0.682013</td>\n",
       "      <td>0.413386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023549</td>\n",
       "      <td>0.219466</td>\n",
       "      <td>-0.084152</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003980</td>\n",
       "      <td>0.007612</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.172549</td>\n",
       "      <td>0.165296</td>\n",
       "      <td>0.176905</td>\n",
       "      <td>0.015824</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.179818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050101</td>\n",
       "      <td>0.292016</td>\n",
       "      <td>0.470626</td>\n",
       "      <td>0.711022</td>\n",
       "      <td>0.418269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.182533</td>\n",
       "      <td>-0.089321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.165296</td>\n",
       "      <td>0.178174</td>\n",
       "      <td>0.036672</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.183644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048638</td>\n",
       "      <td>0.282665</td>\n",
       "      <td>0.438347</td>\n",
       "      <td>0.712347</td>\n",
       "      <td>0.447676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023968</td>\n",
       "      <td>0.151995</td>\n",
       "      <td>-0.024688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>0.007316</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.175416</td>\n",
       "      <td>0.184353</td>\n",
       "      <td>0.061493</td>\n",
       "      <td>0.003786</td>\n",
       "      <td>0.183644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5    6    7         8  \\\n",
       "0  0.052478  0.277631  0.451259  0.718039  0.462785  0.0  0.0  0.018702   \n",
       "1  0.052112  0.329417  0.456423  0.707689  0.431129  0.0  0.0  0.018824   \n",
       "2  0.044798  0.246703  0.482247  0.682013  0.413386  0.0  0.0  0.023549   \n",
       "3  0.050101  0.292016  0.470626  0.711022  0.418269  0.0  0.0  0.019350   \n",
       "4  0.048638  0.282665  0.438347  0.712347  0.447676  0.0  0.0  0.023968   \n",
       "\n",
       "          9        10  ...        25        26        27        28        29  \\\n",
       "0  0.167556 -0.021220  ... -0.000286  0.006156  0.000038  0.098039  0.175416   \n",
       "1  0.188321 -0.046427  ... -0.001681  0.006563  0.000043  0.172549  0.161923   \n",
       "2  0.219466 -0.084152  ... -0.003980  0.007612  0.000058  0.172549  0.165296   \n",
       "3  0.182533 -0.089321  ...  0.000438  0.009006  0.000081  0.149020  0.165296   \n",
       "4  0.151995 -0.024688  ... -0.001410  0.007316  0.000054  0.125490  0.175416   \n",
       "\n",
       "         30        31        32        33  Label  \n",
       "0  0.183196  0.083857  0.007032  0.183644      0  \n",
       "1  0.174437  0.012390  0.000154  0.172166      0  \n",
       "2  0.176905  0.015824  0.000250  0.179818      0  \n",
       "3  0.178174  0.036672  0.001343  0.183644      0  \n",
       "4  0.184353  0.061493  0.003786  0.183644      0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read all data\n",
    "import csv\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv('Dataset/OversampledSwipeData.csv',index_col=0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIeZ6YEDfA-n",
    "outputId": "3f9227e4-2770-4e7f-b28c-3b7936c4e90d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25800, 34)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0E2tEHXVILi",
    "outputId": "14226b39-2e2b-4db7-f225-62d80d4ced4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0     300\n",
       "1     300\n",
       "2     300\n",
       "3     300\n",
       "4     300\n",
       "     ... \n",
       "81    300\n",
       "82    300\n",
       "83    300\n",
       "84    300\n",
       "85    300\n",
       "Name: Label, Length: 86, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace the user ID by class name and count the number of sample in each class\n",
    "dataset['Label'] = pd.factorize(dataset['Label'])[0]\n",
    "dataset.groupby(['Label'])['Label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in training dataset: 68\n",
      "Total user in auxiliary dataset: 18\n"
     ]
    }
   ],
   "source": [
    "#seperate the profile in two groups (80.0%, 20.0%): (i) Training profile (0-155), and (ii) auxiliary profile (156-192)\n",
    "totalUser= len(pd.unique(dataset['Label']))\n",
    "trainingData = dataset[dataset['Label'] <68]\n",
    "auxilaryData = dataset[dataset['Label'] >= 68]\n",
    "print(\"Total user in training dataset:\", len(pd.unique(trainingData['Label'])))\n",
    "print(\"Total user in auxiliary dataset:\", len(pd.unique(auxilaryData['Label'])))\n",
    "#assigned 0-154 users' data to dataset\n",
    "dataset=trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjPJhsxJVILj",
    "outputId": "1e87b85f-bc21-4e99-c3c2-78887612b944"
   },
   "outputs": [],
   "source": [
    "#Prepare the traning data for training and testing the model\n",
    "#import tensorflow\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "#Xtrain, Xtest= train_test_split(trainingData, test_size=0.2, random_state=22)\n",
    "#Xtrain.groupby(['Label'])['Label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in the training dataset: 68\n"
     ]
    }
   ],
   "source": [
    "#total use in the system\n",
    "totalUser= len(pd.unique(dataset['Label']))\n",
    "trainingData=dataset\n",
    "print(\"Total user in the training dataset:\", len(pd.unique(trainingData['Label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XnVsHhXjtjWM",
    "outputId": "46f97e28-a904-48c2-de90-a43a39d75273"
   },
   "outputs": [],
   "source": [
    "#Prepare the traning data for training and testing the model\n",
    "#import tensorflow\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "#Xtrain, Xtest= train_test_split(trainingData, test_size=0.2, random_state=22)\n",
    "#Xtrain.groupby(['Label'])['Label'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EdnsfZolVILk",
    "outputId": "962b2045-7bf2-424a-e495-a305acf5cdd5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_32620\\1304144744.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  trainDatasetRP = pd.concat([trainDatasetRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20400, 34)\n",
      "(20400, 31)\n"
     ]
    }
   ],
   "source": [
    "#random project the training data\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30']\n",
    "trainDatasetRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,68):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = trainingData[trainingData['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=30, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    trainDatasetRP = pd.concat([trainDatasetRP, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(trainingData.shape)\n",
    "print(trainDatasetRP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_h8dq7snVILn"
   },
   "outputs": [],
   "source": [
    "#Prepare group 1 traning data for training and validate the model\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X=trainDatasetRP.drop(columns=['Label'])\n",
    "y=trainDatasetRP['Label']\n",
    "\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size=0.2, random_state=22)\n",
    "\n",
    "#Xtest=testDatasetRP.drop(columns=['Label'])\n",
    "#ytest=testDatasetRP['Label']\n",
    "\n",
    "ytrain = to_categorical(ytrain)\n",
    "yval = to_categorical(yval)\n",
    "#ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdHtViiQVILo",
    "outputId": "a139b9ab-2e8f-4dbc-c00b-4c04de0948f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16320, 30)\n",
      "(16320, 68)\n",
      "(4080, 30)\n",
      "(4080, 68)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(Xval.shape)\n",
    "print(yval.shape)\n",
    "#print(Xtest.shape)\n",
    "#print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nPqBOz3FVILo"
   },
   "outputs": [],
   "source": [
    "# import all necessary package for a neural network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inlineimport keras\n",
    "from keras.layers import Dense, Dropout, Input,Activation,Dropout, Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import mnist\n",
    "#from tqdm import tqdm\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SkLyIb0GVILp"
   },
   "outputs": [],
   "source": [
    "#define optimizers for neural network\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "def adam_optimizer():\n",
    "    return Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "def RMSprop_optimizer():\n",
    "    return RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxWvP-EBVILq",
    "outputId": "312949ee-d986-44c2-f223-2f6bef62f36d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,420</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m1,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m)             │         \u001b[38;5;34m4,420\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,004</span> (93.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,004\u001b[0m (93.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,492</span> (91.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,492\u001b[0m (91.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_classifierRP(release=False, totalClass=68):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(64, input_dim=30))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(Dropout(0.5))\n",
    "\n",
    "    classifier.add(Dense(128))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "\n",
    "    #classifier.add(Dense(256))\n",
    "    #classifier.add(BatchNormalization())\n",
    "    #classifier.add(Activation('relu'))\n",
    "    #classifier.add(Dropout(0.2))\n",
    "\n",
    "    classifier.add(Dense(64))\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Activation('relu'))\n",
    "    classifier.add(Dropout(0.2))\n",
    "\n",
    "    classifier.add(Dense(totalClass, activation='softmax'))\n",
    "\n",
    "\n",
    "    classifier.compile(loss='categorical_crossentropy', optimizer=RMSprop_optimizer(), metrics=['accuracy'])\n",
    "    return classifier\n",
    "\n",
    "Clasf=create_classifierRP()\n",
    "Clasf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2k9bvzvVILr",
    "outputId": "2b079da4-fd5b-417b-e97d-b6008a3e1ee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.2588 - loss: 3.3667 - val_accuracy: 0.9966 - val_loss: 1.3900\n",
      "Epoch 2/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8198 - loss: 1.0737 - val_accuracy: 0.9988 - val_loss: 0.0668\n",
      "Epoch 3/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8927 - loss: 0.4927 - val_accuracy: 0.9988 - val_loss: 0.0112\n",
      "Epoch 4/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9164 - loss: 0.3317 - val_accuracy: 0.9990 - val_loss: 0.0051\n",
      "Epoch 5/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9321 - loss: 0.2569 - val_accuracy: 0.9988 - val_loss: 0.0044\n",
      "Epoch 6/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9352 - loss: 0.2219 - val_accuracy: 0.9988 - val_loss: 0.0041\n",
      "Epoch 7/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9434 - loss: 0.1974 - val_accuracy: 0.9988 - val_loss: 0.0042\n",
      "Epoch 8/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9492 - loss: 0.1806 - val_accuracy: 0.9988 - val_loss: 0.0039\n",
      "Epoch 9/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9478 - loss: 0.1707 - val_accuracy: 0.9990 - val_loss: 0.0033\n",
      "Epoch 10/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9518 - loss: 0.1626 - val_accuracy: 0.9988 - val_loss: 0.0040\n",
      "Epoch 11/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9583 - loss: 0.1417 - val_accuracy: 0.9990 - val_loss: 0.0033\n",
      "Epoch 12/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9573 - loss: 0.1481 - val_accuracy: 0.9988 - val_loss: 0.0038\n",
      "Epoch 13/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9544 - loss: 0.1485 - val_accuracy: 0.9988 - val_loss: 0.0040\n",
      "Epoch 14/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9608 - loss: 0.1367 - val_accuracy: 0.9990 - val_loss: 0.0033\n",
      "Epoch 15/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9596 - loss: 0.1270 - val_accuracy: 0.9990 - val_loss: 0.0033\n",
      "Epoch 16/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9610 - loss: 0.1280 - val_accuracy: 0.9990 - val_loss: 0.0026\n",
      "Epoch 17/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9593 - loss: 0.1353 - val_accuracy: 0.9990 - val_loss: 0.0034\n",
      "Epoch 18/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9634 - loss: 0.1211 - val_accuracy: 0.9988 - val_loss: 0.0034\n",
      "Epoch 19/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9589 - loss: 0.1289 - val_accuracy: 0.9990 - val_loss: 0.0026\n",
      "Epoch 20/20\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9632 - loss: 0.1203 - val_accuracy: 0.9990 - val_loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "#Train the classifier seperately for black-box attack\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
    "# callbacks_list = [learning_rate_reduction]\n",
    "\n",
    "Classfier2= create_classifierRP(True,68)\n",
    "\n",
    "#------Comment will start from here\n",
    "lossc='categorical_crossentropy'\n",
    "optimizerc=RMSprop(learning_rate=0.001, rho=0.9)\n",
    "Classfier2.compile(loss=lossc, optimizer=optimizerc,metrics=['accuracy'])\n",
    "#------Comments will end from here\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "historyc2 =  Classfier2.fit(Xtrain, ytrain, batch_size=64, epochs=20, validation_data=(Xval, yval),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "0pSKQPWhVILt",
    "outputId": "517cf2b4-47b1-49bc-a520-f499c0032cfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epochs')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the classifier loss and accuracy curves for training and validation data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(historyc2.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "plt.plot(historyc2.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test by the data set that come from same user but different random matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test data and seperate test data\n",
    "import csv\n",
    "import pandas as pd\n",
    "testdataset=pd.read_csv('Dataset/SwipeDatatest.csv',index_col=0)\n",
    "testdataset = testdataset[testdataset['Label'] < 68]\n",
    "#testdataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_32620\\570824540.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  testDatasetRP = pd.concat([testDatasetRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1639, 34)\n",
      "(1639, 31)\n"
     ]
    }
   ],
   "source": [
    "#random project the test data\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30']\n",
    "testDatasetRP = pd.DataFrame(columns=column1)\n",
    "\n",
    "for seed in range(0,68):\n",
    "    rng = np.random.RandomState(seed+1)\n",
    "    X = testdataset[testdataset['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=30, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    testDatasetRP = pd.concat([testDatasetRP, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(testdataset.shape)\n",
    "print(testDatasetRP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=testDatasetRP.drop(columns=['Label'])\n",
    "ytest=testDatasetRP['Label']\n",
    "ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4kPqMRQSVILu",
    "outputId": "dd3060ca-d41c-47aa-b98f-7e038599950a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 19.5018  \n",
      "Loss: 19.87772560119629\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Performance of the classifier\n",
    "Classfier2.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "loss, accuracy = Classfier2.evaluate(Xtest, ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test by the attack data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8796</th>\n",
       "      <td>0.013165</td>\n",
       "      <td>0.510668</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>0.775555</td>\n",
       "      <td>0.716926</td>\n",
       "      <td>0.027529</td>\n",
       "      <td>0.024510</td>\n",
       "      <td>0.119330</td>\n",
       "      <td>0.654402</td>\n",
       "      <td>-0.149844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048378</td>\n",
       "      <td>0.018561</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.943739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.97561</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>0.036204</td>\n",
       "      <td>0.510668</td>\n",
       "      <td>0.681730</td>\n",
       "      <td>0.917961</td>\n",
       "      <td>0.661678</td>\n",
       "      <td>0.524333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.307162</td>\n",
       "      <td>-0.126702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>0.026554</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.276953</td>\n",
       "      <td>0.37244</td>\n",
       "      <td>0.138711</td>\n",
       "      <td>0.18747</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>0.051381</td>\n",
       "      <td>0.223365</td>\n",
       "      <td>0.423734</td>\n",
       "      <td>0.417552</td>\n",
       "      <td>0.431169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013251</td>\n",
       "      <td>0.142010</td>\n",
       "      <td>-0.062371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.943739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.97561</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8875</th>\n",
       "      <td>0.016639</td>\n",
       "      <td>0.498441</td>\n",
       "      <td>0.925113</td>\n",
       "      <td>0.880265</td>\n",
       "      <td>0.908020</td>\n",
       "      <td>0.023247</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.066555</td>\n",
       "      <td>0.697454</td>\n",
       "      <td>-0.141167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034539</td>\n",
       "      <td>0.022292</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.943739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.97561</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9294</th>\n",
       "      <td>0.029256</td>\n",
       "      <td>0.412619</td>\n",
       "      <td>0.587617</td>\n",
       "      <td>0.740061</td>\n",
       "      <td>0.507532</td>\n",
       "      <td>0.051660</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.386902</td>\n",
       "      <td>-0.345556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.070412</td>\n",
       "      <td>0.004958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.943739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.97561</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7  \\\n",
       "8796  0.013165  0.510668  0.732085  0.775555  0.716926  0.027529  0.024510   \n",
       "8209  0.036204  0.510668  0.681730  0.917961  0.661678  0.524333  0.333333   \n",
       "8721  0.051381  0.223365  0.423734  0.417552  0.431169  0.000000  0.000000   \n",
       "8875  0.016639  0.498441  0.925113  0.880265  0.908020  0.023247  0.031863   \n",
       "9294  0.029256  0.412619  0.587617  0.740061  0.507532  0.051660  0.039216   \n",
       "\n",
       "             8         9        10  ...        25        26        27  \\\n",
       "8796  0.119330  0.654402 -0.149844  ... -0.048378  0.018561  0.000345   \n",
       "8209 -0.000003  0.307162 -0.126702  ... -0.013927  0.026554  0.000705   \n",
       "8721  0.013251  0.142010 -0.062371  ...  0.002469  0.013271  0.000176   \n",
       "8875  0.066555  0.697454 -0.141167  ... -0.034539  0.022292  0.000497   \n",
       "9294  0.000593  0.386902 -0.345556  ...  0.003056  0.070412  0.004958   \n",
       "\n",
       "            28        29        30       31        32       33  Label  \n",
       "8796  1.000000  0.860215  0.943739  0.00000  0.000000  0.97561     20  \n",
       "8209  0.180392  0.452035  0.276953  0.37244  0.138711  0.18747     28  \n",
       "8721  1.000000  0.860215  0.943739  0.00000  0.000000  0.97561      7  \n",
       "8875  1.000000  0.860215  0.943739  0.00000  0.000000  0.97561      9  \n",
       "9294  1.000000  0.860215  0.943739  0.00000  0.000000  0.97561     27  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invalid test data\n",
    "import csv\n",
    "import pandas as pd\n",
    "testdataset=pd.read_csv('Dataset/SwipeDatatest.csv',index_col=0)\n",
    "testdataset = testdataset[testdataset['Label'] >= 68]\n",
    "newID = np.random.randint(0, 68, size=testdataset.shape[0])\n",
    "testdataset['Label'] = newID\n",
    "testdataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_32620\\570824540.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  testDatasetRP = pd.concat([testDatasetRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425, 34)\n",
      "(425, 31)\n"
     ]
    }
   ],
   "source": [
    "#random project the test data\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30']\n",
    "testDatasetRP = pd.DataFrame(columns=column1)\n",
    "\n",
    "for seed in range(0,68):\n",
    "    rng = np.random.RandomState(seed+1)\n",
    "    X = testdataset[testdataset['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=30, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    testDatasetRP = pd.concat([testDatasetRP, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(testdataset.shape)\n",
    "print(testDatasetRP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=testDatasetRP.drop(columns=['Label'])\n",
    "ytest=testDatasetRP['Label']\n",
    "ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0000e+00 - loss: 24.4437  \n",
      "Loss: 22.515335083007812\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Performance of the classifier\n",
    "Classfier2.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "loss, accuracy = Classfier2.evaluate(Xtest, ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Info: Estimate the verification accuracy of DAC for project data\n",
    "- Used DAC(RP projected) data to train an NN model\n",
    "- There are 193 different user's profiles and each profiles has 1000 data samples (normalized data)\n",
    "- Devide all profiles in two groups: training  profile (96) and auxilary profiles (96) \n",
    "- Each auxilary data semple has 65 different features and RP prjection moved them to 56 features\n",
    "- Random matrix of RP follow following distributions: Pr(x=+1)= 1/2s; Pr(x=-1)= 1/2s, Pr(x=0)= 1-1/s where s=3\n",
    "- The value of dimension reduction k is calculated by k= [(4+2\\beta)/(\\epsolon^2/2+\\epsolon^3/2)]log (n) where n is total sample in a profile and \\epsolon,\\beta>0\n",
    "- Construct a NN regressor has 4 dense layers along with 'BatchNormalization' and 'relu' activation funcation\n",
    "- Last layer is sigmoid function. Input dimension of model is 65 and output dimension 56.\n",
    "- Trained regressor to recover the plain data from the projected data for the 96 auxilary data classes\n",
    "- This traind regressor will be used to recove the training data of classifer.\n",
    "- Let say attacker has the access of RP data of original data and their corresponding label. Attacker can find it by model inversion attack\n",
    "\n",
    "- Included a summary of the NN architecture\n",
    "- Need shallow as RP make users profile more distinct\n",
    "- For 10 rounds of training training accurach reached to 100.0% and validation accuracy reached to 100.0%\n",
    "- Included a graph that shows change of training and validation acccruacy in different ephocs\n",
    "- Test accruacy 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052478</td>\n",
       "      <td>0.277631</td>\n",
       "      <td>0.451259</td>\n",
       "      <td>0.718039</td>\n",
       "      <td>0.462785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>0.167556</td>\n",
       "      <td>-0.021220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.175416</td>\n",
       "      <td>0.183196</td>\n",
       "      <td>0.083857</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.183644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052112</td>\n",
       "      <td>0.329417</td>\n",
       "      <td>0.456423</td>\n",
       "      <td>0.707689</td>\n",
       "      <td>0.431129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018824</td>\n",
       "      <td>0.188321</td>\n",
       "      <td>-0.046427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>0.006563</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.172549</td>\n",
       "      <td>0.161923</td>\n",
       "      <td>0.174437</td>\n",
       "      <td>0.012390</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.172166</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044798</td>\n",
       "      <td>0.246703</td>\n",
       "      <td>0.482247</td>\n",
       "      <td>0.682013</td>\n",
       "      <td>0.413386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023549</td>\n",
       "      <td>0.219466</td>\n",
       "      <td>-0.084152</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003980</td>\n",
       "      <td>0.007612</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.172549</td>\n",
       "      <td>0.165296</td>\n",
       "      <td>0.176905</td>\n",
       "      <td>0.015824</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.179818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050101</td>\n",
       "      <td>0.292016</td>\n",
       "      <td>0.470626</td>\n",
       "      <td>0.711022</td>\n",
       "      <td>0.418269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019350</td>\n",
       "      <td>0.182533</td>\n",
       "      <td>-0.089321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.009006</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.165296</td>\n",
       "      <td>0.178174</td>\n",
       "      <td>0.036672</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.183644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048638</td>\n",
       "      <td>0.282665</td>\n",
       "      <td>0.438347</td>\n",
       "      <td>0.712347</td>\n",
       "      <td>0.447676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023968</td>\n",
       "      <td>0.151995</td>\n",
       "      <td>-0.024688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001410</td>\n",
       "      <td>0.007316</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.175416</td>\n",
       "      <td>0.184353</td>\n",
       "      <td>0.061493</td>\n",
       "      <td>0.003786</td>\n",
       "      <td>0.183644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5    6    7         8  \\\n",
       "0  0.052478  0.277631  0.451259  0.718039  0.462785  0.0  0.0  0.018702   \n",
       "1  0.052112  0.329417  0.456423  0.707689  0.431129  0.0  0.0  0.018824   \n",
       "2  0.044798  0.246703  0.482247  0.682013  0.413386  0.0  0.0  0.023549   \n",
       "3  0.050101  0.292016  0.470626  0.711022  0.418269  0.0  0.0  0.019350   \n",
       "4  0.048638  0.282665  0.438347  0.712347  0.447676  0.0  0.0  0.023968   \n",
       "\n",
       "          9        10  ...        25        26        27        28        29  \\\n",
       "0  0.167556 -0.021220  ... -0.000286  0.006156  0.000038  0.098039  0.175416   \n",
       "1  0.188321 -0.046427  ... -0.001681  0.006563  0.000043  0.172549  0.161923   \n",
       "2  0.219466 -0.084152  ... -0.003980  0.007612  0.000058  0.172549  0.165296   \n",
       "3  0.182533 -0.089321  ...  0.000438  0.009006  0.000081  0.149020  0.165296   \n",
       "4  0.151995 -0.024688  ... -0.001410  0.007316  0.000054  0.125490  0.175416   \n",
       "\n",
       "         30        31        32        33  Label  \n",
       "0  0.183196  0.083857  0.007032  0.183644      0  \n",
       "1  0.174437  0.012390  0.000154  0.172166      0  \n",
       "2  0.176905  0.015824  0.000250  0.179818      0  \n",
       "3  0.178174  0.036672  0.001343  0.183644      0  \n",
       "4  0.184353  0.061493  0.003786  0.183644      0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read all data [194 users' oversampled data]\n",
    "import csv\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv('Dataset/OversampledSwipeData.csv',index_col=0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0     300\n",
       "1     300\n",
       "2     300\n",
       "3     300\n",
       "4     300\n",
       "     ... \n",
       "81    300\n",
       "82    300\n",
       "83    300\n",
       "84    300\n",
       "85    300\n",
       "Name: Label, Length: 86, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace the user ID by class name and count the number of sample in each class\n",
    "#dataset['Label'] = pd.factorize(dataset['Label'])[0]\n",
    "dataset.groupby(['Label'])['Label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in training dataset: 68\n",
      "Total user in auxiliary dataset: 18\n"
     ]
    }
   ],
   "source": [
    "#seperate the profile in two groups: (i) Training profile (0-95), and (ii) auxiliary profile (96-193)\n",
    "totalUser= len(pd.unique(dataset['Label']))\n",
    "trainingData = dataset[dataset['Label'] < 68]\n",
    "attackData = dataset[dataset['Label'] >= 68]\n",
    "print(\"Total user in training dataset:\", len(pd.unique(trainingData['Label'])))\n",
    "print(\"Total user in auxiliary dataset:\", len(pd.unique(attackData['Label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1         1.000000\n",
      "2         1.000000\n",
      "3         0.881859\n",
      "4         1.000000\n",
      "5         1.000000\n",
      "6         0.978824\n",
      "7         0.916667\n",
      "8         0.558563\n",
      "9         0.970466\n",
      "10        0.228430\n",
      "11        1.000000\n",
      "12        1.000000\n",
      "13        0.698851\n",
      "14        1.000000\n",
      "15        0.970992\n",
      "16        1.000000\n",
      "17        0.937500\n",
      "18        0.970820\n",
      "19        1.000000\n",
      "20        1.000000\n",
      "21        1.000000\n",
      "22        1.000000\n",
      "23        0.071859\n",
      "24        1.000000\n",
      "25        0.691649\n",
      "26        1.000000\n",
      "27        1.000000\n",
      "28        1.000000\n",
      "29        1.000000\n",
      "30        1.000000\n",
      "31        1.000000\n",
      "32        1.000000\n",
      "33        1.000000\n",
      "Label    67.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#value range of training data\n",
    "print(trainingData.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When attacker only knows the R. Attacker will train the attack model by the reandom projected attack data that are train by random generated RP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036570</td>\n",
       "      <td>0.481898</td>\n",
       "      <td>0.716591</td>\n",
       "      <td>0.893528</td>\n",
       "      <td>0.675328</td>\n",
       "      <td>0.572000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>8.445437e-07</td>\n",
       "      <td>0.304637</td>\n",
       "      <td>-0.101942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004482</td>\n",
       "      <td>0.012411</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.364326</td>\n",
       "      <td>0.232851</td>\n",
       "      <td>0.227159</td>\n",
       "      <td>0.051601</td>\n",
       "      <td>0.175992</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033278</td>\n",
       "      <td>0.532965</td>\n",
       "      <td>0.658489</td>\n",
       "      <td>0.896320</td>\n",
       "      <td>0.620729</td>\n",
       "      <td>0.458545</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>6.582123e-03</td>\n",
       "      <td>0.248048</td>\n",
       "      <td>-0.241116</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025788</td>\n",
       "      <td>0.029325</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.310352</td>\n",
       "      <td>0.212972</td>\n",
       "      <td>0.169912</td>\n",
       "      <td>0.028870</td>\n",
       "      <td>0.175992</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033278</td>\n",
       "      <td>0.476144</td>\n",
       "      <td>0.686249</td>\n",
       "      <td>0.924243</td>\n",
       "      <td>0.638929</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-1.382825e-06</td>\n",
       "      <td>0.320908</td>\n",
       "      <td>-0.113102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013060</td>\n",
       "      <td>0.016745</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.438541</td>\n",
       "      <td>0.257383</td>\n",
       "      <td>0.338065</td>\n",
       "      <td>0.114288</td>\n",
       "      <td>0.187470</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042238</td>\n",
       "      <td>0.486933</td>\n",
       "      <td>0.700452</td>\n",
       "      <td>0.890038</td>\n",
       "      <td>0.651278</td>\n",
       "      <td>0.479143</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4.734417e-03</td>\n",
       "      <td>0.249816</td>\n",
       "      <td>-0.121032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012044</td>\n",
       "      <td>0.020041</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.293485</td>\n",
       "      <td>0.200379</td>\n",
       "      <td>0.164789</td>\n",
       "      <td>0.027155</td>\n",
       "      <td>0.172166</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.030353</td>\n",
       "      <td>0.522895</td>\n",
       "      <td>0.685604</td>\n",
       "      <td>0.914470</td>\n",
       "      <td>0.684427</td>\n",
       "      <td>0.561600</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.532140e-03</td>\n",
       "      <td>0.340246</td>\n",
       "      <td>-0.046994</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004801</td>\n",
       "      <td>0.016356</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.364326</td>\n",
       "      <td>0.227237</td>\n",
       "      <td>0.248330</td>\n",
       "      <td>0.061668</td>\n",
       "      <td>0.221903</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.036570  0.481898  0.716591  0.893528  0.675328  0.572000  0.416667   \n",
       "1  0.033278  0.532965  0.658489  0.896320  0.620729  0.458545  0.250000   \n",
       "2  0.033278  0.476144  0.686249  0.924243  0.638929  0.520000  0.416667   \n",
       "3  0.042238  0.486933  0.700452  0.890038  0.651278  0.479143  0.250000   \n",
       "4  0.030353  0.522895  0.685604  0.914470  0.684427  0.561600  0.500000   \n",
       "\n",
       "              8         9        10  ...        25        26        27  \\\n",
       "0  8.445437e-07  0.304637 -0.101942  ... -0.004482  0.012411  0.000154   \n",
       "1  6.582123e-03  0.248048 -0.241116  ... -0.025788  0.029325  0.000860   \n",
       "2 -1.382825e-06  0.320908 -0.113102  ... -0.013060  0.016745  0.000280   \n",
       "3  4.734417e-03  0.249816 -0.121032  ... -0.012044  0.020041  0.000402   \n",
       "4  6.532140e-03  0.340246 -0.046994  ... -0.004801  0.016356  0.000268   \n",
       "\n",
       "         28        29        30        31        32        33  Label  \n",
       "0  0.184314  0.364326  0.232851  0.227159  0.051601  0.175992     68  \n",
       "1  0.184314  0.310352  0.212972  0.169912  0.028870  0.175992     68  \n",
       "2  0.176471  0.438541  0.257383  0.338065  0.114288  0.187470     68  \n",
       "3  0.141176  0.293485  0.200379  0.164789  0.027155  0.172166     68  \n",
       "4  0.180392  0.364326  0.227237  0.248330  0.061668  0.221903     68  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#newID = np.random.randint(0, 68, size=attackData.shape[0])\n",
    "#print(newID.shape)\n",
    "#attackData.drop(columns=['Label'])\n",
    "#attackData['Label'] = newID\n",
    "attackData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 34)\n",
      "(5400, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_31520\\4233209104.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  attackDataRP = pd.concat([attackDataRP, XRP], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Random project the attack dataset\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30']\n",
    "attackDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(68,86):\n",
    "    rng = np.random.RandomState(seed-68)\n",
    "    X = attackData[attackData['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=30, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    attackDataRP = pd.concat([attackDataRP, XRP], ignore_index=True)\n",
    "    #print(auxilaryDataRP)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(attackData.shape)\n",
    "print(attackDataRP.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       68.0\n",
      "1       68.0\n",
      "2       68.0\n",
      "3       68.0\n",
      "4       68.0\n",
      "        ... \n",
      "5395    85.0\n",
      "5396    85.0\n",
      "5397    85.0\n",
      "5398    85.0\n",
      "5399    85.0\n",
      "Name: Label, Length: 5400, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#user id in auxilary data\n",
    "print(attackDataRP['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the traning data for training and testing the attacker's model\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Xdata=attackData.drop(columns=['Label'])\n",
    "XRPdata=attackDataRP.drop(columns=['Label'])\n",
    "\n",
    "\n",
    "Xtrain, Xval, XRPtrain, XRPval = train_test_split(Xdata, XRPdata, test_size=0.2, random_state=22)\n",
    "#Xtrain, Xval, XRPtrain, XRPval = train_test_split(Xtrain, XRPtrain, test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4320, 33)\n",
      "(4320, 30)\n",
      "(1080, 33)\n",
      "(1080, 30)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(XRPtrain.shape)\n",
    "#print(Xtest.shape)\n",
    "#print(XRPtest.shape)\n",
    "print(Xval.shape)\n",
    "print(XRPval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary package for a neural network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inlineimport keras\n",
    "from keras.layers import Dense, Dropout, Input,Activation,Dropout, Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import mnist\n",
    "#from tqdm import tqdm\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizers for neural network\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "def adam_optimizer():\n",
    "    return Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "def RMSprop_optimizer():\n",
    "    return RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,968</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m3,968\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)             │         \u001b[38;5;34m4,257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">143,009</span> (558.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m143,009\u001b[0m (558.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">141,473</span> (552.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m141,473\u001b[0m (552.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neural network architecture for training a regressor\n",
    "\n",
    "def create_Regressor(release=False,outDim=33):\n",
    "  classifier = Sequential()\n",
    "  classifier.add(Dense(128, input_dim=30))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "   \n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(256))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "\n",
    "  classifier.add(Dense(256))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "\n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(128))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "\n",
    "  #if release:\n",
    "  classifier.add(Dense(outDim, activation='sigmoid'))\n",
    "  #else:\n",
    "  #   classifier.add(Dense(Tuser))\n",
    "  #np.log_softmax_v2(a, axis=axis)\n",
    "  #classifier.add(F.softmax(a, dim=1))\n",
    "\n",
    "  classifier.compile(loss='mean_squared_error', optimizer='SGD',metrics=['mean_squared_error'])\n",
    "  return classifier\n",
    "\n",
    "Clasf=create_Regressor()\n",
    "Clasf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0816 - mean_squared_error: 0.0816 - val_loss: 0.0846 - val_mean_squared_error: 0.0846\n",
      "Epoch 2/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0304 - val_mean_squared_error: 0.0304\n",
      "Epoch 3/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0122 - val_mean_squared_error: 0.0122\n",
      "Epoch 4/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 5/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0030 - val_mean_squared_error: 0.0030\n",
      "Epoch 6/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0024 - val_mean_squared_error: 0.0024\n",
      "Epoch 7/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 8/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 9/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0019 - val_mean_squared_error: 0.0019\n",
      "Epoch 10/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 11/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 12/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 13/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0018 - val_mean_squared_error: 0.0018\n",
      "Epoch 14/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 15/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 16/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 17/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 18/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 19/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 20/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 21/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0017 - val_mean_squared_error: 0.0017\n",
      "Epoch 22/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 23/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 24/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 25/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 26/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 27/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 28/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 29/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 30/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0019 - mean_squared_error: 0.0019 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 31/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 32/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 33/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 34/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0016 - val_mean_squared_error: 0.0016\n",
      "Epoch 35/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 36/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 37/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 38/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 39/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 40/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 41/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 42/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 43/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 44/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 45/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 46/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0015 - val_mean_squared_error: 0.0015\n",
      "Epoch 47/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 48/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 49/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n",
      "Epoch 50/50\n",
      "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0014 - val_mean_squared_error: 0.0014\n"
     ]
    }
   ],
   "source": [
    "#Train the regressor  by auxilary dataset\n",
    "# Input: Projected data\n",
    "# Output: Plain data\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
    "callbacks_list = [learning_rate_reduction]\n",
    "\n",
    "Regressor= create_Regressor(True,33)\n",
    "\n",
    "#------Comment will start from here\n",
    "lossc='mean_squared_error'\n",
    "optimizerc=RMSprop(learning_rate=0.001, rho=0.9)\n",
    "Regressor.compile(loss=lossc, optimizer=optimizerc,metrics=['mean_squared_error'])\n",
    "#------Comments will end from here\n",
    "Rhistoryc2 =  Regressor.fit(XRPtrain, Xtrain, batch_size=64, epochs=50, validation_data=(XRPval, Xval),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model by pre-seperated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test data and seperate test data\n",
    "import csv\n",
    "import pandas as pd\n",
    "testattackdata=pd.read_csv('Dataset/SwipeDatatest.csv',index_col=0)\n",
    "testattackdata = testattackdata[testattackdata['Label'] >= 68]\n",
    "#testdataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8796</th>\n",
       "      <td>0.013165</td>\n",
       "      <td>0.510668</td>\n",
       "      <td>0.732085</td>\n",
       "      <td>0.775555</td>\n",
       "      <td>0.716926</td>\n",
       "      <td>0.027529</td>\n",
       "      <td>0.024510</td>\n",
       "      <td>0.119330</td>\n",
       "      <td>0.654402</td>\n",
       "      <td>-0.149844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048378</td>\n",
       "      <td>0.018561</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.943739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.97561</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>0.036204</td>\n",
       "      <td>0.510668</td>\n",
       "      <td>0.681730</td>\n",
       "      <td>0.917961</td>\n",
       "      <td>0.661678</td>\n",
       "      <td>0.524333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.307162</td>\n",
       "      <td>-0.126702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>0.026554</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.452035</td>\n",
       "      <td>0.276953</td>\n",
       "      <td>0.37244</td>\n",
       "      <td>0.138711</td>\n",
       "      <td>0.18747</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>0.051381</td>\n",
       "      <td>0.223365</td>\n",
       "      <td>0.423734</td>\n",
       "      <td>0.417552</td>\n",
       "      <td>0.431169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013251</td>\n",
       "      <td>0.142010</td>\n",
       "      <td>-0.062371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.943739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.97561</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8875</th>\n",
       "      <td>0.016639</td>\n",
       "      <td>0.498441</td>\n",
       "      <td>0.925113</td>\n",
       "      <td>0.880265</td>\n",
       "      <td>0.908020</td>\n",
       "      <td>0.023247</td>\n",
       "      <td>0.031863</td>\n",
       "      <td>0.066555</td>\n",
       "      <td>0.697454</td>\n",
       "      <td>-0.141167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034539</td>\n",
       "      <td>0.022292</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.943739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.97561</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9294</th>\n",
       "      <td>0.029256</td>\n",
       "      <td>0.412619</td>\n",
       "      <td>0.587617</td>\n",
       "      <td>0.740061</td>\n",
       "      <td>0.507532</td>\n",
       "      <td>0.051660</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.386902</td>\n",
       "      <td>-0.345556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.070412</td>\n",
       "      <td>0.004958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.943739</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.97561</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7  \\\n",
       "8796  0.013165  0.510668  0.732085  0.775555  0.716926  0.027529  0.024510   \n",
       "8209  0.036204  0.510668  0.681730  0.917961  0.661678  0.524333  0.333333   \n",
       "8721  0.051381  0.223365  0.423734  0.417552  0.431169  0.000000  0.000000   \n",
       "8875  0.016639  0.498441  0.925113  0.880265  0.908020  0.023247  0.031863   \n",
       "9294  0.029256  0.412619  0.587617  0.740061  0.507532  0.051660  0.039216   \n",
       "\n",
       "             8         9        10  ...        25        26        27  \\\n",
       "8796  0.119330  0.654402 -0.149844  ... -0.048378  0.018561  0.000345   \n",
       "8209 -0.000003  0.307162 -0.126702  ... -0.013927  0.026554  0.000705   \n",
       "8721  0.013251  0.142010 -0.062371  ...  0.002469  0.013271  0.000176   \n",
       "8875  0.066555  0.697454 -0.141167  ... -0.034539  0.022292  0.000497   \n",
       "9294  0.000593  0.386902 -0.345556  ...  0.003056  0.070412  0.004958   \n",
       "\n",
       "            28        29        30       31        32       33  Label  \n",
       "8796  1.000000  0.860215  0.943739  0.00000  0.000000  0.97561     73  \n",
       "8209  0.180392  0.452035  0.276953  0.37244  0.138711  0.18747     68  \n",
       "8721  1.000000  0.860215  0.943739  0.00000  0.000000  0.97561     72  \n",
       "8875  1.000000  0.860215  0.943739  0.00000  0.000000  0.97561     73  \n",
       "9294  1.000000  0.860215  0.943739  0.00000  0.000000  0.97561     77  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#newID = np.random.randint(0, 68, size=testattackdata.shape[0])\n",
    "#print(newID.shape)\n",
    "#testattackdata.drop(columns=['Label'])\n",
    "#testattackdata['Label'] = newID\n",
    "testattackdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425, 34)\n",
      "(425, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_31520\\848389873.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  testattackDataRP = pd.concat([testattackDataRP, XRP], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Random project the auxiliary dataset\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30']\n",
    "testattackDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(68,86):\n",
    "    rng = np.random.RandomState(seed-68)\n",
    "    X = testattackdata[testattackdata['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=30, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    testattackDataRP = pd.concat([testattackDataRP, XRP], ignore_index=True)\n",
    "    #print(auxilaryDataRP)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(testattackdata.shape)\n",
    "print(testattackDataRP.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testattackdata=testattackdata.drop(columns=['Label'])\n",
    "testattackDataRP=testattackDataRP.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/14\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0478 - mean_squared_error: 0.0478"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0439 - mean_squared_error: 0.0439 \n",
      "Loss: 0.04309019073843956\n",
      "Accuracy: 0.04309019073843956\n"
     ]
    }
   ],
   "source": [
    "#Performance of the trained attacker regressor\n",
    "loss, accuracy = Regressor.evaluate(testattackDataRP, testattackdata)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let say attacker has the access of Random projected data of the original data profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_31520\\2605144147.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  trainingDataRP = pd.concat([trainingDataRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20400, 34)\n",
      "(20400, 31)\n"
     ]
    }
   ],
   "source": [
    "#Let say attacker has the access of RP data of original data\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30']\n",
    "trainingDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,68):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = trainingData[trainingData['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=30, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    trainingDataRP = pd.concat([trainingDataRP, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(trainingData.shape)\n",
    "print(trainingDataRP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.0\n",
      "1         0.0\n",
      "2         0.0\n",
      "3         0.0\n",
      "4         0.0\n",
      "         ... \n",
      "20395    67.0\n",
      "20396    67.0\n",
      "20397    67.0\n",
      "20398    67.0\n",
      "20399    67.0\n",
      "Name: Label, Length: 20400, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#user id in original projected data\n",
    "print(trainingDataRP['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "(20400, 33)\n"
     ]
    }
   ],
   "source": [
    "#Prediction of plain data by the attacker mdoel assuming that attacker has access of projected data\n",
    "tDataRP=trainingDataRP.drop(columns=['Label'])\n",
    "tDataReg= Regressor.predict(tDataRP)\n",
    "print(tDataReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#assume that along with project data attacker know the label of the data.\n",
    "# Add id with recovered data\n",
    "print(type(tDataReg))\n",
    "print(type(trainingDataRP['Label'].to_numpy()))\n",
    "traningdataReg = pd.concat([pd.DataFrame(tDataReg), trainingDataRP['Label'].to_frame()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20400, 34)\n"
     ]
    }
   ],
   "source": [
    "# recovered data by the attacker model from projected data\n",
    "print(traningdataReg.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test the qulity of recover data we did this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1         2         3         4         5         6         7  \\\n",
      "0      0.045677  0.403419  0.625335  0.762482  0.528411  0.207181  0.132942   \n",
      "1      0.046499  0.401224  0.630379  0.761939  0.502119  0.198773  0.142331   \n",
      "2      0.059392  0.357926  0.657758  0.711460  0.478999  0.173289  0.122857   \n",
      "3      0.054305  0.368306  0.650155  0.725368  0.496583  0.184765  0.137986   \n",
      "4      0.049123  0.400643  0.620673  0.754890  0.515228  0.196526  0.132151   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "20395  0.094651  0.234990  0.349805  0.867724  0.466662  0.053494  0.120624   \n",
      "20396  0.109281  0.257263  0.344495  0.869686  0.474173  0.057778  0.107770   \n",
      "20397  0.099354  0.243360  0.344092  0.876605  0.463380  0.057116  0.106101   \n",
      "20398  0.093797  0.232029  0.323431  0.857107  0.449464  0.047859  0.103073   \n",
      "20399  0.071598  0.201422  0.288529  0.842490  0.442580  0.036445  0.061268   \n",
      "\n",
      "              8         9        10  ...        25        26        27  \\\n",
      "0      0.009684  0.239343  0.002899  ...  0.007936  0.018568  0.006721   \n",
      "1      0.009299  0.235515  0.003261  ...  0.007930  0.015778  0.006101   \n",
      "2      0.010260  0.230202  0.004231  ...  0.009044  0.021905  0.007237   \n",
      "3      0.010121  0.224208  0.003670  ...  0.008483  0.020386  0.006937   \n",
      "4      0.009714  0.224845  0.003116  ...  0.007668  0.017530  0.006383   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "20395  0.039341  0.079997  0.000262  ...  0.020958  0.006545  0.003587   \n",
      "20396  0.037708  0.078326  0.000263  ...  0.020580  0.007718  0.003442   \n",
      "20397  0.042789  0.079574  0.000218  ...  0.019629  0.006800  0.002701   \n",
      "20398  0.036221  0.083755  0.000299  ...  0.017569  0.006751  0.002688   \n",
      "20399  0.033839  0.078665  0.000205  ...  0.015241  0.004965  0.001276   \n",
      "\n",
      "             28        29        30        31        32        33  Label  \n",
      "0      0.108140  0.250713  0.176751  0.081696  0.033592  0.250317    0.0  \n",
      "1      0.106827  0.241593  0.191288  0.075298  0.027418  0.260937    0.0  \n",
      "2      0.133467  0.265728  0.262348  0.066177  0.021287  0.330750    0.0  \n",
      "3      0.121328  0.254272  0.235896  0.071505  0.024028  0.291408    0.0  \n",
      "4      0.110038  0.240483  0.179341  0.078347  0.030834  0.255266    0.0  \n",
      "...         ...       ...       ...       ...       ...       ...    ...  \n",
      "20395  0.698611  0.335346  0.346283  0.051724  0.002778  0.660272   67.0  \n",
      "20396  0.701987  0.361535  0.360515  0.057549  0.003447  0.670865   67.0  \n",
      "20397  0.693939  0.339605  0.316943  0.063312  0.002695  0.673341   67.0  \n",
      "20398  0.621420  0.300602  0.352355  0.058816  0.003936  0.634525   67.0  \n",
      "20399  0.596120  0.216318  0.292847  0.045968  0.003437  0.634591   67.0  \n",
      "\n",
      "[20400 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "traningdataReg.columns=list(trainingData.columns)\n",
    "print(traningdataReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "allPvalue=np.zeros((86,33))\n",
    "for id in range(0,68):\n",
    "    dataset1=traningdataReg[traningdataReg['Label']==id]\n",
    "    dataset2=trainingData[trainingData['Label']==id]\n",
    "    for col in range (0,33):\n",
    "        sample1=dataset1.iloc[:,col]\n",
    "        sample2=dataset2.iloc[:,col]\n",
    "        statistics, allPvalue[id,col]=stats.kstest(sample1, sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.28283733e-007 2.36785122e-111 7.93819639e-164 ... 3.74262317e-028\n",
      "  6.02208108e-155 1.48029788e-179]\n",
      " [2.18575317e-020 7.09312363e-137 5.48367198e-104 ... 1.48029788e-179\n",
      "  1.48029788e-179 1.48029788e-179]\n",
      " [4.07255145e-032 1.48029788e-179 1.48029788e-179 ... 1.92886797e-029\n",
      "  1.32057061e-047 4.34891023e-060]\n",
      " ...\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n"
     ]
    }
   ],
   "source": [
    "print(allPvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "allPvalue = np.where(allPvalue < 0.05, 0, 1)\n",
    "#allPvalue[allPvalue < 0.05] = 0\n",
    "print(allPvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(allPvalue, axis=1))\n",
    "print(len(allPvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAGwCAYAAAC5ACFFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuRUlEQVR4nO3de1RU9f7/8ddwhxRI5BIJ3r4UmLcOJmJWJ+F7qDwlaaUuTniMb34rb4lZUibdKTtW2kW/2UlzpUePlR2zDmao1EnygmZe6WZh6oBKQKECwv790c85zRF3jM44M/p8rLVXzGd/9sx792k1r7X3Zz7bYhiGIQAAALTIx90FAAAAeDLCEgAAgAnCEgAAgAnCEgAAgAnCEgAAgAnCEgAAgAnCEgAAgAk/dxdwLmhubtb+/fvVtm1bWSwWd5cDAABawTAM/fTTT4qNjZWPz6mvHxGWnGD//v2Ki4tzdxkAAOA07N27Vx06dDjlfsKSE7Rt21bSL/+yQ0ND3VwNAABojdraWsXFxdm+x0+FsOQEJ269hYaGEpYAAPAyvzWFhgneAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJghLAAAAJrwuLL388svq1KmTgoKClJKSog0bNpj2X7p0qRITExUUFKQePXrogw8+OGXfu+66SxaLRS+88IKTqwYAAN7Kq8LSkiVLlJubq/z8fG3evFm9evVSRkaGKisrW+y/bt06jRgxQjk5OdqyZYsyMzOVmZmp7du3n9R32bJl+uyzzxQbG+vq0wAAAF7Eq8LSc889pzvvvFOjRo1St27dNGfOHIWEhOj1119vsf/MmTN13XXXafLkyUpKStLjjz+u3/3ud3rppZfs+u3bt0/jxo3TwoUL5e/vfzZOBQAAeAmvCUsNDQ0qLS1Venq6rc3Hx0fp6ekqKSlp8ZiSkhK7/pKUkZFh17+5uVm33367Jk+erMsuu6xVtdTX16u2ttZuAwAA5yavCUuHDh1SU1OToqOj7dqjo6NltVpbPMZqtf5m/2eeeUZ+fn4aP358q2spKChQWFiYbYuLi3PgTAAAgDfxmrDkCqWlpZo5c6bmz58vi8XS6uPy8vJUU1Nj2/bu3evCKgEAgDt5TVhq3769fH19VVFRYddeUVGhmJiYFo+JiYkx7f/JJ5+osrJS8fHx8vPzk5+fn77//ntNmjRJnTp1OmUtgYGBCg0NtdsAAMC5yWvCUkBAgJKTk1VUVGRra25uVlFRkVJTU1s8JjU11a6/JK1atcrW//bbb9cXX3yhzz//3LbFxsZq8uTJWrlypetOBgAAeA0/dxfgiNzcXI0cOVJ9+vRR37599cILL6iurk6jRo2SJGVnZ+viiy9WQUGBJGnChAm65pprNGPGDA0aNEiLFy/Wpk2b9Oqrr0qSIiIiFBERYfcZ/v7+iomJ0aWXXnp2Tw4AAHgkrwpLw4YN08GDBzVt2jRZrVb17t1bhYWFtknc5eXl8vH598Wy/v37a9GiRZo6daoefPBBJSQk6N1331X37t3ddQoAAMDLWAzDMNxdhLerra1VWFiYampqmL8EAICXaO33t9fMWQIAAHAHwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJwhIAAIAJrwtLL7/8sjp16qSgoCClpKRow4YNpv2XLl2qxMREBQUFqUePHvrggw9s+xobG/XAAw+oR48euuCCCxQbG6vs7Gzt37/f1acBAAC8hFeFpSVLlig3N1f5+fnavHmzevXqpYyMDFVWVrbYf926dRoxYoRycnK0ZcsWZWZmKjMzU9u3b5ckHTlyRJs3b9bDDz+szZs365133lFZWZluuumms3laAADAg1kMwzDcXURrpaSk6IorrtBLL70kSWpublZcXJzGjRunKVOmnNR/2LBhqqur04oVK2xt/fr1U+/evTVnzpwWP2Pjxo3q27evvv/+e8XHx7eqrtraWoWFhammpkahoaGncWYAAOBsa+33t9dcWWpoaFBpaanS09NtbT4+PkpPT1dJSUmLx5SUlNj1l6SMjIxT9pekmpoaWSwWhYeHn7JPfX29amtr7TYAAHBu8pqwdOjQITU1NSk6OtquPTo6WlartcVjrFarQ/2PHTumBx54QCNGjDBNmAUFBQoLC7NtcXFxDp4NAADwFl4TllytsbFRt912mwzD0OzZs0375uXlqaamxrbt3bv3LFUJAADONr8zfYPa2lqtXr1al156qZKSkpxRU4vat28vX19fVVRU2LVXVFQoJiamxWNiYmJa1f9EUPr++++1evXq35x3FBgYqMDAwNM4CwAA4G0cvrJ022232SZYHz16VH369NFtt92mnj176u2333Z6gScEBAQoOTlZRUVFtrbm5mYVFRUpNTW1xWNSU1Pt+kvSqlWr7PqfCEpfffWVPvroI0VERLjmBAAAgFdyOCx9/PHHuuqqqyRJy5Ytk2EYqq6u1qxZs/TEE084vcBfy83N1dy5c/XGG29o165duvvuu1VXV6dRo0ZJkrKzs5WXl2frP2HCBBUWFmrGjBnavXu3HnnkEW3atEljx46V9EtQuuWWW7Rp0yYtXLhQTU1NslqtslqtamhocOm5AAAA7+Dwbbiamhq1a9dOklRYWKihQ4cqJCREgwYN0uTJk51e4K8NGzZMBw8e1LRp02S1WtW7d28VFhbaJnGXl5fLx+ff+a9///5atGiRpk6dqgcffFAJCQl699131b17d0nSvn37tHz5cklS79697T5rzZo1+v3vf+/S8wEAAJ7P4XWWLrnkEj3xxBMaNGiQOnfurMWLF2vgwIHaunWr0tLSdOjQIVfV6rFYZwkAAO/T2u9vh68s3XvvvcrKylKbNm0UHx9vu/ry8ccfq0ePHqddMAAAgCdyOCzdc8896tu3r/bu3av//u//tt326tKli8vnLAEAAJxtp/24k4aGBu3Zs0ddu3aVn98Zr0Dg1bgNBwCA93HZ406OHDminJwchYSE6LLLLlN5ebkkady4cXr66adPv2IAAAAP5HBYysvL09atW7V27VoFBQXZ2tPT07VkyRKnFgcAAOBuDt8/e/fdd7VkyRL169dPFovF1n7ZZZfpm2++cWpxAAAA7ubwlaWDBw8qKirqpPa6ujq78AQAAHAucDgs9enTR++//77t9YmA9Nprr53ysSMAAADeyuHbcE899ZSuv/567dy5U8ePH9fMmTO1c+dOrVu3TsXFxa6oEQAAwG0cvrI0YMAAbd26VcePH1ePHj304YcfKioqSiUlJUpOTnZFjQAAAG7j0JWlxsZG/e///q8efvhhzZ0711U1AQAAeAyHriz5+/vr7bffdlUtAAAAHsfh23CZmZl69913XVAKAACA53F4gndCQoIee+wxffrpp0pOTtYFF1xgt3/8+PFOKw4AAMDdHH42XOfOnU/9ZhaLvv322zMuytvwbDgAALxPa7+/Hb6ytGfPnjMqDAAAwJs4PGcJAADgfOLwlaU77rjDdP/rr79+2sUAAAB4GofD0o8//mj3urGxUdu3b1d1dbUGDhzotMIAAAA8gcNhadmyZSe1NTc36+6771bXrl2dUhQAAICncMqcJR8fH+Xm5ur55593xtsBAAB4DKdN8P7mm290/PhxZ70dAACAR3D4Nlxubq7da8MwdODAAb3//vsaOXKk0woDAADwBA6HpS1btti99vHxUWRkpGbMmPGbv5QDAADwNg6HpTVr1riiDgAAAI/k8JylgQMHqrq6+qT22tpalg4AAADnHIfD0tq1a9XQ0HBS+7Fjx/TJJ584pSgAAABP0erbcF988YXt7507d8pqtdpeNzU1qbCwUBdffLFzqwMAAHCzVoel3r17y2KxyGKxtHi7LTg4WC+++KJTiwMAAHC3VoelPXv2yDAMdenSRRs2bFBkZKRtX0BAgKKiouTr6+uSIgEAANyl1WGpY8eOkn55tAkAAMD5wuGlA07YuXOnysvLT5rsfdNNN51xUQAAAJ7C4bD07bff6uabb9a2bdtksVhkGIYkyWKxSPplsjcAAMC5wuGlAyZMmKDOnTursrJSISEh2rFjhz7++GP16dNHa9eudUGJAAAA7uPwlaWSkhKtXr1a7du3l4+Pj3x8fDRgwAAVFBRo/PjxJz0OBQAAwJs5fGWpqalJbdu2lSS1b99e+/fvl/TLBPCysjLnVgcAAOBmDl9Z6t69u7Zu3arOnTsrJSVF06dPV0BAgF599VV16dLFFTUCAAC4jcNhaerUqaqrq5MkPfbYY/rjH/+oq666ShEREVqyZInTCwQAAHAni3Hi52xnoKqqShdeeKHtF3Hnm9raWoWFhammpkahoaHuLgcAALRCa7+/HZ6zdMLXX3+tlStX6ujRo2rXrt3pvg0AAIBHczgsHT58WGlpabrkkkt0ww036MCBA5KknJwcTZo0yekFAgAAuJPDYWnixIny9/dXeXm5QkJCbO3Dhg1TYWGhU4sDAABwN4cneH/44YdauXKlOnToYNeekJCg77//3mmFAQAAeAKHryzV1dXZXVE6oaqqSoGBgU4pCgAAwFM4HJauuuoqLViwwPbaYrGoublZ06dP17XXXuvU4gAAANzN4dtw06dPV1pamjZt2qSGhgbdf//92rFjh6qqqvTpp5+6okYAAAC3cfjKUvfu3fXll19qwIABGjx4sOrq6jRkyBBt2bJFXbt2dUWNAAAAbtOqK0tDhgzR/PnzFRoaqgULFmjYsGF66KGHXF0bAACA27XqytKKFStsjzgZNWqUampqXFoUAACAp2jVlaXExETl5eXp2muvlWEY+vvf/37KZcGzs7OdWiAAAIA7terZcOvWrVNubq6++eYbVVVVqW3bti0+B85isaiqqsolhXoyng0HAID3ae33t8MP0vXx8ZHValVUVNQZF3muICwBAOB9XPYg3T179igyMvKMigMAAPAWDq+z1LFjR1fUAQAA4JEcvrIEAABwPiEsAQAAmGhVWFq+fLkaGxtdXUurvPzyy+rUqZOCgoKUkpKiDRs2mPZfunSpEhMTFRQUpB49euiDDz6w228YhqZNm6aLLrpIwcHBSk9P11dffeXKUwAAAF6kVWHp5ptvVnV1tSTJ19dXlZWVrqzplJYsWaLc3Fzl5+dr8+bN6tWrlzIyMk5Zz7p16zRixAjl5ORoy5YtyszMVGZmprZv327rM336dM2aNUtz5szR+vXrdcEFFygjI0PHjh07W6cFAAA8WKuWDoiJidHcuXN14403ysfHRxUVFW75RVxKSoquuOIKvfTSS5Kk5uZmxcXFady4cZoyZcpJ/YcNG6a6ujqtWLHC1tavXz/17t1bc+bMkWEYio2N1aRJk3TfffdJkmpqahQdHa358+dr+PDhraqLpQMAAPA+Tl064K677tLgwYPl6+sri8WimJgY+fr6tri5SkNDg0pLS5Wenv7v4n18lJ6erpKSkhaPKSkpsesvSRkZGbb+e/bskdVqtesTFhamlJSUU76nJNXX16u2ttZuAwAA56ZWLR3wyCOPaPjw4fr666910003ad68eQoPD3dxafYOHTqkpqYmRUdH27VHR0dr9+7dLR5jtVpb7G+1Wm37T7Sdqk9LCgoK9Oijjzp8DgAAwPu0ep2lxMREJSYmKj8/X7feeqtCQkJcWZdHy8vLU25uru11bW2t4uLi3FgRAABwFYcXpczPz5ckHTx4UGVlZZKkSy+91OVzmNq3by9fX19VVFTYtVdUVCgmJqbFY2JiYkz7n/hnRUWFLrroIrs+vXv3PmUtgYGBCgwMPJ3TAAAAXsbhdZaOHDmiO+64Q7Gxsbr66qt19dVXKzY2Vjk5OTpy5IgrapQkBQQEKDk5WUVFRba25uZmFRUVKTU1tcVjUlNT7fpL0qpVq2z9O3furJiYGLs+tbW1Wr9+/SnfEwAAnF8cDksTJ05UcXGxli9frurqalVXV+sf//iHiouLNWnSJFfUaJObm6u5c+fqjTfe0K5du3T33Xerrq5Oo0aNkiRlZ2crLy/P1n/ChAkqLCzUjBkztHv3bj3yyCPatGmTxo4dK0myWCy699579cQTT2j58uXatm2bsrOzFRsbq8zMTJeeCwAA8A4O34Z7++239dZbb+n3v/+9re2GG25QcHCwbrvtNs2ePduZ9dkZNmyYDh48qGnTpslqtap3794qLCy0TdAuLy+Xj8+/81///v21aNEiTZ06VQ8++KASEhL07rvvqnv37rY+999/v+rq6jR69GhVV1drwIABKiwsVFBQkMvOAwAAeI9WrbP0ayEhISotLVVSUpJd+44dO9S3b1/V1dU5tUBvwDpLAAB4H6eus/Rrqampys/Pt1vh+ujRo3r00UeZ5wMAAM45Dt+GmzlzpjIyMtShQwf16tVLkrR161YFBQVp5cqVTi8QAADAnRy+DSf98ou4hQsX2haDTEpKUlZWloKDg51eoDfgNhwAAN6ntd/fDl9Zkn6Zt3TnnXeednEAAADewuE5SwAAAOcTwhIAAIAJwhIAAIAJwhIAAIAJh8NSly5ddPjw4ZPaq6ur1aVLF6cUBQAA4CkcDkvfffedmpqaTmqvr6/Xvn37nFIUAACAp2j10gHLly+3/b1y5UqFhYXZXjc1NamoqEidOnVyanEAAADu1uqwlJmZKUmyWCwaOXKk3T5/f3916tRJM2bMcGpxAAAA7tbqsNTc3CxJ6ty5szZu3Kj27du7rCgAAABP4fAK3nv27HFFHQAAAB7ptB53UlRUpKKiIlVWVtquOJ3w+uuvO6UwAAAAT+BwWHr00Uf12GOPqU+fPrroootksVhcURcAAIBHcDgszZkzR/Pnz9ftt9/uinoAAAA8isPrLDU0NKh///6uqAUAAMDjOByW/ud//keLFi1yRS0AAAAex+HbcMeOHdOrr76qjz76SD179pS/v7/d/ueee85pxQEAALibw2Hpiy++UO/evSVJ27dvt9vHZG8AAHCucTgsrVmzxhV1AAAAeCSH5yyd8PXXX2vlypU6evSoJMkwDKcVBQAA4CkcDkuHDx9WWlqaLrnkEt1www06cOCAJCknJ0eTJk1yeoEAAADu5HBYmjhxovz9/VVeXq6QkBBb+7Bhw1RYWOjU4gAAANzN4TlLH374oVauXKkOHTrYtSckJOj77793WmEAAACewOErS3V1dXZXlE6oqqpSYGCgU4oCAADwFA6HpauuukoLFiywvbZYLGpubtb06dN17bXXOrU4AAAAd3P4Ntz06dOVlpamTZs2qaGhQffff7927Nihqqoqffrpp66oEQAAwG0cvrLUvXt3ffnllxowYIAGDx6suro6DRkyRFu2bFHXrl1dUSMAAIDbWAwWSDpjtbW1CgsLU01NjUJDQ91dDgAAaIXWfn87fGVp3rx5Wrp06UntS5cu1RtvvOHo2wEAAHg0h8NSQUGB2rdvf1J7VFSUnnrqKacUBQAA4CkcDkvl5eXq3LnzSe0dO3ZUeXm5U4oCAADwFA6HpaioKH3xxRcntW/dulURERFOKQoAAMBTOByWRowYofHjx2vNmjVqampSU1OTVq9erQkTJmj48OGuqBEAAMBtHF5n6fHHH9d3332ntLQ0+fn9cnhzc7Oys7OZswQAAM45Di0dYBiG9u7dq8jISP3www/6/PPPFRwcrB49eqhjx46urNOjsXQAAADep7Xf3w5dWTIMQ//1X/+lHTt2KCEhQQkJCWdcKAAAgCdzaM6Sj4+PEhISdPjwYVfVAwAA4FEcnuD99NNPa/Lkydq+fbsr6gEAAPAoDj/u5MILL9SRI0d0/PhxBQQEKDg42G5/VVWVUwv0BsxZAgDA+7hkzpIkvfDCC2dSFwAAgFdxOCyNHDnSFXUAAAB4JIfnLEnSN998o6lTp2rEiBGqrKyUJP3zn//Ujh07nFocAACAuzkcloqLi9WjRw+tX79e77zzjn7++WdJvzzuJD8/3+kFAgAAuJPDYWnKlCl64okntGrVKgUEBNjaBw4cqM8++8ypxQEAALibw2Fp27Ztuvnmm09qj4qK0qFDh5xSFAAAgKdwOCyFh4frwIEDJ7Vv2bJFF198sVOKAgAA8BQOh6Xhw4frgQcekNVqlcViUXNzsz799FPdd999ys7OdkWNAAAAbuNwWHrqqaeUmJiouLg4/fzzz+rWrZuuvvpq9e/fX1OnTnVFjQAAAG7j8AreJ5SXl2v79u36+eefdfnll5/XD9VlBW8AALyPy1bwPiE+Pl5xcXGSJIvFcrpvAwAA4NFOa1HKv/71r+revbuCgoIUFBSk7t2767XXXnN2bQAAAG7n8JWladOm6bnnntO4ceOUmpoqSSopKdHEiRNVXl6uxx57zOlFAgAAuIvDc5YiIyM1a9YsjRgxwq79b3/7m8aNG3derrXEnCUAALxPa7+/Hb4N19jYqD59+pzUnpycrOPHjzv6dgAAAB7N4bB0++23a/bs2Se1v/rqq8rKynJKUS2pqqpSVlaWQkNDFR4erpycHNtz6U7l2LFjGjNmjCIiItSmTRsNHTpUFRUVtv1bt27ViBEjFBcXp+DgYCUlJWnmzJkuOwcAAOB9TuvXcH/961/14Ycfql+/fpKk9evXq7y8XNnZ2crNzbX1e+6555xTpaSsrCwdOHBAq1atUmNjo0aNGqXRo0dr0aJFpzxm4sSJev/997V06VKFhYVp7NixGjJkiD799FNJUmlpqaKiovTmm28qLi5O69at0+jRo+Xr66uxY8c6rXYAAOC9HJ6zdO2117bujS0WrV69+rSK+k+7du1St27dtHHjRtstwMLCQt1www364YcfFBsbe9IxNTU1ioyM1KJFi3TLLbdIknbv3q2kpCSVlJTYgt5/GjNmjHbt2mVae319verr622va2trFRcXx5wlAAC8iMvWWVqzZs0ZFXY6SkpKFB4ebjdXKj09XT4+Plq/fn2LD/YtLS1VY2Oj0tPTbW2JiYmKj483DUs1NTVq166daT0FBQV69NFHT/NsAACANzmtdZbONqvVqqioKLs2Pz8/tWvXTlar9ZTHBAQEKDw83K49Ojr6lMesW7dOS5Ys0ejRo03rycvLU01NjW3bu3dv608GAAB4FbeGpSlTpshisZhuu3fvPiu1bN++XYMHD1Z+fr7+8Ic/mPYNDAxUaGio3QYAAM5Np/24E2eYNGmS/vznP5v26dKli2JiYlRZWWnXfvz4cVVVVSkmJqbF42JiYtTQ0KDq6mq7q0sVFRUnHbNz506lpaVp9OjRPAwYAADYcWtYioyMVGRk5G/2S01NVXV1tUpLS5WcnCxJWr16tZqbm5WSktLiMcnJyfL391dRUZGGDh0qSSorK1N5eblt5XFJ2rFjhwYOHKiRI0fqySefdMJZAQCAc4nDv4Zzl+uvv14VFRWaM2eObemAPn362JYO2Ldvn9LS0rRgwQL17dtXknT33Xfrgw8+0Pz58xUaGqpx48ZJ+mVukvTLrbeBAwcqIyNDzz77rO2zfH19WxXiTmAFbwAAvI/Lfg3nLgsXLtTYsWOVlpYmHx8fDR06VLNmzbLtb2xsVFlZmY4cOWJre/7552196+vrlZGRoVdeecW2/6233tLBgwf15ptv6s0337S1d+zYUd99991ZOS8AAODZvObKkifjyhIAAN7HZc+GAwAAOJ8QlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEwQlgAAAEx4TViqqqpSVlaWQkNDFR4erpycHP3888+mxxw7dkxjxoxRRESE2rRpo6FDh6qioqLFvocPH1aHDh1ksVhUXV3tgjMAAADeyGvCUlZWlnbs2KFVq1ZpxYoV+vjjjzV69GjTYyZOnKj33ntPS5cuVXFxsfbv368hQ4a02DcnJ0c9e/Z0RekAAMCLWQzDMNxdxG/ZtWuXunXrpo0bN6pPnz6SpMLCQt1www364YcfFBsbe9IxNTU1ioyM1KJFi3TLLbdIknbv3q2kpCSVlJSoX79+tr6zZ8/WkiVLNG3aNKWlpenHH39UeHj4Keupr69XfX297XVtba3i4uJUU1Oj0NBQJ501AABwpdraWoWFhf3m97dXXFkqKSlReHi4LShJUnp6unx8fLR+/foWjyktLVVjY6PS09NtbYmJiYqPj1dJSYmtbefOnXrssce0YMEC+fi07l9HQUGBwsLCbFtcXNxpnhkAAPB0XhGWrFaroqKi7Nr8/PzUrl07Wa3WUx4TEBBw0hWi6Oho2zH19fUaMWKEnn32WcXHx7e6nry8PNXU1Ni2vXv3OnZCAADAa7g1LE2ZMkUWi8V02717t8s+Py8vT0lJSfrTn/7k0HGBgYEKDQ212wAAwLnJz50fPmnSJP35z3827dOlSxfFxMSosrLSrv348eOqqqpSTExMi8fFxMSooaFB1dXVdleXKioqbMesXr1a27Zt01tvvSVJOjF9q3379nrooYf06KOPnuaZAQCAc4Vbw1JkZKQiIyN/s19qaqqqq6tVWlqq5ORkSb8EnebmZqWkpLR4THJysvz9/VVUVKShQ4dKksrKylReXq7U1FRJ0ttvv62jR4/ajtm4caPuuOMOffLJJ+rateuZnh4AADgHuDUstVZSUpKuu+463XnnnZozZ44aGxs1duxYDR8+3PZLuH379iktLU0LFixQ3759FRYWppycHOXm5qpdu3YKDQ3VuHHjlJqaavsl3H8GokOHDtk+z+zXcAAA4PzhFWFJkhYuXKixY8cqLS1NPj4+Gjp0qGbNmmXb39jYqLKyMh05csTW9vzzz9v61tfXKyMjQ6+88oo7ygcAAF7KK9ZZ8nStXacBAAB4jnNqnSUAAAB3ISwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACY8HN3AecCwzAkSbW1tW6uBAAAtNaJ7+0T3+OnQlhygp9++kmSFBcX5+ZKAACAo3766SeFhYWdcr/F+K04hd/U3Nys/fv3q23btrJYLGf0XrW1tYqLi9PevXsVGhrqpArhTIyR52OMPB9j5PnOhzEyDEM//fSTYmNj5eNz6plJXFlyAh8fH3Xo0MGp7xkaGnrO/sd5rmCMPB9j5PkYI893ro+R2RWlE5jgDQAAYIKwBAAAYIKw5GECAwOVn5+vwMBAd5eCU2CMPB9j5PkYI8/HGP0bE7wBAABMcGUJAADABGEJAADABGEJAADABGEJAADABGHJg7z88svq1KmTgoKClJKSog0bNri7pPNWQUGBrrjiCrVt21ZRUVHKzMxUWVmZXZ9jx45pzJgxioiIUJs2bTR06FBVVFS4qWI8/fTTslgsuvfee21tjJH77du3T3/6058UERGh4OBg9ejRQ5s2bbLtNwxD06ZN00UXXaTg4GClp6frq6++cmPF55+mpiY9/PDD6ty5s4KDg9W1a1c9/vjjds9LO9/HibDkIZYsWaLc3Fzl5+dr8+bN6tWrlzIyMlRZWenu0s5LxcXFGjNmjD777DOtWrVKjY2N+sMf/qC6ujpbn4kTJ+q9997T0qVLVVxcrP3792vIkCFurPr8tXHjRv3f//2fevbsadfOGLnXjz/+qCuvvFL+/v765z//qZ07d2rGjBm68MILbX2mT5+uWbNmac6cOVq/fr0uuOACZWRk6NixY26s/PzyzDPPaPbs2XrppZe0a9cuPfPMM5o+fbpefPFFW5/zfpwMeIS+ffsaY8aMsb1uamoyYmNjjYKCAjdWhRMqKysNSUZxcbFhGIZRXV1t+Pv7G0uXLrX12bVrlyHJKCkpcVeZ56WffvrJSEhIMFatWmVcc801xoQJEwzDYIw8wQMPPGAMGDDglPubm5uNmJgY49lnn7W1VVdXG4GBgcbf/va3s1EiDMMYNGiQcccdd9i1DRkyxMjKyjIMg3EyDMPgypIHaGhoUGlpqdLT021tPj4+Sk9PV0lJiRsrwwk1NTWSpHbt2kmSSktL1djYaDdmiYmJio+PZ8zOsjFjxmjQoEF2YyExRp5g+fLl6tOnj2699VZFRUXp8ssv19y5c2379+zZI6vVajdGYWFhSklJYYzOov79+6uoqEhffvmlJGnr1q3617/+peuvv14S4yTxIF2PcOjQITU1NSk6OtquPTo6Wrt373ZTVTihublZ9957r6688kp1795dkmS1WhUQEKDw8HC7vtHR0bJarW6o8vy0ePFibd68WRs3bjxpH2Pkft9++61mz56t3NxcPfjgg9q4caPGjx+vgIAAjRw50jYOLf2/jzE6e6ZMmaLa2lolJibK19dXTU1NevLJJ5WVlSVJjJMIS8BvGjNmjLZv365//etf7i4Fv7J3715NmDBBq1atUlBQkLvLQQuam5vVp08fPfXUU5Kkyy+/XNu3b9ecOXM0cuRIN1eHE/7+979r4cKFWrRokS677DJ9/vnnuvfeexUbG8s4/X/chvMA7du3l6+v70m/0qmoqFBMTIybqoIkjR07VitWrNCaNWvUoUMHW3tMTIwaGhpUXV1t158xO3tKS0tVWVmp3/3ud/Lz85Ofn5+Ki4s1a9Ys+fn5KTo6mjFys4suukjdunWza0tKSlJ5ebkk2caB//e51+TJkzVlyhQNHz5cPXr00O23366JEyeqoKBAEuMkEZY8QkBAgJKTk1VUVGRra25uVlFRkVJTU91Y2fnLMAyNHTtWy5Yt0+rVq9W5c2e7/cnJyfL397cbs7KyMpWXlzNmZ0laWpq2bdumzz//3Lb16dNHWVlZtr8ZI/e68sorT1py48svv1THjh0lSZ07d1ZMTIzdGNXW1mr9+vWM0Vl05MgR+fjYxwFfX181NzdLYpwk8Ws4T7F48WIjMDDQmD9/vrFz505j9OjRRnh4uGG1Wt1d2nnp7rvvNsLCwoy1a9caBw4csG1Hjhyx9bnrrruM+Ph4Y/Xq1camTZuM1NRUIzU11Y1V49e/hjMMxsjdNmzYYPj5+RlPPvmk8dVXXxkLFy40QkJCjDfffNPW5+mnnzbCw8ONf/zjH8YXX3xhDB482OjcubNx9OhRN1Z+fhk5cqRx8cUXGytWrDD27NljvPPOO0b79u2N+++/39bnfB8nwpIHefHFF434+HgjICDA6Nu3r/HZZ5+5u6TzlqQWt3nz5tn6HD161LjnnnuMCy+80AgJCTFuvvlm48CBA+4rGieFJcbI/d577z2je/fuRmBgoJGYmGi8+uqrdvubm5uNhx9+2IiOjjYCAwONtLQ0o6yszE3Vnp9qa2uNCRMmGPHx8UZQUJDRpUsX46GHHjLq6+ttfc73cbIYxq+W6AQAAIAd5iwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBAACYICwBgBOtXbtWFovlpAf4/tr8+fMVHh5+1moCcGYISwDgRP3799eBAwcUFhbm7lIAOAlhCQBOQ1NTk+2p7L8WEBCgmJgYWSwWN1QFwBUISwDOCZ06ddILL7xg19a7d2898sgjkiTDMPTII48oPj5egYGBio2N1fjx42196+vrdd999+niiy/WBRdcoJSUFK1du9a2/8Sts+XLl6tbt24KDAxUeXn5SXW0dBtu/vz5io+PV0hIiG6++WYdPnzYmacOwMX83F0AAJwNb7/9tp5//nktXrxYl112maxWq7Zu3WrbP3bsWO3cuVOLFy9WbGysli1bpuuuu07btm1TQkKCJOnIkSN65pln9NprrykiIkJRUVG/+bnr169XTk6OCgoKlJmZqcLCQuXn57vsPAE4H2EJwHmhvLxcMTExSk9Pl7+/v+Lj49W3b1/bvnnz5qm8vFyxsbGSpPvuu0+FhYWaN2+ennrqKUlSY2OjXnnlFfXq1avVnztz5kxdd911uv/++yVJl1xyidatW6fCwkInnyEAV+E2HIDzwq233qqjR4+qS5cuuvPOO7Vs2TIdP35ckrRt2zY1NTXpkksuUZs2bWxbcXGxvvnmG9t7BAQEqGfPng597q5du5SSkmLXlpqaeuYnBOCs4coSgHOCj4+PDMOwa2tsbLT9HRcXp7KyMn300UdatWqV7rnnHj377LMqLi7Wzz//LF9fX5WWlsrX19fuPdq0aWP7Ozg4mInbwHmIsATgnBAZGakDBw7YXtfW1mrPnj12fYKDg3XjjTfqxhtv1JgxY5SYmKht27bp8ssvV1NTkyorK3XVVVc5ta6kpCStX7/eru2zzz5z6mcAcC3CEoBzwsCBAzV//nzdeOONCg8P17Rp0+yuEs2fP19NTU1KSUlRSEiI3nzzTQUHB6tjx46KiIhQVlaWsrOzNWPGDF1++eU6ePCgioqK1LNnTw0aNOi06xo/fryuvPJK/eUvf9HgwYO1cuVK5isBXoY5SwDOCXl5ebrmmmv0xz/+UYMGDVJmZqa6du1q2x8eHq65c+fqyiuvVM+ePfXRRx/pvffeU0REhCRp3rx5ys7O1qRJk3TppZcqMzNTGzduVHx8/BnV1a9fP82dO1czZ85Ur1699OGHH2rq1Kln9J4Azi6L8Z83+QEAAGDDlSUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAThCUAAAAT/w+dtQfwo7iMcgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data=np.sum(allPvalue, axis=1)*100/33\n",
    "index=[i for i in range (1,len(data)+1)]\n",
    "plt.bar(index, data)\n",
    "plt.xlabel('user id')\n",
    "plt.ylabel('percent of features')\n",
    "#plt.title('Total features in a profile (out of 65 features) that passed the similarity test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97999, 66)\n",
      "(97999, 57)\n"
     ]
    }
   ],
   "source": [
    "#Random project the recover data to impersonate the user. Used different seed\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "#traningdataReg = pd.concat([tDataReg, trainingDataRP['ID']], axis=1)\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','ID']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "traningdataReg.columns=dataset.columns\n",
    "trainingDataRPReg = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,96):\n",
    "    rng = np.random.RandomState(seed+10)\n",
    "    X = traningdataReg[traningdataReg['ID']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['ID'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['ID']=seed\n",
    "    trainingDataRPReg = pd.concat([trainingDataRPReg, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(traningdataReg.shape)\n",
    "print(trainingDataRPReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in test dataset: 96\n"
     ]
    }
   ],
   "source": [
    "print(\"Total user in test dataset:\", len(pd.unique(trainingDataRPReg['ID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3063/3063 [==============================] - 4s 1ms/step - loss: 19.9066 - accuracy: 0.0000e+00\n",
      "Loss: 19.906648635864258\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Performance of the attacker by using the random projected recover data\n",
    "#UserModel.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "Xtest=trainingDataRPReg.drop(columns=['ID'])\n",
    "ytest=trainingDataRPReg['ID']\n",
    "ytest=to_categorical(ytest)\n",
    "loss, accuracy = TrainedClassifier.evaluate(Xtest,ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97999, 66)\n",
      "(97999, 57)\n"
     ]
    }
   ],
   "source": [
    "#Random project the recover data to impersonate the user. If the attacker know the key\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "#traningdataReg = pd.concat([tDataReg, trainingDataRP['ID']], axis=1)\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','ID']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "traningdataReg.columns=dataset.columns\n",
    "trainingDataRPReg = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,96):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = traningdataReg[traningdataReg['ID']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['ID'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['ID']=seed\n",
    "    trainingDataRPReg = pd.concat([trainingDataRPReg, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(traningdataReg.shape)\n",
    "print(trainingDataRPReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3063/3063 [==============================] - 4s 1ms/step - loss: 0.1201 - accuracy: 0.9698\n",
      "Loss: 0.12010253220796585\n",
      "Accuracy: 0.969836413860321\n"
     ]
    }
   ],
   "source": [
    "#Performance of the attacker by using the random projected recover data when key is known\n",
    "#UserModel.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "Xtest=trainingDataRPReg.drop(columns=['ID'])\n",
    "ytest=trainingDataRPReg['ID']\n",
    "ytest=to_categorical(ytest)\n",
    "loss, accuracy = TrainedClassifier.evaluate(Xtest,ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

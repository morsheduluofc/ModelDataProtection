{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Info: Estimate the verification accuracy of DAC for project data when attacker doenot know the key\n",
    "- Used DAC (RP projected) data to train an NN model\n",
    "- Training data is project by a set of key\n",
    "- Test data is projectd by a set of different key (assume that attacker does not know the key of the user) \n",
    "- Follow same RP approach that mentioned before\n",
    "- Follow the same NN architecture\n",
    "- For 10 rounds of training training accurach reached to 100.0% and validation accuracy reached to 100.0%\n",
    "- However, for attacker data the accuracy reduced to 0.18%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.251082</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.366255</td>\n",
       "      <td>0.323770</td>\n",
       "      <td>0.065844</td>\n",
       "      <td>0.020243</td>\n",
       "      <td>0.032129</td>\n",
       "      <td>0.060729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.732435</td>\n",
       "      <td>0.736285</td>\n",
       "      <td>0.734870</td>\n",
       "      <td>0.707042</td>\n",
       "      <td>0.131098</td>\n",
       "      <td>0.196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.069264</td>\n",
       "      <td>0.074380</td>\n",
       "      <td>0.246914</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.198381</td>\n",
       "      <td>0.116466</td>\n",
       "      <td>0.085020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670837</td>\n",
       "      <td>0.671800</td>\n",
       "      <td>0.670509</td>\n",
       "      <td>0.651643</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.162602</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.510823</td>\n",
       "      <td>0.280992</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.184739</td>\n",
       "      <td>0.259109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651588</td>\n",
       "      <td>0.651588</td>\n",
       "      <td>0.657061</td>\n",
       "      <td>0.642254</td>\n",
       "      <td>0.320122</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085366</td>\n",
       "      <td>0.045833</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.181070</td>\n",
       "      <td>0.090164</td>\n",
       "      <td>0.045267</td>\n",
       "      <td>0.194332</td>\n",
       "      <td>0.188755</td>\n",
       "      <td>0.323887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641963</td>\n",
       "      <td>0.641963</td>\n",
       "      <td>0.645533</td>\n",
       "      <td>0.635681</td>\n",
       "      <td>0.204268</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.054645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.295833</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.378601</td>\n",
       "      <td>0.069672</td>\n",
       "      <td>0.600823</td>\n",
       "      <td>0.785425</td>\n",
       "      <td>0.465863</td>\n",
       "      <td>0.477733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727623</td>\n",
       "      <td>0.732435</td>\n",
       "      <td>0.731028</td>\n",
       "      <td>0.694836</td>\n",
       "      <td>0.173780</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.178862  0.316667  0.251082  0.144628  0.366255  0.323770  0.065844   \n",
       "1  0.166667  0.108333  0.069264  0.074380  0.246914  0.327869  0.213992   \n",
       "2  0.162602  0.112500  0.510823  0.280992  0.213992  0.040984  0.086420   \n",
       "3  0.085366  0.045833  0.025974  0.045455  0.181070  0.090164  0.045267   \n",
       "4  0.512195  0.295833  0.220779  0.247934  0.378601  0.069672  0.600823   \n",
       "\n",
       "          8         9        10  ...        57        58        59        60  \\\n",
       "0  0.020243  0.032129  0.060729  ...  0.732435  0.736285  0.734870  0.707042   \n",
       "1  0.198381  0.116466  0.085020  ...  0.670837  0.671800  0.670509  0.651643   \n",
       "2  0.052632  0.184739  0.259109  ...  0.651588  0.651588  0.657061  0.642254   \n",
       "3  0.194332  0.188755  0.323887  ...  0.641963  0.641963  0.645533  0.635681   \n",
       "4  0.785425  0.465863  0.477733  ...  0.727623  0.732435  0.731028  0.694836   \n",
       "\n",
       "         61     62   63        64        65  Label  \n",
       "0  0.131098  0.196  1.0  0.279070  0.016393      0  \n",
       "1  0.170732  0.156  0.0  0.209302  0.234973      0  \n",
       "2  0.320122  0.124  0.0  0.186047  0.327869      0  \n",
       "3  0.204268  0.080  1.0  0.325581  0.054645      0  \n",
       "4  0.173780  0.504  0.0  0.255814  0.245902      0  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read all data [194 users' oversampled data]\n",
    "import csv\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv('Dataset/OversampledDACData.csv',index_col=0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57940, 66)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0      300\n",
       "1      300\n",
       "2      300\n",
       "3      300\n",
       "4      300\n",
       "      ... \n",
       "188    300\n",
       "189    300\n",
       "190    300\n",
       "191    300\n",
       "192    300\n",
       "Name: Label, Length: 193, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace the user ID by class name and count the number of sample in each class\n",
    "dataset['Label'] = pd.factorize(dataset['Label'])[0]\n",
    "dataset.groupby(['Label'])['Label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in training dataset: 155\n",
      "Total user in auxiliary dataset: 38\n"
     ]
    }
   ],
   "source": [
    "#seperate the profile in two groups (80.0%, 20.0%): (i) Training profile (0-155), and (ii) auxiliary profile (156-192)\n",
    "totalUser= len(pd.unique(dataset['Label']))\n",
    "trainingData = dataset[dataset['Label'] <155]\n",
    "auxilaryData = dataset[dataset['Label'] >= 155]\n",
    "print(\"Total user in training dataset:\", len(pd.unique(trainingData['Label'])))\n",
    "print(\"Total user in auxiliary dataset:\", len(pd.unique(auxilaryData['Label'])))\n",
    "#assigned 0-154 users' data to dataset\n",
    "dataset=trainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in the training dataset: 155\n"
     ]
    }
   ],
   "source": [
    "#total use in the system\n",
    "totalUser= len(pd.unique(dataset['Label']))\n",
    "trainingData=dataset\n",
    "print(\"Total user in the training dataset:\", len(pd.unique(trainingData['Label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_27740\\1267165762.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  trainDatasetRP = pd.concat([trainDatasetRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46540, 66)\n",
      "(46540, 57)\n"
     ]
    }
   ],
   "source": [
    "#random project the training data\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','Label']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "trainDatasetRP = pd.DataFrame(columns=column1)\n",
    "\n",
    "for seed in range(0,155):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = trainingData[trainingData['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    trainDatasetRP = pd.concat([trainDatasetRP, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(trainingData.shape)\n",
    "print(trainDatasetRP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare group 1 traning data for training and validate the model\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X=trainDatasetRP.drop(columns=['Label'])\n",
    "y=trainDatasetRP['Label']\n",
    "\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size=0.2, random_state=22)\n",
    "\n",
    "#Xtest=testDatasetRP.drop(columns=['Label'])\n",
    "#ytest=testDatasetRP['Label']\n",
    "\n",
    "ytrain = to_categorical(ytrain)\n",
    "yval = to_categorical(yval)\n",
    "#ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37232, 56)\n",
      "(37232, 155)\n",
      "(9308, 56)\n",
      "(9308, 155)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(Xval.shape)\n",
    "print(yval.shape)\n",
    "#print(Xtest.shape)\n",
    "#print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary package for a neural network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inlineimport keras\n",
    "from keras.layers import Dense, Dropout, Input,Activation,Dropout, Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import mnist\n",
    "#from tqdm import tqdm\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizers for neural network\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "def adam_optimizer():\n",
    "    return Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "def RMSprop_optimizer():\n",
    "    return RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,648</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">155</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,075</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m3,648\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m155\u001b[0m)            │        \u001b[38;5;34m10,075\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,323</span> (122.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,323\u001b[0m (122.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,811</span> (120.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m30,811\u001b[0m (120.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neural network architecture for model training\n",
    "\n",
    "def create_classifierRP(release=False,totalClass=155):\n",
    "  classifier = Sequential()\n",
    "  classifier.add(Dense(64, input_dim=56))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.5))\n",
    "  \n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(128))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.2))\n",
    "  \n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(64))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.3))\n",
    "\n",
    "  #if release:\n",
    "  classifier.add(Dense(totalClass, activation='softmax'))\n",
    "  #else:\n",
    "  #   classifier.add(Dense(Tuser))\n",
    "  #np.log_softmax_v2(a, axis=axis)\n",
    "  #classifier.add(F.softmax(a, dim=1))\n",
    "\n",
    "  classifier.compile(loss='categorical_crossentropy', optimizer=RMSprop_optimizer(),metrics=['accuracy'])\n",
    "  return classifier\n",
    "\n",
    "Clasf=create_classifierRP()\n",
    "Clasf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.2072 - loss: 4.0019 - val_accuracy: 0.9926 - val_loss: 0.6503\n",
      "Epoch 2/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6901 - loss: 1.3186 - val_accuracy: 0.9975 - val_loss: 0.0483\n",
      "Epoch 3/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7716 - loss: 0.8373 - val_accuracy: 0.9997 - val_loss: 0.0131\n",
      "Epoch 4/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7992 - loss: 0.7020 - val_accuracy: 0.9997 - val_loss: 0.0074\n",
      "Epoch 5/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8232 - loss: 0.6197 - val_accuracy: 0.9997 - val_loss: 0.0052\n",
      "Epoch 6/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8338 - loss: 0.5672 - val_accuracy: 0.9997 - val_loss: 0.0042\n",
      "Epoch 7/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8424 - loss: 0.5431 - val_accuracy: 0.9998 - val_loss: 0.0033\n",
      "Epoch 8/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8558 - loss: 0.5036 - val_accuracy: 0.9998 - val_loss: 0.0031\n",
      "Epoch 9/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8580 - loss: 0.4979 - val_accuracy: 1.0000 - val_loss: 0.0025\n",
      "Epoch 10/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8624 - loss: 0.4720 - val_accuracy: 1.0000 - val_loss: 0.0022\n",
      "Epoch 11/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8712 - loss: 0.4514 - val_accuracy: 0.9999 - val_loss: 0.0019\n",
      "Epoch 12/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8718 - loss: 0.4360 - val_accuracy: 0.9999 - val_loss: 0.0017\n",
      "Epoch 13/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8767 - loss: 0.4188 - val_accuracy: 0.9998 - val_loss: 0.0015\n",
      "Epoch 14/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8806 - loss: 0.4232 - val_accuracy: 0.9999 - val_loss: 0.0014\n",
      "Epoch 15/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8818 - loss: 0.4125 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 16/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8804 - loss: 0.4200 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
      "Epoch 17/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8834 - loss: 0.4050 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 18/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8864 - loss: 0.3984 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 19/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8834 - loss: 0.3960 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
      "Epoch 20/20\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8860 - loss: 0.3909 - val_accuracy: 1.0000 - val_loss: 0.0011\n"
     ]
    }
   ],
   "source": [
    "#Train the classifier seperately for black-box attack\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
    "callbacks_list = [learning_rate_reduction]\n",
    "\n",
    "Classfier2= create_classifierRP(True,155)\n",
    "\n",
    "#------Comment will start from here\n",
    "lossc='categorical_crossentropy'\n",
    "optimizerc=RMSprop(learning_rate=0.001, rho=0.9)\n",
    "Classfier2.compile(loss=lossc, optimizer=optimizerc,metrics=['accuracy'])\n",
    "#------Comments will end from here\n",
    "historyc2 =  Classfier2.fit(Xtrain, ytrain, batch_size=64, epochs=20, validation_data=(Xval, yval),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epochs')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the classifier loss and accuracy curves for training and validation data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(historyc2.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "plt.plot(historyc2.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test by the data set that come from same user but different random matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test data and seperate test data\n",
    "import csv\n",
    "import pandas as pd\n",
    "testdataset=pd.read_csv('Dataset/DACDatatest.csv',index_col=0)\n",
    "testdataset = testdataset[testdataset['Label'] < 155]\n",
    "#testdataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_27740\\1841679469.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  testDatasetRP = pd.concat([testDatasetRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4121, 66)\n",
      "(4121, 57)\n"
     ]
    }
   ],
   "source": [
    "#random project the test data\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','Label']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "testDatasetRP = pd.DataFrame(columns=column1)\n",
    "\n",
    "for seed in range(0,155):\n",
    "    rng = np.random.RandomState(seed+1)\n",
    "    X = testdataset[testdataset['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    testDatasetRP = pd.concat([testDatasetRP, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(testdataset.shape)\n",
    "print(testDatasetRP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=testDatasetRP.drop(columns=['Label'])\n",
    "ytest=testDatasetRP['Label']\n",
    "ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.0327 - loss: 19.7747\n",
      "Loss: 20.4298152923584\n",
      "Accuracy: 0.00897840317338705\n"
     ]
    }
   ],
   "source": [
    "#Performance of the classifier\n",
    "Classfier2.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "loss, accuracy = Classfier2.evaluate(Xtest, ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test by the attack data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23457</th>\n",
       "      <td>142</td>\n",
       "      <td>0.256098</td>\n",
       "      <td>0.070833</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.392562</td>\n",
       "      <td>0.115226</td>\n",
       "      <td>0.028689</td>\n",
       "      <td>0.065844</td>\n",
       "      <td>0.101215</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746628</td>\n",
       "      <td>0.745910</td>\n",
       "      <td>0.743022</td>\n",
       "      <td>0.741595</td>\n",
       "      <td>0.735211</td>\n",
       "      <td>0.344512</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.103825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22231</th>\n",
       "      <td>29</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.045833</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>0.143443</td>\n",
       "      <td>0.094650</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.064257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752408</td>\n",
       "      <td>0.751684</td>\n",
       "      <td>0.749759</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>0.728638</td>\n",
       "      <td>0.256098</td>\n",
       "      <td>0.088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.125683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21701</th>\n",
       "      <td>152</td>\n",
       "      <td>0.382114</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.103306</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>0.209016</td>\n",
       "      <td>0.156379</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.152610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929672</td>\n",
       "      <td>0.928778</td>\n",
       "      <td>0.928778</td>\n",
       "      <td>0.926993</td>\n",
       "      <td>0.906103</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23943</th>\n",
       "      <td>152</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.020661</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.352459</td>\n",
       "      <td>0.144033</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.076305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859345</td>\n",
       "      <td>0.858518</td>\n",
       "      <td>0.864293</td>\n",
       "      <td>0.862632</td>\n",
       "      <td>0.839437</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.092896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22889</th>\n",
       "      <td>18</td>\n",
       "      <td>0.329268</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.090535</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>0.115226</td>\n",
       "      <td>0.218623</td>\n",
       "      <td>0.092369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458574</td>\n",
       "      <td>0.458133</td>\n",
       "      <td>0.459095</td>\n",
       "      <td>0.456292</td>\n",
       "      <td>0.442254</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.147541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label         1         2         3         4         5         6  \\\n",
       "23457    142  0.256098  0.070833  0.393939  0.392562  0.115226  0.028689   \n",
       "22231     29  0.060976  0.045833  0.004329  0.024793  0.189300  0.143443   \n",
       "21701    152  0.382114  0.262500  0.151515  0.103306  0.189300  0.209016   \n",
       "23943    152  0.130081  0.075000  0.090909  0.020661  0.222222  0.352459   \n",
       "22889     18  0.329268  0.025000  0.060606  0.045455  0.090535  0.114754   \n",
       "\n",
       "              7         8         9  ...        56        57        58  \\\n",
       "23457  0.065844  0.101215  0.040161  ...  0.746628  0.745910  0.743022   \n",
       "22231  0.094650  0.141700  0.064257  ...  0.752408  0.751684  0.749759   \n",
       "21701  0.156379  0.153846  0.152610  ...  0.929672  0.928778  0.928778   \n",
       "23943  0.144033  0.157895  0.076305  ...  0.859345  0.858518  0.864293   \n",
       "22889  0.115226  0.218623  0.092369  ...  0.458574  0.458133  0.459095   \n",
       "\n",
       "             59        60        61     62   63        64        65  \n",
       "23457  0.741595  0.735211  0.344512  0.248  0.0  0.139535  0.103825  \n",
       "22231  0.747358  0.728638  0.256098  0.088  1.0  0.232558  0.125683  \n",
       "21701  0.926993  0.906103  0.201220  0.372  0.0  0.139535  0.000000  \n",
       "23943  0.862632  0.839437  0.060976  0.216  0.0  0.093023  0.092896  \n",
       "22889  0.456292  0.442254  0.060976  0.200  1.0  0.325581  0.147541  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invalid test data\n",
    "import csv\n",
    "import pandas as pd\n",
    "testdataset=pd.read_csv('Dataset/DACDatatest.csv',index_col=0)\n",
    "testdataset = testdataset[testdataset['Label'] >= 155]\n",
    "newID = np.random.randint(0, 155, size=testdataset.shape[0])\n",
    "testdataset['Label'] = newID\n",
    "testdataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_27740\\3170691182.py:23: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  testDatasetRP = pd.concat([testDatasetRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1003, 66)\n",
      "(1003, 57)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','Label']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "testDatasetRP = pd.DataFrame(columns=column1)\n",
    "\n",
    "testDatasetRP = pd.DataFrame(columns=column1)\n",
    "\n",
    "for seed in range(0,155):\n",
    "    rng = np.random.RandomState(seed+1)\n",
    "    X = testdataset[testdataset['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    testDatasetRP = pd.concat([testDatasetRP, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(testdataset.shape)\n",
    "print(testDatasetRP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=testDatasetRP.drop(columns=['Label'])\n",
    "ytest=testDatasetRP['Label']\n",
    "ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0014 - loss: 20.5291      \n",
      "Loss: 20.293466567993164\n",
      "Accuracy: 0.0009970089886337519\n"
     ]
    }
   ],
   "source": [
    "#Performance of the classifier\n",
    "Classfier2.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "loss, accuracy = Classfier2.evaluate(Xtest, ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

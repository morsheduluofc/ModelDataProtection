{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Info: Estimate the verification accuracy of DAC for project data\n",
    "- Used DAC(RP projected) data to train an NN model\n",
    "- There are 193 different user's profiles and each profiles has 1000 data samples (normalized data)\n",
    "- Devide all profiles in two groups: training  profile (96) and auxilary profiles (96) \n",
    "- Each auxilary data semple has 65 different features and RP prjection moved them to 56 features\n",
    "- Random matrix of RP follow following distributions: Pr(x=+1)= 1/2s; Pr(x=-1)= 1/2s, Pr(x=0)= 1-1/s where s=3\n",
    "- The value of dimension reduction k is calculated by k= [(4+2\\beta)/(\\epsolon^2/2+\\epsolon^3/2)]log (n) where n is total sample in a profile and \\epsolon,\\beta>0\n",
    "- Construct a NN regressor has 4 dense layers along with 'BatchNormalization' and 'relu' activation funcation\n",
    "- Last layer is sigmoid function. Input dimension of model is 65 and output dimension 56.\n",
    "- Trained regressor to recover the plain data from the projected data for the 96 auxilary data classes\n",
    "- This traind regressor will be used to recove the training data of classifer.\n",
    "- Let say attacker has the access of RP data of original data and their corresponding label. Attacker can find it by model inversion attack\n",
    "\n",
    "- Included a summary of the NN architecture\n",
    "- Need shallow as RP make users profile more distinct\n",
    "- For 10 rounds of training training accurach reached to 100.0% and validation accuracy reached to 100.0%\n",
    "- Included a graph that shows change of training and validation acccruacy in different ephocs\n",
    "- Test accruacy 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.178862</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.251082</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.366255</td>\n",
       "      <td>0.323770</td>\n",
       "      <td>0.065844</td>\n",
       "      <td>0.020243</td>\n",
       "      <td>0.032129</td>\n",
       "      <td>0.060729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.732435</td>\n",
       "      <td>0.736285</td>\n",
       "      <td>0.734870</td>\n",
       "      <td>0.707042</td>\n",
       "      <td>0.131098</td>\n",
       "      <td>0.196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.069264</td>\n",
       "      <td>0.074380</td>\n",
       "      <td>0.246914</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.198381</td>\n",
       "      <td>0.116466</td>\n",
       "      <td>0.085020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670837</td>\n",
       "      <td>0.671800</td>\n",
       "      <td>0.670509</td>\n",
       "      <td>0.651643</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.162602</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.510823</td>\n",
       "      <td>0.280992</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.040984</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.184739</td>\n",
       "      <td>0.259109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651588</td>\n",
       "      <td>0.651588</td>\n",
       "      <td>0.657061</td>\n",
       "      <td>0.642254</td>\n",
       "      <td>0.320122</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085366</td>\n",
       "      <td>0.045833</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.181070</td>\n",
       "      <td>0.090164</td>\n",
       "      <td>0.045267</td>\n",
       "      <td>0.194332</td>\n",
       "      <td>0.188755</td>\n",
       "      <td>0.323887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.641963</td>\n",
       "      <td>0.641963</td>\n",
       "      <td>0.645533</td>\n",
       "      <td>0.635681</td>\n",
       "      <td>0.204268</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.054645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.295833</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.378601</td>\n",
       "      <td>0.069672</td>\n",
       "      <td>0.600823</td>\n",
       "      <td>0.785425</td>\n",
       "      <td>0.465863</td>\n",
       "      <td>0.477733</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727623</td>\n",
       "      <td>0.732435</td>\n",
       "      <td>0.731028</td>\n",
       "      <td>0.694836</td>\n",
       "      <td>0.173780</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255814</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.178862  0.316667  0.251082  0.144628  0.366255  0.323770  0.065844   \n",
       "1  0.166667  0.108333  0.069264  0.074380  0.246914  0.327869  0.213992   \n",
       "2  0.162602  0.112500  0.510823  0.280992  0.213992  0.040984  0.086420   \n",
       "3  0.085366  0.045833  0.025974  0.045455  0.181070  0.090164  0.045267   \n",
       "4  0.512195  0.295833  0.220779  0.247934  0.378601  0.069672  0.600823   \n",
       "\n",
       "          8         9        10  ...        57        58        59        60  \\\n",
       "0  0.020243  0.032129  0.060729  ...  0.732435  0.736285  0.734870  0.707042   \n",
       "1  0.198381  0.116466  0.085020  ...  0.670837  0.671800  0.670509  0.651643   \n",
       "2  0.052632  0.184739  0.259109  ...  0.651588  0.651588  0.657061  0.642254   \n",
       "3  0.194332  0.188755  0.323887  ...  0.641963  0.641963  0.645533  0.635681   \n",
       "4  0.785425  0.465863  0.477733  ...  0.727623  0.732435  0.731028  0.694836   \n",
       "\n",
       "         61     62   63        64        65  Label  \n",
       "0  0.131098  0.196  1.0  0.279070  0.016393      0  \n",
       "1  0.170732  0.156  0.0  0.209302  0.234973      0  \n",
       "2  0.320122  0.124  0.0  0.186047  0.327869      0  \n",
       "3  0.204268  0.080  1.0  0.325581  0.054645      0  \n",
       "4  0.173780  0.504  0.0  0.255814  0.245902      0  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read all data [194 users' oversampled data]\n",
    "import csv\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv('Dataset/OversampledDACData.csv',index_col=0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0      300\n",
       "1      300\n",
       "2      300\n",
       "3      300\n",
       "4      300\n",
       "      ... \n",
       "188    300\n",
       "189    300\n",
       "190    300\n",
       "191    300\n",
       "192    300\n",
       "Name: Label, Length: 193, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace the user ID by class name and count the number of sample in each class\n",
    "#dataset['Label'] = pd.factorize(dataset['Label'])[0]\n",
    "dataset.groupby(['Label'])['Label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in training dataset: 155\n",
      "Total user in auxiliary dataset: 38\n"
     ]
    }
   ],
   "source": [
    "#seperate the profile in two groups: (i) Training profile (0-95), and (ii) auxiliary profile (96-193)\n",
    "totalUser= len(pd.unique(dataset['Label']))\n",
    "trainingData = dataset[dataset['Label'] < 155]\n",
    "attackData = dataset[dataset['Label'] >= 155]\n",
    "print(\"Total user in training dataset:\", len(pd.unique(trainingData['Label'])))\n",
    "print(\"Total user in auxiliary dataset:\", len(pd.unique(attackData['Label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1          1.000000\n",
      "2          1.000000\n",
      "3          0.991342\n",
      "4          1.000000\n",
      "5          0.967078\n",
      "            ...    \n",
      "62         1.000000\n",
      "63         1.000000\n",
      "64         1.000000\n",
      "65         0.874317\n",
      "Label    154.000000\n",
      "Length: 66, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#value range of training data\n",
    "print(trainingData.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#When attacker only knows the R. Attacker will train the attack model by the reandom projected attack data that are train by random generated RP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.220833</td>\n",
       "      <td>0.125541</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>0.200820</td>\n",
       "      <td>0.218107</td>\n",
       "      <td>0.161943</td>\n",
       "      <td>0.220884</td>\n",
       "      <td>0.340081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360924</td>\n",
       "      <td>0.360924</td>\n",
       "      <td>0.360231</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.076220</td>\n",
       "      <td>0.224</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>0.191257</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.473251</td>\n",
       "      <td>0.504098</td>\n",
       "      <td>0.358025</td>\n",
       "      <td>0.469636</td>\n",
       "      <td>0.606426</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362849</td>\n",
       "      <td>0.362849</td>\n",
       "      <td>0.362152</td>\n",
       "      <td>0.353991</td>\n",
       "      <td>0.460366</td>\n",
       "      <td>0.148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.174797</td>\n",
       "      <td>0.120833</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.095041</td>\n",
       "      <td>0.082305</td>\n",
       "      <td>0.110656</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.194332</td>\n",
       "      <td>0.152610</td>\n",
       "      <td>0.202429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361886</td>\n",
       "      <td>0.361886</td>\n",
       "      <td>0.359270</td>\n",
       "      <td>0.351174</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117886</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.164502</td>\n",
       "      <td>0.119835</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>0.094650</td>\n",
       "      <td>0.234818</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.190283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360924</td>\n",
       "      <td>0.360924</td>\n",
       "      <td>0.360231</td>\n",
       "      <td>0.352113</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.234973</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.052846</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.160173</td>\n",
       "      <td>0.115702</td>\n",
       "      <td>0.304527</td>\n",
       "      <td>0.299180</td>\n",
       "      <td>0.152263</td>\n",
       "      <td>0.170040</td>\n",
       "      <td>0.148594</td>\n",
       "      <td>0.080972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362849</td>\n",
       "      <td>0.363811</td>\n",
       "      <td>0.363112</td>\n",
       "      <td>0.354930</td>\n",
       "      <td>0.161585</td>\n",
       "      <td>0.240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.245902</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.341463  0.220833  0.125541  0.070248  0.057613  0.200820  0.218107   \n",
       "1  0.439024  0.495833  0.432900  0.272727  0.473251  0.504098  0.358025   \n",
       "2  0.174797  0.120833  0.129870  0.095041  0.082305  0.110656  0.086420   \n",
       "3  0.117886  0.100000  0.164502  0.119835  0.041152  0.061475  0.094650   \n",
       "4  0.052846  0.075000  0.160173  0.115702  0.304527  0.299180  0.152263   \n",
       "\n",
       "          8         9        10  ...        57        58        59        60  \\\n",
       "0  0.161943  0.220884  0.340081  ...  0.360924  0.360924  0.360231  0.352113   \n",
       "1  0.469636  0.606426  0.384615  ...  0.362849  0.362849  0.362152  0.353991   \n",
       "2  0.194332  0.152610  0.202429  ...  0.361886  0.361886  0.359270  0.351174   \n",
       "3  0.234818  0.216867  0.190283  ...  0.360924  0.360924  0.360231  0.352113   \n",
       "4  0.170040  0.148594  0.080972  ...  0.362849  0.363811  0.363112  0.354930   \n",
       "\n",
       "         61     62   63        64        65  Label  \n",
       "0  0.076220  0.224  1.0  0.186047  0.191257    155  \n",
       "1  0.460366  0.148  1.0  0.209302  0.754098    155  \n",
       "2  0.426829  0.252  1.0  0.209302  0.180328    155  \n",
       "3  0.390244  0.124  1.0  0.232558  0.234973    155  \n",
       "4  0.161585  0.240  1.0  0.209302  0.245902    155  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#newID = np.random.randint(0, 155, size=attackData.shape[0])\n",
    "#print(newID.shape)\n",
    "#attackData.drop(columns=['Label'])\n",
    "#attackData['Label'] = newID\n",
    "attackData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_29312\\2307350339.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  attackDataRP = pd.concat([attackDataRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11400, 66)\n",
      "(11400, 57)\n"
     ]
    }
   ],
   "source": [
    "#Random project the auxiliary dataset\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','Label']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "\n",
    "attackDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(155,193):\n",
    "    rng = np.random.RandomState(seed-155)\n",
    "    X = attackData[attackData['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    attackDataRP = pd.concat([attackDataRP, XRP], ignore_index=True)\n",
    "    #print(auxilaryDataRP)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(attackData.shape)\n",
    "print(attackDataRP.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        155.0\n",
      "1        155.0\n",
      "2        155.0\n",
      "3        155.0\n",
      "4        155.0\n",
      "         ...  \n",
      "11395    192.0\n",
      "11396    192.0\n",
      "11397    192.0\n",
      "11398    192.0\n",
      "11399    192.0\n",
      "Name: Label, Length: 11400, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#user id in auxilary data\n",
    "print(attackDataRP['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the traning data for training and testing the attacker's model\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Xdata=attackData.drop(columns=['Label'])\n",
    "XRPdata=attackDataRP.drop(columns=['Label'])\n",
    "\n",
    "\n",
    "Xtrain, Xval, XRPtrain, XRPval = train_test_split(Xdata, XRPdata, test_size=0.2, random_state=22)\n",
    "#Xtrain, Xval, XRPtrain, XRPval = train_test_split(Xtrain, XRPtrain, test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9120, 65)\n",
      "(9120, 56)\n",
      "(2280, 65)\n",
      "(2280, 56)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(XRPtrain.shape)\n",
    "#print(Xtest.shape)\n",
    "#print(XRPtest.shape)\n",
    "print(Xval.shape)\n",
    "print(XRPval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary package for a neural network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inlineimport keras\n",
    "from keras.layers import Dense, Dropout, Input,Activation,Dropout, Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import mnist\n",
    "#from tqdm import tqdm\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizers for neural network\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "def adam_optimizer():\n",
    "    return Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "def RMSprop_optimizer():\n",
    "    return RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,296</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,385</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m7,296\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)             │         \u001b[38;5;34m8,385\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,465</span> (587.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m150,465\u001b[0m (587.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">148,929</span> (581.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m148,929\u001b[0m (581.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neural network architecture for training a regressor\n",
    "\n",
    "def create_Regressor(release=False,outDim=65):\n",
    "  classifier = Sequential()\n",
    "  classifier.add(Dense(128, input_dim=56))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.2))\n",
    "   \n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(256))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.2))\n",
    "\n",
    "  classifier.add(Dense(256))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.2))\n",
    "\n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(128))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.2))\n",
    "\n",
    "  #if release:\n",
    "  classifier.add(Dense(outDim, activation='sigmoid'))\n",
    "  #else:\n",
    "  #   classifier.add(Dense(Tuser))\n",
    "  #np.log_softmax_v2(a, axis=axis)\n",
    "  #classifier.add(F.softmax(a, dim=1))\n",
    "\n",
    "  classifier.compile(loss='mean_squared_error', optimizer='SGD',metrics=['mean_squared_error'])\n",
    "  return classifier\n",
    "\n",
    "Clasf=create_Regressor()\n",
    "Clasf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 0.0702 - mean_squared_error: 0.0702 - val_loss: 0.0243 - val_mean_squared_error: 0.0243\n",
      "Epoch 2/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0153 - mean_squared_error: 0.0153 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "Epoch 3/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0077 - val_mean_squared_error: 0.0077\n",
      "Epoch 4/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0068 - val_mean_squared_error: 0.0068\n",
      "Epoch 5/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 6/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 7/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0091 - mean_squared_error: 0.0091 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 8/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0086 - mean_squared_error: 0.0086 - val_loss: 0.0051 - val_mean_squared_error: 0.0051\n",
      "Epoch 9/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 10/10\n",
      "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0047 - val_mean_squared_error: 0.0047\n"
     ]
    }
   ],
   "source": [
    "#Train the regressor  by auxilary dataset\n",
    "# Input: Projected data\n",
    "# Output: Plain data\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
    "callbacks_list = [learning_rate_reduction]\n",
    "\n",
    "Regressor= create_Regressor(True,65)\n",
    "\n",
    "#------Comment will start from here\n",
    "lossc='mean_squared_error'\n",
    "optimizerc=RMSprop(learning_rate=0.001, rho=0.9)\n",
    "Regressor.compile(loss=lossc, optimizer=optimizerc,metrics=['mean_squared_error'])\n",
    "#------Comments will end from here\n",
    "Rhistoryc2 =  Regressor.fit(XRPtrain, Xtrain, batch_size=64, epochs=10, validation_data=(XRPval, Xval),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model by pre-seperated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test data and seperate test data\n",
    "import csv\n",
    "import pandas as pd\n",
    "testattackdata=pd.read_csv('Dataset/DACDatatest.csv',index_col=0)\n",
    "testattackdata = testattackdata[testattackdata['Label'] >= 155]\n",
    "#testdataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1003,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23457</th>\n",
       "      <td>61</td>\n",
       "      <td>0.256098</td>\n",
       "      <td>0.070833</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.392562</td>\n",
       "      <td>0.115226</td>\n",
       "      <td>0.028689</td>\n",
       "      <td>0.065844</td>\n",
       "      <td>0.101215</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746628</td>\n",
       "      <td>0.745910</td>\n",
       "      <td>0.743022</td>\n",
       "      <td>0.741595</td>\n",
       "      <td>0.735211</td>\n",
       "      <td>0.344512</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.103825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22231</th>\n",
       "      <td>56</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.045833</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>0.143443</td>\n",
       "      <td>0.094650</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>0.064257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752408</td>\n",
       "      <td>0.751684</td>\n",
       "      <td>0.749759</td>\n",
       "      <td>0.747358</td>\n",
       "      <td>0.728638</td>\n",
       "      <td>0.256098</td>\n",
       "      <td>0.088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.125683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21701</th>\n",
       "      <td>140</td>\n",
       "      <td>0.382114</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.103306</td>\n",
       "      <td>0.189300</td>\n",
       "      <td>0.209016</td>\n",
       "      <td>0.156379</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.152610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929672</td>\n",
       "      <td>0.928778</td>\n",
       "      <td>0.928778</td>\n",
       "      <td>0.926993</td>\n",
       "      <td>0.906103</td>\n",
       "      <td>0.201220</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23943</th>\n",
       "      <td>142</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.020661</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.352459</td>\n",
       "      <td>0.144033</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.076305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859345</td>\n",
       "      <td>0.858518</td>\n",
       "      <td>0.864293</td>\n",
       "      <td>0.862632</td>\n",
       "      <td>0.839437</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.092896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22889</th>\n",
       "      <td>106</td>\n",
       "      <td>0.329268</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.090535</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>0.115226</td>\n",
       "      <td>0.218623</td>\n",
       "      <td>0.092369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458574</td>\n",
       "      <td>0.458133</td>\n",
       "      <td>0.459095</td>\n",
       "      <td>0.456292</td>\n",
       "      <td>0.442254</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.147541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label         1         2         3         4         5         6  \\\n",
       "23457     61  0.256098  0.070833  0.393939  0.392562  0.115226  0.028689   \n",
       "22231     56  0.060976  0.045833  0.004329  0.024793  0.189300  0.143443   \n",
       "21701    140  0.382114  0.262500  0.151515  0.103306  0.189300  0.209016   \n",
       "23943    142  0.130081  0.075000  0.090909  0.020661  0.222222  0.352459   \n",
       "22889    106  0.329268  0.025000  0.060606  0.045455  0.090535  0.114754   \n",
       "\n",
       "              7         8         9  ...        56        57        58  \\\n",
       "23457  0.065844  0.101215  0.040161  ...  0.746628  0.745910  0.743022   \n",
       "22231  0.094650  0.141700  0.064257  ...  0.752408  0.751684  0.749759   \n",
       "21701  0.156379  0.153846  0.152610  ...  0.929672  0.928778  0.928778   \n",
       "23943  0.144033  0.157895  0.076305  ...  0.859345  0.858518  0.864293   \n",
       "22889  0.115226  0.218623  0.092369  ...  0.458574  0.458133  0.459095   \n",
       "\n",
       "             59        60        61     62   63        64        65  \n",
       "23457  0.741595  0.735211  0.344512  0.248  0.0  0.139535  0.103825  \n",
       "22231  0.747358  0.728638  0.256098  0.088  1.0  0.232558  0.125683  \n",
       "21701  0.926993  0.906103  0.201220  0.372  0.0  0.139535  0.000000  \n",
       "23943  0.862632  0.839437  0.060976  0.216  0.0  0.093023  0.092896  \n",
       "22889  0.456292  0.442254  0.060976  0.200  1.0  0.325581  0.147541  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "newID = np.random.randint(0, 155, size=testattackdata.shape[0])\n",
    "print(newID.shape)\n",
    "testattackdata.drop(columns=['Label'])\n",
    "testattackdata['Label'] = newID\n",
    "testattackdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_29312\\2188155608.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  testattackDataRP = pd.concat([testattackDataRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1003, 66)\n",
      "(1003, 57)\n"
     ]
    }
   ],
   "source": [
    "#Random project the auxiliary dataset\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','Label']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "\n",
    "testattackDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,155):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = testattackdata[testattackdata['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    testattackDataRP = pd.concat([testattackDataRP, XRP], ignore_index=True)\n",
    "    #print(auxilaryDataRP)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(testattackdata.shape)\n",
    "print(testattackDataRP.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testattackdata=testattackdata.drop(columns=['Label'])\n",
    "testattackDataRP=testattackDataRP.drop(columns=['Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0601 - mean_squared_error: 0.0601\n",
      "Loss: 0.058681175112724304\n",
      "Accuracy: 0.058681175112724304\n"
     ]
    }
   ],
   "source": [
    "#Performance of the trained attacker regressor\n",
    "loss, accuracy = Regressor.evaluate(testattackDataRP, testattackdata)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let say attacker has the access of Random projected data of the original data profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_29312\\1838576605.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  trainingDataRP = pd.concat([trainingDataRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46540, 66)\n",
      "(46540, 57)\n"
     ]
    }
   ],
   "source": [
    "#Let say attacker has the access of RP data of original data\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','Label']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "\n",
    "trainingDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,155):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = trainingData[trainingData['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    trainingDataRP = pd.concat([trainingDataRP, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(trainingData.shape)\n",
    "print(trainingDataRP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0.0\n",
      "1          0.0\n",
      "2          0.0\n",
      "3          0.0\n",
      "4          0.0\n",
      "         ...  \n",
      "46535    154.0\n",
      "46536    154.0\n",
      "46537    154.0\n",
      "46538    154.0\n",
      "46539    154.0\n",
      "Name: Label, Length: 46540, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#user id in original projected data\n",
    "print(trainingDataRP['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1455/1455\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step\n",
      "(46540, 65)\n"
     ]
    }
   ],
   "source": [
    "#Prediction of plain data by the attacker mdoel assuming that attacker has access of projected data\n",
    "tDataRP=trainingDataRP.drop(columns=['Label'])\n",
    "tDataReg= Regressor.predict(tDataRP)\n",
    "print(tDataReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#assume that along with project data attacker know the label of the data.\n",
    "# Add id with recovered data\n",
    "print(type(tDataReg))\n",
    "print(type(trainingDataRP['Label'].to_numpy()))\n",
    "traningdataReg = pd.concat([pd.DataFrame(tDataReg), trainingDataRP['Label'].to_frame()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46540, 66)\n"
     ]
    }
   ],
   "source": [
    "# recovered data by the attacker model from projected data\n",
    "print(traningdataReg.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test the qulity of recover data we did this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1         2         3         4         5         6         7  \\\n",
      "0      0.208853  0.183157  0.219295  0.193336  0.192574  0.248379  0.201413   \n",
      "1      0.207664  0.186287  0.236972  0.197757  0.175529  0.240326  0.195937   \n",
      "2      0.239393  0.221332  0.265436  0.232111  0.209495  0.277180  0.236056   \n",
      "3      0.203209  0.181092  0.233142  0.184322  0.177902  0.231640  0.196541   \n",
      "4      0.272950  0.259814  0.282706  0.280621  0.228014  0.278717  0.303132   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "46535  0.201335  0.205574  0.221969  0.205801  0.215680  0.202855  0.212637   \n",
      "46536  0.199706  0.200204  0.239592  0.207617  0.196971  0.208261  0.205438   \n",
      "46537  0.187971  0.203583  0.237067  0.209523  0.182819  0.196423  0.183536   \n",
      "46538  0.190083  0.198870  0.209776  0.200751  0.197605  0.194827  0.202447   \n",
      "46539  0.195618  0.197188  0.214858  0.202785  0.198069  0.210371  0.215173   \n",
      "\n",
      "              8         9        10  ...        57        58        59  \\\n",
      "0      0.242703  0.268674  0.226957  ...  0.379285  0.376215  0.353430   \n",
      "1      0.241245  0.262293  0.236166  ...  0.353149  0.358672  0.333965   \n",
      "2      0.282431  0.325786  0.280038  ...  0.388413  0.400681  0.395382   \n",
      "3      0.230759  0.274305  0.232951  ...  0.385864  0.377880  0.377603   \n",
      "4      0.315540  0.346789  0.306978  ...  0.367862  0.384178  0.362146   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "46535  0.258266  0.188992  0.220880  ...  0.587684  0.548585  0.564572   \n",
      "46536  0.230187  0.225280  0.236915  ...  0.478738  0.429115  0.429065   \n",
      "46537  0.217351  0.208021  0.231968  ...  0.460232  0.400286  0.410234   \n",
      "46538  0.228661  0.193136  0.220915  ...  0.541167  0.502560  0.518207   \n",
      "46539  0.235768  0.205036  0.239290  ...  0.532204  0.489650  0.500958   \n",
      "\n",
      "             60        61        62        63        64        65  Label  \n",
      "0      0.359815  0.242809  0.234074  0.978148  0.201860  0.259069    0.0  \n",
      "1      0.338456  0.244893  0.252733  0.959293  0.186695  0.268670    0.0  \n",
      "2      0.402137  0.287779  0.247026  0.988948  0.206946  0.323099    0.0  \n",
      "3      0.374559  0.236766  0.233181  0.985755  0.205542  0.256744    0.0  \n",
      "4      0.366920  0.318352  0.285970  0.987092  0.198598  0.394315    0.0  \n",
      "...         ...       ...       ...       ...       ...       ...    ...  \n",
      "46535  0.549525  0.293990  0.280347  0.554830  0.267869  0.208341  154.0  \n",
      "46536  0.463679  0.293305  0.292659  0.591533  0.262409  0.181142  154.0  \n",
      "46537  0.444345  0.275967  0.281470  0.814016  0.279391  0.169442  154.0  \n",
      "46538  0.504640  0.283154  0.285156  0.596749  0.269083  0.188622  154.0  \n",
      "46539  0.495014  0.276791  0.284333  0.746804  0.283303  0.199165  154.0  \n",
      "\n",
      "[46540 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "traningdataReg.columns=list(trainingData.columns)\n",
    "print(traningdataReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "allPvalue=np.zeros((155,65))\n",
    "for id in range(0,155):\n",
    "    dataset1=traningdataReg[traningdataReg['Label']==id]\n",
    "    dataset2=trainingData[trainingData['Label']==id]\n",
    "    for col in range (0,65):\n",
    "        sample1=dataset1.iloc[:,col]\n",
    "        sample2=dataset2.iloc[:,col]\n",
    "        statistics, allPvalue[id,col]=stats.kstest(sample1, sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.37611548e-059 1.16028800e-043 2.94177198e-069 ... 2.08851534e-039\n",
      "  2.26461490e-092 5.74340873e-102]\n",
      " [9.10207325e-045 5.90839504e-050 2.24122071e-052 ... 1.48029788e-179\n",
      "  9.35477688e-166 6.73492982e-027]\n",
      " [9.21153378e-031 9.64457033e-021 7.90395287e-022 ... 1.42289251e-042\n",
      "  1.60690341e-068 5.33345712e-019]\n",
      " ...\n",
      " [9.17381021e-052 1.42289251e-042 3.80668223e-056 ... 7.91391649e-170\n",
      "  1.81506781e-145 9.01504695e-049]\n",
      " [3.10139620e-130 1.08538863e-127 5.29580758e-099 ... 9.01504695e-049\n",
      "  1.48029788e-179 1.40598415e-121]\n",
      " [2.03085902e-059 8.62330055e-057 2.31992428e-075 ... 8.14465183e-063\n",
      "  1.48029788e-179 3.16051840e-094]]\n"
     ]
    }
   ],
   "source": [
    "print(allPvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "allPvalue = np.where(allPvalue < 0.05, 0, 1)\n",
    "#allPvalue[allPvalue < 0.05] = 0\n",
    "print(allPvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0]\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(allPvalue, axis=1))\n",
    "print(len(allPvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data=np.sum(allPvalue, axis=1)\n",
    "index=[i for i in range (1,len(data)+1)]\n",
    "plt.bar(index, data)\n",
    "plt.xlabel('user id')\n",
    "plt.ylabel('total features')\n",
    "#plt.title('Total features in a profile (out of 65 features) that passed the similarity test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97999, 66)\n",
      "(97999, 57)\n"
     ]
    }
   ],
   "source": [
    "#Random project the recover data to impersonate the user. Used different seed\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "#traningdataReg = pd.concat([tDataReg, trainingDataRP['ID']], axis=1)\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','ID']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "traningdataReg.columns=dataset.columns\n",
    "trainingDataRPReg = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,96):\n",
    "    rng = np.random.RandomState(seed+10)\n",
    "    X = traningdataReg[traningdataReg['ID']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['ID'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['ID']=seed\n",
    "    trainingDataRPReg = pd.concat([trainingDataRPReg, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(traningdataReg.shape)\n",
    "print(trainingDataRPReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in test dataset: 96\n"
     ]
    }
   ],
   "source": [
    "print(\"Total user in test dataset:\", len(pd.unique(trainingDataRPReg['ID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3063/3063 [==============================] - 4s 1ms/step - loss: 19.9066 - accuracy: 0.0000e+00\n",
      "Loss: 19.906648635864258\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Performance of the attacker by using the random projected recover data\n",
    "#UserModel.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "Xtest=trainingDataRPReg.drop(columns=['ID'])\n",
    "ytest=trainingDataRPReg['ID']\n",
    "ytest=to_categorical(ytest)\n",
    "loss, accuracy = TrainedClassifier.evaluate(Xtest,ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97999, 66)\n",
      "(97999, 57)\n"
     ]
    }
   ],
   "source": [
    "#Random project the recover data to impersonate the user. If the attacker know the key\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "#traningdataReg = pd.concat([tDataReg, trainingDataRP['ID']], axis=1)\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','ID']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "traningdataReg.columns=dataset.columns\n",
    "trainingDataRPReg = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,96):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = traningdataReg[traningdataReg['ID']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['ID'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['ID']=seed\n",
    "    trainingDataRPReg = pd.concat([trainingDataRPReg, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(traningdataReg.shape)\n",
    "print(trainingDataRPReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3063/3063 [==============================] - 4s 1ms/step - loss: 0.1201 - accuracy: 0.9698\n",
      "Loss: 0.12010253220796585\n",
      "Accuracy: 0.969836413860321\n"
     ]
    }
   ],
   "source": [
    "#Performance of the attacker by using the random projected recover data when key is known\n",
    "#UserModel.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "Xtest=trainingDataRPReg.drop(columns=['ID'])\n",
    "ytest=trainingDataRPReg['ID']\n",
    "ytest=to_categorical(ytest)\n",
    "loss, accuracy = TrainedClassifier.evaluate(Xtest,ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

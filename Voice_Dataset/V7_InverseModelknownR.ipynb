{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Info: Estimate the verification accuracy of DAC for project data\n",
    "- Used DAC(RP projected) data to train an NN model\n",
    "- There are 193 different user's profiles and each profiles has 1000 data samples (normalized data)\n",
    "- Devide all profiles in two groups: training  profile (96) and auxilary profiles (96) \n",
    "- Each auxilary data semple has 65 different features and RP prjection moved them to 56 features\n",
    "- Random matrix of RP follow following distributions: Pr(x=+1)= 1/2s; Pr(x=-1)= 1/2s, Pr(x=0)= 1-1/s where s=3\n",
    "- The value of dimension reduction k is calculated by k= [(4+2\\beta)/(\\epsolon^2/2+\\epsolon^3/2)]log (n) where n is total sample in a profile and \\epsolon,\\beta>0\n",
    "- Construct a NN regressor has 4 dense layers along with 'BatchNormalization' and 'relu' activation funcation\n",
    "- Last layer is sigmoid function. Input dimension of model is 65 and output dimension 56.\n",
    "- Trained regressor to recover the plain data from the projected data for the 96 auxilary data classes\n",
    "- This traind regressor will be used to recove the training data of classifer.\n",
    "- Let say attacker has the access of RP data of original data and their corresponding label. Attacker can find it by model inversion attack\n",
    "\n",
    "- Included a summary of the NN architecture\n",
    "- Need shallow as RP make users profile more distinct\n",
    "- For 10 rounds of training training accurach reached to 100.0% and validation accuracy reached to 100.0%\n",
    "- Included a graph that shows change of training and validation acccruacy in different ephocs\n",
    "- Test accruacy 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.582305</td>\n",
       "      <td>-0.091624</td>\n",
       "      <td>-0.113317</td>\n",
       "      <td>0.069735</td>\n",
       "      <td>-0.056116</td>\n",
       "      <td>0.071154</td>\n",
       "      <td>-0.146473</td>\n",
       "      <td>0.049818</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.020655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137189</td>\n",
       "      <td>-0.202803</td>\n",
       "      <td>-0.061708</td>\n",
       "      <td>-0.239653</td>\n",
       "      <td>0.033237</td>\n",
       "      <td>0.097121</td>\n",
       "      <td>0.090061</td>\n",
       "      <td>-0.064359</td>\n",
       "      <td>-0.222110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.403713</td>\n",
       "      <td>-0.178570</td>\n",
       "      <td>0.066751</td>\n",
       "      <td>-0.055265</td>\n",
       "      <td>-0.010389</td>\n",
       "      <td>0.041391</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>0.018638</td>\n",
       "      <td>-0.047548</td>\n",
       "      <td>-0.012591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434401</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>-0.172818</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>0.126595</td>\n",
       "      <td>-0.084699</td>\n",
       "      <td>-0.008194</td>\n",
       "      <td>-0.019724</td>\n",
       "      <td>-0.256300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.400374</td>\n",
       "      <td>-0.185792</td>\n",
       "      <td>0.012088</td>\n",
       "      <td>-0.067337</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0.049996</td>\n",
       "      <td>0.012814</td>\n",
       "      <td>-0.025244</td>\n",
       "      <td>-0.030817</td>\n",
       "      <td>-0.068128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156473</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>-0.124143</td>\n",
       "      <td>-0.129319</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>-0.046710</td>\n",
       "      <td>-0.008062</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>-0.075796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.477862</td>\n",
       "      <td>-0.018475</td>\n",
       "      <td>0.071872</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>-0.016150</td>\n",
       "      <td>-0.035783</td>\n",
       "      <td>0.070162</td>\n",
       "      <td>-0.061627</td>\n",
       "      <td>-0.107956</td>\n",
       "      <td>-0.082812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127110</td>\n",
       "      <td>0.179423</td>\n",
       "      <td>-0.115302</td>\n",
       "      <td>0.104676</td>\n",
       "      <td>-0.107193</td>\n",
       "      <td>0.209552</td>\n",
       "      <td>0.027887</td>\n",
       "      <td>0.117224</td>\n",
       "      <td>0.104110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.465657</td>\n",
       "      <td>-0.029432</td>\n",
       "      <td>-0.029614</td>\n",
       "      <td>0.028301</td>\n",
       "      <td>0.067341</td>\n",
       "      <td>-0.131359</td>\n",
       "      <td>0.090756</td>\n",
       "      <td>-0.053325</td>\n",
       "      <td>-0.038439</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122469</td>\n",
       "      <td>-0.361710</td>\n",
       "      <td>-0.096674</td>\n",
       "      <td>0.032555</td>\n",
       "      <td>0.192928</td>\n",
       "      <td>-0.012668</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>-0.094648</td>\n",
       "      <td>0.170670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.582305 -0.091624 -0.113317  0.069735 -0.056116  0.071154 -0.146473   \n",
       "1  0.403713 -0.178570  0.066751 -0.055265 -0.010389  0.041391  0.013069   \n",
       "2  0.400374 -0.185792  0.012088 -0.067337  0.038272  0.049996  0.012814   \n",
       "3  0.477862 -0.018475  0.071872 -0.000463 -0.016150 -0.035783  0.070162   \n",
       "4  0.465657 -0.029432 -0.029614  0.028301  0.067341 -0.131359  0.090756   \n",
       "\n",
       "          8         9        10  ...        96        97        98        99  \\\n",
       "0  0.049818  0.002500  0.020655  ...  0.137189 -0.202803 -0.061708 -0.239653   \n",
       "1  0.018638 -0.047548 -0.012591  ...  0.434401  0.033208 -0.172818  0.005580   \n",
       "2 -0.025244 -0.030817 -0.068128  ...  0.156473  0.000591 -0.124143 -0.129319   \n",
       "3 -0.061627 -0.107956 -0.082812  ...  0.127110  0.179423 -0.115302  0.104676   \n",
       "4 -0.053325 -0.038439  0.009342  ...  0.122469 -0.361710 -0.096674  0.032555   \n",
       "\n",
       "        100       101       102       103       104  Label  \n",
       "0  0.033237  0.097121  0.090061 -0.064359 -0.222110      0  \n",
       "1  0.126595 -0.084699 -0.008194 -0.019724 -0.256300      0  \n",
       "2  0.004072 -0.046710 -0.008062  0.001104 -0.075796      0  \n",
       "3 -0.107193  0.209552  0.027887  0.117224  0.104110      0  \n",
       "4  0.192928 -0.012668  0.027907 -0.094648  0.170670      0  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read all data [194 users' oversampled data]\n",
    "import csv\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv('Dataset/OversampledVoiceData.csv',index_col=0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0     200\n",
       "1     200\n",
       "2     200\n",
       "3     200\n",
       "4     200\n",
       "     ... \n",
       "81    200\n",
       "82    200\n",
       "83    200\n",
       "84    200\n",
       "85    200\n",
       "Name: Label, Length: 86, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace the user ID by class name and count the number of sample in each class\n",
    "#dataset['Label'] = pd.factorize(dataset['Label'])[0]\n",
    "dataset.groupby(['Label'])['Label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in training dataset: 68\n",
      "Total user in auxiliary dataset: 18\n"
     ]
    }
   ],
   "source": [
    "#seperate the profile in two groups: (i) Training profile (0-95), and (ii) auxiliary profile (96-193)\n",
    "totalUser= len(pd.unique(dataset['Label']))\n",
    "trainingData = dataset[dataset['Label'] < 68]\n",
    "attackData = dataset[dataset['Label'] >= 68]\n",
    "print(\"Total user in training dataset:\", len(pd.unique(trainingData['Label'])))\n",
    "print(\"Total user in auxiliary dataset:\", len(pd.unique(attackData['Label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1         0.952207\n",
      "2         1.000000\n",
      "3         0.766847\n",
      "4         0.747409\n",
      "5         1.000000\n",
      "           ...    \n",
      "101       0.745435\n",
      "102       0.526589\n",
      "103       0.586205\n",
      "104       2.246000\n",
      "Label    67.000000\n",
      "Length: 105, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#value range of training data\n",
    "print(trainingData.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When attacker only knows the R. Attacker will train the attack model by the reandom projected attack data that are train by random generated RP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.260391</td>\n",
       "      <td>-0.229405</td>\n",
       "      <td>-0.248368</td>\n",
       "      <td>0.227097</td>\n",
       "      <td>0.185063</td>\n",
       "      <td>-0.119205</td>\n",
       "      <td>-0.069965</td>\n",
       "      <td>0.105068</td>\n",
       "      <td>-0.133080</td>\n",
       "      <td>-0.097943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152346</td>\n",
       "      <td>0.520244</td>\n",
       "      <td>0.115349</td>\n",
       "      <td>-0.764296</td>\n",
       "      <td>-0.396995</td>\n",
       "      <td>0.211961</td>\n",
       "      <td>-0.101186</td>\n",
       "      <td>-0.010535</td>\n",
       "      <td>-0.473510</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.249270</td>\n",
       "      <td>-0.321824</td>\n",
       "      <td>-0.150643</td>\n",
       "      <td>0.196687</td>\n",
       "      <td>0.099907</td>\n",
       "      <td>-0.182337</td>\n",
       "      <td>0.061410</td>\n",
       "      <td>0.333429</td>\n",
       "      <td>-0.049392</td>\n",
       "      <td>-0.111920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.365685</td>\n",
       "      <td>0.308413</td>\n",
       "      <td>0.044198</td>\n",
       "      <td>0.281915</td>\n",
       "      <td>-0.060768</td>\n",
       "      <td>-0.100559</td>\n",
       "      <td>-0.002525</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.008750</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.257959</td>\n",
       "      <td>-0.304113</td>\n",
       "      <td>-0.101192</td>\n",
       "      <td>0.044406</td>\n",
       "      <td>0.029702</td>\n",
       "      <td>-0.190166</td>\n",
       "      <td>-0.063139</td>\n",
       "      <td>0.299697</td>\n",
       "      <td>-0.002749</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093462</td>\n",
       "      <td>0.062238</td>\n",
       "      <td>-0.026726</td>\n",
       "      <td>0.160072</td>\n",
       "      <td>-0.009009</td>\n",
       "      <td>-0.020880</td>\n",
       "      <td>0.165518</td>\n",
       "      <td>0.068974</td>\n",
       "      <td>-0.101060</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.244061</td>\n",
       "      <td>-0.305087</td>\n",
       "      <td>-0.153881</td>\n",
       "      <td>0.129018</td>\n",
       "      <td>0.094431</td>\n",
       "      <td>-0.223138</td>\n",
       "      <td>-0.033561</td>\n",
       "      <td>0.256919</td>\n",
       "      <td>-0.078319</td>\n",
       "      <td>0.035862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.394759</td>\n",
       "      <td>0.157913</td>\n",
       "      <td>-0.189077</td>\n",
       "      <td>-0.278222</td>\n",
       "      <td>-0.030589</td>\n",
       "      <td>-0.071742</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>-0.039473</td>\n",
       "      <td>0.093794</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.275271</td>\n",
       "      <td>-0.203260</td>\n",
       "      <td>-0.036980</td>\n",
       "      <td>0.100723</td>\n",
       "      <td>0.005855</td>\n",
       "      <td>-0.305598</td>\n",
       "      <td>-0.088652</td>\n",
       "      <td>0.349468</td>\n",
       "      <td>-0.154561</td>\n",
       "      <td>-0.125210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419931</td>\n",
       "      <td>0.109951</td>\n",
       "      <td>0.123341</td>\n",
       "      <td>0.151534</td>\n",
       "      <td>-0.362632</td>\n",
       "      <td>-0.079346</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.250863</td>\n",
       "      <td>-0.694110</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.260391 -0.229405 -0.248368  0.227097  0.185063 -0.119205 -0.069965   \n",
       "1  0.249270 -0.321824 -0.150643  0.196687  0.099907 -0.182337  0.061410   \n",
       "2  0.257959 -0.304113 -0.101192  0.044406  0.029702 -0.190166 -0.063139   \n",
       "3  0.244061 -0.305087 -0.153881  0.129018  0.094431 -0.223138 -0.033561   \n",
       "4  0.275271 -0.203260 -0.036980  0.100723  0.005855 -0.305598 -0.088652   \n",
       "\n",
       "          8         9        10  ...        96        97        98        99  \\\n",
       "0  0.105068 -0.133080 -0.097943  ... -0.152346  0.520244  0.115349 -0.764296   \n",
       "1  0.333429 -0.049392 -0.111920  ... -0.365685  0.308413  0.044198  0.281915   \n",
       "2  0.299697 -0.002749  0.006757  ... -0.093462  0.062238 -0.026726  0.160072   \n",
       "3  0.256919 -0.078319  0.035862  ... -0.394759  0.157913 -0.189077 -0.278222   \n",
       "4  0.349468 -0.154561 -0.125210  ... -0.419931  0.109951  0.123341  0.151534   \n",
       "\n",
       "        100       101       102       103       104  Label  \n",
       "0 -0.396995  0.211961 -0.101186 -0.010535 -0.473510     68  \n",
       "1 -0.060768 -0.100559 -0.002525  0.002430  0.008750     68  \n",
       "2 -0.009009 -0.020880  0.165518  0.068974 -0.101060     68  \n",
       "3 -0.030589 -0.071742  0.004178 -0.039473  0.093794     68  \n",
       "4 -0.362632 -0.079346  0.002570  0.250863 -0.694110     68  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#newID = np.random.randint(0, 68, size=attackData.shape[0])\n",
    "#print(newID.shape)\n",
    "#attackData.drop(columns=['Label'])\n",
    "#attackData['Label'] = newID\n",
    "attackData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 105)\n",
      "(3600, 95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_23124\\1569631733.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  attackDataRP = pd.concat([attackDataRP, XRP], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Random project the auxiliary dataset\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94']\n",
    "\n",
    "attackDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(68,86):\n",
    "    rng = np.random.RandomState(seed-68)\n",
    "    X = attackData[attackData['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=94, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    attackDataRP = pd.concat([attackDataRP, XRP], ignore_index=True)\n",
    "    #print(auxilaryDataRP)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(attackData.shape)\n",
    "print(attackDataRP.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       68.0\n",
      "1       68.0\n",
      "2       68.0\n",
      "3       68.0\n",
      "4       68.0\n",
      "        ... \n",
      "3595    85.0\n",
      "3596    85.0\n",
      "3597    85.0\n",
      "3598    85.0\n",
      "3599    85.0\n",
      "Name: Label, Length: 3600, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#user id in auxilary data\n",
    "print(attackDataRP['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the traning data for training and testing the attacker's model\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Xdata=attackData.drop(columns=['Label'])\n",
    "XRPdata=attackDataRP.drop(columns=['Label'])\n",
    "\n",
    "\n",
    "Xtrain, Xval, XRPtrain, XRPval = train_test_split(Xdata, XRPdata, test_size=0.2, random_state=22)\n",
    "#Xtrain, Xval, XRPtrain, XRPval = train_test_split(Xtrain, XRPtrain, test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2880, 104)\n",
      "(2880, 94)\n",
      "(720, 104)\n",
      "(720, 94)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(XRPtrain.shape)\n",
    "#print(Xtest.shape)\n",
    "#print(XRPtest.shape)\n",
    "print(Xval.shape)\n",
    "print(XRPval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary package for a neural network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inlineimport keras\n",
    "from keras.layers import Dense, Dropout, Input,Activation,Dropout, Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import mnist\n",
    "#from tqdm import tqdm\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizers for neural network\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "def adam_optimizer():\n",
    "    return Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "def RMSprop_optimizer():\n",
    "    return RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">104</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,416</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m12,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m104\u001b[0m)            │        \u001b[38;5;34m13,416\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,360</span> (626.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m160,360\u001b[0m (626.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">158,824</span> (620.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m158,824\u001b[0m (620.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neural network architecture for training a regressor\n",
    "\n",
    "def create_Regressor(release=False,outDim=104):\n",
    "  classifier = Sequential()\n",
    "  classifier.add(Dense(128, input_dim=94))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "   \n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(256))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "\n",
    "  classifier.add(Dense(256))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "\n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(128))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "\n",
    "  #if release:\n",
    "  classifier.add(Dense(outDim, activation='sigmoid'))\n",
    "  #else:\n",
    "  #   classifier.add(Dense(Tuser))\n",
    "  #np.log_softmax_v2(a, axis=axis)\n",
    "  #classifier.add(F.softmax(a, dim=1))\n",
    "\n",
    "  classifier.compile(loss='mean_squared_error', optimizer='SGD',metrics=['mean_squared_error'])\n",
    "  return classifier\n",
    "\n",
    "Clasf=create_Regressor()\n",
    "Clasf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.1145 - mean_squared_error: 0.1145 - val_loss: 0.1219 - val_mean_squared_error: 0.1219\n",
      "Epoch 2/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0290 - mean_squared_error: 0.0290 - val_loss: 0.0689 - val_mean_squared_error: 0.0689\n",
      "Epoch 3/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0419 - val_mean_squared_error: 0.0419\n",
      "Epoch 4/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0296 - val_mean_squared_error: 0.0296\n",
      "Epoch 5/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0238 - val_mean_squared_error: 0.0238\n",
      "Epoch 6/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0210 - val_mean_squared_error: 0.0210\n",
      "Epoch 7/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
      "Epoch 8/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0186 - val_mean_squared_error: 0.0186\n",
      "Epoch 9/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0182 - val_mean_squared_error: 0.0182\n",
      "Epoch 10/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0179 - val_mean_squared_error: 0.0179\n",
      "Epoch 11/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
      "Epoch 12/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0175 - val_mean_squared_error: 0.0175\n",
      "Epoch 13/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
      "Epoch 14/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0173 - val_mean_squared_error: 0.0173\n",
      "Epoch 15/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0172 - val_mean_squared_error: 0.0172\n",
      "Epoch 16/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0171 - val_mean_squared_error: 0.0171\n",
      "Epoch 17/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0170 - val_mean_squared_error: 0.0170\n",
      "Epoch 18/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0173 - mean_squared_error: 0.0173 - val_loss: 0.0169 - val_mean_squared_error: 0.0169\n",
      "Epoch 19/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0169 - val_mean_squared_error: 0.0169\n",
      "Epoch 20/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0168 - val_mean_squared_error: 0.0168\n",
      "Epoch 21/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0168 - val_mean_squared_error: 0.0168\n",
      "Epoch 22/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
      "Epoch 23/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
      "Epoch 24/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0166 - val_mean_squared_error: 0.0166\n",
      "Epoch 25/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0166 - val_mean_squared_error: 0.0166\n",
      "Epoch 26/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0165 - val_mean_squared_error: 0.0165\n",
      "Epoch 27/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0165 - val_mean_squared_error: 0.0165\n",
      "Epoch 28/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
      "Epoch 29/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
      "Epoch 30/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
      "Epoch 31/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
      "Epoch 32/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
      "Epoch 33/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
      "Epoch 34/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0161 - mean_squared_error: 0.0161 - val_loss: 0.0162 - val_mean_squared_error: 0.0162\n",
      "Epoch 35/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0162 - val_mean_squared_error: 0.0162\n",
      "Epoch 36/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0162 - val_mean_squared_error: 0.0162\n",
      "Epoch 37/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0161 - val_mean_squared_error: 0.0161\n",
      "Epoch 38/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0161 - val_mean_squared_error: 0.0161\n",
      "Epoch 39/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0161 - val_mean_squared_error: 0.0161\n",
      "Epoch 40/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 41/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0161 - mean_squared_error: 0.0161 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 42/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 43/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 44/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 45/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0161 - mean_squared_error: 0.0161 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 46/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 47/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0158 - mean_squared_error: 0.0158 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 48/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 49/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
      "Epoch 50/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0156 - mean_squared_error: 0.0156 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n"
     ]
    }
   ],
   "source": [
    "#Train the regressor  by auxilary dataset\n",
    "# Input: Projected data\n",
    "# Output: Plain data\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
    "callbacks_list = [learning_rate_reduction]\n",
    "\n",
    "Regressor= create_Regressor(True,104)\n",
    "\n",
    "#------Comment will start from here\n",
    "lossc='mean_squared_error'\n",
    "optimizerc=RMSprop(learning_rate=0.001, rho=0.9)\n",
    "Regressor.compile(loss=lossc, optimizer=optimizerc,metrics=['mean_squared_error'])\n",
    "#------Comments will end from here\n",
    "Rhistoryc2 =  Regressor.fit(XRPtrain, Xtrain, batch_size=64, epochs=50, validation_data=(XRPval, Xval),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model by pre-seperated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test data and seperate test data\n",
    "import csv\n",
    "import pandas as pd\n",
    "testattackdata=pd.read_csv('Dataset/VoiceDatatest.csv',index_col=0)\n",
    "testattackdata = testattackdata[testattackdata['Label'] >= 68]\n",
    "#testdataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8796</th>\n",
       "      <td>0.577301</td>\n",
       "      <td>-0.521846</td>\n",
       "      <td>-0.458939</td>\n",
       "      <td>-0.169801</td>\n",
       "      <td>0.492290</td>\n",
       "      <td>-0.608976</td>\n",
       "      <td>-0.075974</td>\n",
       "      <td>-0.057947</td>\n",
       "      <td>0.272287</td>\n",
       "      <td>-0.016334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048005</td>\n",
       "      <td>-0.225233</td>\n",
       "      <td>-0.009301</td>\n",
       "      <td>0.182901</td>\n",
       "      <td>-0.043218</td>\n",
       "      <td>-0.021152</td>\n",
       "      <td>0.009831</td>\n",
       "      <td>0.063303</td>\n",
       "      <td>-0.055561</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>0.327843</td>\n",
       "      <td>-0.168374</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>0.294810</td>\n",
       "      <td>-0.125812</td>\n",
       "      <td>-0.565006</td>\n",
       "      <td>0.021564</td>\n",
       "      <td>0.336860</td>\n",
       "      <td>-0.243190</td>\n",
       "      <td>-0.249655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293992</td>\n",
       "      <td>-0.315545</td>\n",
       "      <td>0.182196</td>\n",
       "      <td>0.142055</td>\n",
       "      <td>-0.040365</td>\n",
       "      <td>-0.009444</td>\n",
       "      <td>-0.190533</td>\n",
       "      <td>0.016862</td>\n",
       "      <td>-0.503840</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721</th>\n",
       "      <td>0.436850</td>\n",
       "      <td>0.242402</td>\n",
       "      <td>-0.545839</td>\n",
       "      <td>0.490844</td>\n",
       "      <td>-0.176363</td>\n",
       "      <td>0.218081</td>\n",
       "      <td>-0.479460</td>\n",
       "      <td>0.485923</td>\n",
       "      <td>-0.089936</td>\n",
       "      <td>0.129415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454873</td>\n",
       "      <td>-0.007616</td>\n",
       "      <td>-0.550065</td>\n",
       "      <td>0.344938</td>\n",
       "      <td>-0.151026</td>\n",
       "      <td>0.131478</td>\n",
       "      <td>0.022711</td>\n",
       "      <td>-0.051380</td>\n",
       "      <td>0.116940</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8875</th>\n",
       "      <td>0.824280</td>\n",
       "      <td>-0.238647</td>\n",
       "      <td>-0.423929</td>\n",
       "      <td>-0.034956</td>\n",
       "      <td>0.228715</td>\n",
       "      <td>-0.374185</td>\n",
       "      <td>-0.142289</td>\n",
       "      <td>0.114626</td>\n",
       "      <td>0.166814</td>\n",
       "      <td>0.080253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069756</td>\n",
       "      <td>-0.344372</td>\n",
       "      <td>-0.107741</td>\n",
       "      <td>0.179268</td>\n",
       "      <td>0.074635</td>\n",
       "      <td>-0.068413</td>\n",
       "      <td>-0.087623</td>\n",
       "      <td>0.231988</td>\n",
       "      <td>-0.060065</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9294</th>\n",
       "      <td>0.537243</td>\n",
       "      <td>0.459554</td>\n",
       "      <td>0.087789</td>\n",
       "      <td>-0.018138</td>\n",
       "      <td>-0.026348</td>\n",
       "      <td>0.170799</td>\n",
       "      <td>-0.129531</td>\n",
       "      <td>-0.216600</td>\n",
       "      <td>-0.362393</td>\n",
       "      <td>0.169105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104851</td>\n",
       "      <td>-0.217039</td>\n",
       "      <td>0.072538</td>\n",
       "      <td>0.095208</td>\n",
       "      <td>0.076355</td>\n",
       "      <td>0.045516</td>\n",
       "      <td>-0.025971</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>-0.144510</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7  \\\n",
       "8796  0.577301 -0.521846 -0.458939 -0.169801  0.492290 -0.608976 -0.075974   \n",
       "8209  0.327843 -0.168374  0.086000  0.294810 -0.125812 -0.565006  0.021564   \n",
       "8721  0.436850  0.242402 -0.545839  0.490844 -0.176363  0.218081 -0.479460   \n",
       "8875  0.824280 -0.238647 -0.423929 -0.034956  0.228715 -0.374185 -0.142289   \n",
       "9294  0.537243  0.459554  0.087789 -0.018138 -0.026348  0.170799 -0.129531   \n",
       "\n",
       "             8         9        10  ...        96        97        98  \\\n",
       "8796 -0.057947  0.272287 -0.016334  ...  0.048005 -0.225233 -0.009301   \n",
       "8209  0.336860 -0.243190 -0.249655  ... -0.293992 -0.315545  0.182196   \n",
       "8721  0.485923 -0.089936  0.129415  ...  0.454873 -0.007616 -0.550065   \n",
       "8875  0.114626  0.166814  0.080253  ...  0.069756 -0.344372 -0.107741   \n",
       "9294 -0.216600 -0.362393  0.169105  ... -0.104851 -0.217039  0.072538   \n",
       "\n",
       "            99       100       101       102       103       104  Label  \n",
       "8796  0.182901 -0.043218 -0.021152  0.009831  0.063303 -0.055561     47  \n",
       "8209  0.142055 -0.040365 -0.009444 -0.190533  0.016862 -0.503840      4  \n",
       "8721  0.344938 -0.151026  0.131478  0.022711 -0.051380  0.116940     41  \n",
       "8875  0.179268  0.074635 -0.068413 -0.087623  0.231988 -0.060065     61  \n",
       "9294  0.095208  0.076355  0.045516 -0.025971  0.003157 -0.144510     53  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "newID = np.random.randint(0, 68, size=testattackdata.shape[0])\n",
    "print(newID.shape)\n",
    "testattackdata.drop(columns=['Label'])\n",
    "testattackdata['Label'] = newID\n",
    "testattackdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_23124\\592712411.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  testattackDataRP = pd.concat([testattackDataRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425, 105)\n",
      "(425, 95)\n"
     ]
    }
   ],
   "source": [
    "#Random project the auxiliary dataset\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94']\n",
    "\n",
    "testattackDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,68):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = testattackdata[testattackdata['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=94, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    testattackDataRP = pd.concat([testattackDataRP, XRP], ignore_index=True)\n",
    "    #print(auxilaryDataRP)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(testattackdata.shape)\n",
    "print(testattackDataRP.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testattackdata=testattackdata.drop(columns=['Label'])\n",
    "testattackDataRP=testattackDataRP.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0302 - mean_squared_error: 0.0302 \n",
      "Loss: 0.029590586200356483\n",
      "Accuracy: 0.029590586200356483\n"
     ]
    }
   ],
   "source": [
    "#Performance of the trained attacker regressor\n",
    "loss, accuracy = Regressor.evaluate(testattackDataRP, testattackdata)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let say attacker has the access of Random projected data of the original data profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_23124\\3761924160.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  trainingDataRP = pd.concat([trainingDataRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13600, 105)\n",
      "(13600, 95)\n"
     ]
    }
   ],
   "source": [
    "#Let say attacker has the access of RP data of original data\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94']\n",
    "\n",
    "trainingDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,68):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = trainingData[trainingData['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=94, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    trainingDataRP = pd.concat([trainingDataRP, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(trainingData.shape)\n",
    "print(trainingDataRP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.0\n",
      "1         0.0\n",
      "2         0.0\n",
      "3         0.0\n",
      "4         0.0\n",
      "         ... \n",
      "13595    67.0\n",
      "13596    67.0\n",
      "13597    67.0\n",
      "13598    67.0\n",
      "13599    67.0\n",
      "Name: Label, Length: 13600, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#user id in original projected data\n",
    "print(trainingDataRP['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "(13600, 104)\n"
     ]
    }
   ],
   "source": [
    "#Prediction of plain data by the attacker mdoel assuming that attacker has access of projected data\n",
    "tDataRP=trainingDataRP.drop(columns=['Label'])\n",
    "tDataReg= Regressor.predict(tDataRP)\n",
    "print(tDataReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#assume that along with project data attacker know the label of the data.\n",
    "# Add id with recovered data\n",
    "print(type(tDataReg))\n",
    "print(type(trainingDataRP['Label'].to_numpy()))\n",
    "traningdataReg = pd.concat([pd.DataFrame(tDataReg), trainingDataRP['Label'].to_frame()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13600, 105)\n"
     ]
    }
   ],
   "source": [
    "# recovered data by the attacker model from projected data\n",
    "print(traningdataReg.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test the qulity of recover data we did this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1         2         3         4         5         6         7  \\\n",
      "0      0.246940  0.004005  0.006768  0.183912  0.024739  0.018536  0.008899   \n",
      "1      0.325839  0.002908  0.002977  0.043523  0.027954  0.002909  0.007637   \n",
      "2      0.407672  0.002913  0.003905  0.043220  0.029759  0.002501  0.006330   \n",
      "3      0.336315  0.004888  0.008607  0.121507  0.011288  0.005487  0.007672   \n",
      "4      0.310229  0.012752  0.003248  0.185619  0.011403  0.014125  0.007777   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "13595  0.593484  0.043899  0.030528  0.024667  0.043915  0.021170  0.022344   \n",
      "13596  0.491393  0.067099  0.013868  0.058099  0.027974  0.032125  0.009262   \n",
      "13597  0.437260  0.034053  0.003350  0.104214  0.018291  0.058070  0.003644   \n",
      "13598  0.575406  0.040853  0.012749  0.033865  0.039871  0.018497  0.008627   \n",
      "13599  0.594873  0.084919  0.027127  0.017729  0.042168  0.016203  0.028543   \n",
      "\n",
      "              8         9        10  ...        96        97        98  \\\n",
      "0      0.124836  0.018567  0.020587  ...  0.010227  0.012136  0.019636   \n",
      "1      0.034052  0.022177  0.004657  ...  0.002655  0.005544  0.005957   \n",
      "2      0.044679  0.011293  0.004513  ...  0.002171  0.002885  0.006102   \n",
      "3      0.102650  0.006845  0.005080  ...  0.005666  0.003949  0.007251   \n",
      "4      0.176016  0.011799  0.003426  ...  0.009741  0.008294  0.006521   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "13595  0.110383  0.028806  0.034447  ...  0.010446  0.008719  0.027369   \n",
      "13596  0.117532  0.026880  0.018694  ...  0.013962  0.008877  0.022037   \n",
      "13597  0.083644  0.025599  0.014064  ...  0.012195  0.006350  0.018429   \n",
      "13598  0.068561  0.030955  0.016599  ...  0.006710  0.007460  0.021381   \n",
      "13599  0.083641  0.060014  0.050979  ...  0.008329  0.014939  0.028283   \n",
      "\n",
      "             99       100       101       102       103       104  Label  \n",
      "0      0.016245  0.011506  0.024315  0.019470  0.021143  0.013644    0.0  \n",
      "1      0.013635  0.009712  0.005053  0.009947  0.014156  0.010177    0.0  \n",
      "2      0.011169  0.010117  0.003742  0.008226  0.012428  0.010122    0.0  \n",
      "3      0.012848  0.005515  0.008765  0.011817  0.015565  0.014690    0.0  \n",
      "4      0.006766  0.007910  0.010288  0.011515  0.019593  0.017279    0.0  \n",
      "...         ...       ...       ...       ...       ...       ...    ...  \n",
      "13595  0.067652  0.043212  0.036094  0.060461  0.026656  0.116228   67.0  \n",
      "13596  0.021989  0.026751  0.034321  0.028793  0.018054  0.058236   67.0  \n",
      "13597  0.010363  0.008281  0.022242  0.014249  0.012833  0.057923   67.0  \n",
      "13598  0.037438  0.027260  0.036694  0.046770  0.014581  0.058913   67.0  \n",
      "13599  0.074757  0.054857  0.054453  0.071636  0.021897  0.143831   67.0  \n",
      "\n",
      "[13600 rows x 105 columns]\n"
     ]
    }
   ],
   "source": [
    "traningdataReg.columns=list(trainingData.columns)\n",
    "print(traningdataReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "allPvalue=np.zeros((86,104))\n",
    "for id in range(0,68):\n",
    "    dataset1=traningdataReg[traningdataReg['Label']==id]\n",
    "    dataset2=trainingData[trainingData['Label']==id]\n",
    "    for col in range (0,104):\n",
    "        sample1=dataset1.iloc[:,col]\n",
    "        sample2=dataset2.iloc[:,col]\n",
    "        statistics, allPvalue[id,col]=stats.kstest(sample1, sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.99042356e-105 1.53407213e-024 3.79202747e-022 ... 1.44688522e-023\n",
      "  4.83652567e-032 1.52432699e-053]\n",
      " [2.48538534e-030 1.77685640e-098 4.89490121e-025 ... 3.67726166e-040\n",
      "  8.93193867e-046 3.30042803e-042]\n",
      " [5.02225608e-035 9.80181265e-085 1.68858500e-083 ... 4.56922482e-066\n",
      "  1.53520048e-059 1.94056241e-060]\n",
      " ...\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n"
     ]
    }
   ],
   "source": [
    "print(allPvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "allPvalue = np.where(allPvalue < 0.05, 0, 1)\n",
    "#allPvalue[allPvalue < 0.05] = 0\n",
    "print(allPvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 3 5 5 4 5 6 1 3 3 8 2 5 3 5 6 5 4 5 5 3 5 3 4 4 1 0 0 4 8 0 4 0 6 1 0 3\n",
      " 1 1 2 1 2 4 7 2 1 2 3 3 1 0 2 2 3 0 3 0 3 5 3 2 3 2 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(allPvalue, axis=1))\n",
    "print(len(allPvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data=np.sum(allPvalue, axis=1)*100/104\n",
    "index=[i for i in range (1,len(data)+1)]\n",
    "plt.bar(index, data)\n",
    "plt.xlabel('user id')\n",
    "plt.ylabel('percent of features')\n",
    "#plt.title('Total features in a profile (out of 65 features) that passed the similarity test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97999, 66)\n",
      "(97999, 57)\n"
     ]
    }
   ],
   "source": [
    "#Random project the recover data to impersonate the user. Used different seed\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "#traningdataReg = pd.concat([tDataReg, trainingDataRP['ID']], axis=1)\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','ID']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "traningdataReg.columns=dataset.columns\n",
    "trainingDataRPReg = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,96):\n",
    "    rng = np.random.RandomState(seed+10)\n",
    "    X = traningdataReg[traningdataReg['ID']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['ID'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['ID']=seed\n",
    "    trainingDataRPReg = pd.concat([trainingDataRPReg, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(traningdataReg.shape)\n",
    "print(trainingDataRPReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in test dataset: 96\n"
     ]
    }
   ],
   "source": [
    "print(\"Total user in test dataset:\", len(pd.unique(trainingDataRPReg['ID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3063/3063 [==============================] - 4s 1ms/step - loss: 19.9066 - accuracy: 0.0000e+00\n",
      "Loss: 19.906648635864258\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Performance of the attacker by using the random projected recover data\n",
    "#UserModel.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "Xtest=trainingDataRPReg.drop(columns=['ID'])\n",
    "ytest=trainingDataRPReg['ID']\n",
    "ytest=to_categorical(ytest)\n",
    "loss, accuracy = TrainedClassifier.evaluate(Xtest,ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97999, 66)\n",
      "(97999, 57)\n"
     ]
    }
   ],
   "source": [
    "#Random project the recover data to impersonate the user. If the attacker know the key\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "#traningdataReg = pd.concat([tDataReg, trainingDataRP['ID']], axis=1)\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','ID']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "traningdataReg.columns=dataset.columns\n",
    "trainingDataRPReg = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,96):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = traningdataReg[traningdataReg['ID']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['ID'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['ID']=seed\n",
    "    trainingDataRPReg = pd.concat([trainingDataRPReg, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(traningdataReg.shape)\n",
    "print(trainingDataRPReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3063/3063 [==============================] - 4s 1ms/step - loss: 0.1201 - accuracy: 0.9698\n",
      "Loss: 0.12010253220796585\n",
      "Accuracy: 0.969836413860321\n"
     ]
    }
   ],
   "source": [
    "#Performance of the attacker by using the random projected recover data when key is known\n",
    "#UserModel.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "Xtest=trainingDataRPReg.drop(columns=['ID'])\n",
    "ytest=trainingDataRPReg['ID']\n",
    "ytest=to_categorical(ytest)\n",
    "loss, accuracy = TrainedClassifier.evaluate(Xtest,ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

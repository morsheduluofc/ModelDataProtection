{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDdeQb0Q7K7_"
   },
   "source": [
    "#Description of this program:\n",
    "## Read the oversampled data and seperate all profile in two groups: Traning profile and Auxiliary profiles\n",
    "## Prepare the training data to train an ML model\n",
    "## Validate the correctness and security of the trained model\n",
    "- Virtual environment and keras: https://www.tutorialspoint.com/keras/keras_installation.htm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "tFCVTFWV7K8U",
    "outputId": "adf4b84f-1f1c-43fd-8381-99b75aa58a0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.582305</td>\n",
       "      <td>-0.091624</td>\n",
       "      <td>-0.113317</td>\n",
       "      <td>0.069735</td>\n",
       "      <td>-0.056116</td>\n",
       "      <td>0.071154</td>\n",
       "      <td>-0.146473</td>\n",
       "      <td>0.049818</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.020655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137189</td>\n",
       "      <td>-0.202803</td>\n",
       "      <td>-0.061708</td>\n",
       "      <td>-0.239653</td>\n",
       "      <td>0.033237</td>\n",
       "      <td>0.097121</td>\n",
       "      <td>0.090061</td>\n",
       "      <td>-0.064359</td>\n",
       "      <td>-0.222110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.403713</td>\n",
       "      <td>-0.178570</td>\n",
       "      <td>0.066751</td>\n",
       "      <td>-0.055265</td>\n",
       "      <td>-0.010389</td>\n",
       "      <td>0.041391</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>0.018638</td>\n",
       "      <td>-0.047548</td>\n",
       "      <td>-0.012591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434401</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>-0.172818</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>0.126595</td>\n",
       "      <td>-0.084699</td>\n",
       "      <td>-0.008194</td>\n",
       "      <td>-0.019724</td>\n",
       "      <td>-0.256300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.400374</td>\n",
       "      <td>-0.185792</td>\n",
       "      <td>0.012088</td>\n",
       "      <td>-0.067337</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0.049996</td>\n",
       "      <td>0.012814</td>\n",
       "      <td>-0.025244</td>\n",
       "      <td>-0.030817</td>\n",
       "      <td>-0.068128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156473</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>-0.124143</td>\n",
       "      <td>-0.129319</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>-0.046710</td>\n",
       "      <td>-0.008062</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>-0.075796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.477862</td>\n",
       "      <td>-0.018475</td>\n",
       "      <td>0.071872</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>-0.016150</td>\n",
       "      <td>-0.035783</td>\n",
       "      <td>0.070162</td>\n",
       "      <td>-0.061627</td>\n",
       "      <td>-0.107956</td>\n",
       "      <td>-0.082812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127110</td>\n",
       "      <td>0.179423</td>\n",
       "      <td>-0.115302</td>\n",
       "      <td>0.104676</td>\n",
       "      <td>-0.107193</td>\n",
       "      <td>0.209552</td>\n",
       "      <td>0.027887</td>\n",
       "      <td>0.117224</td>\n",
       "      <td>0.104110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.465657</td>\n",
       "      <td>-0.029432</td>\n",
       "      <td>-0.029614</td>\n",
       "      <td>0.028301</td>\n",
       "      <td>0.067341</td>\n",
       "      <td>-0.131359</td>\n",
       "      <td>0.090756</td>\n",
       "      <td>-0.053325</td>\n",
       "      <td>-0.038439</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122469</td>\n",
       "      <td>-0.361710</td>\n",
       "      <td>-0.096674</td>\n",
       "      <td>0.032555</td>\n",
       "      <td>0.192928</td>\n",
       "      <td>-0.012668</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>-0.094648</td>\n",
       "      <td>0.170670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.582305 -0.091624 -0.113317  0.069735 -0.056116  0.071154 -0.146473   \n",
       "1  0.403713 -0.178570  0.066751 -0.055265 -0.010389  0.041391  0.013069   \n",
       "2  0.400374 -0.185792  0.012088 -0.067337  0.038272  0.049996  0.012814   \n",
       "3  0.477862 -0.018475  0.071872 -0.000463 -0.016150 -0.035783  0.070162   \n",
       "4  0.465657 -0.029432 -0.029614  0.028301  0.067341 -0.131359  0.090756   \n",
       "\n",
       "          8         9        10  ...        96        97        98        99  \\\n",
       "0  0.049818  0.002500  0.020655  ...  0.137189 -0.202803 -0.061708 -0.239653   \n",
       "1  0.018638 -0.047548 -0.012591  ...  0.434401  0.033208 -0.172818  0.005580   \n",
       "2 -0.025244 -0.030817 -0.068128  ...  0.156473  0.000591 -0.124143 -0.129319   \n",
       "3 -0.061627 -0.107956 -0.082812  ...  0.127110  0.179423 -0.115302  0.104676   \n",
       "4 -0.053325 -0.038439  0.009342  ...  0.122469 -0.361710 -0.096674  0.032555   \n",
       "\n",
       "        100       101       102       103       104  Label  \n",
       "0  0.033237  0.097121  0.090061 -0.064359 -0.222110      0  \n",
       "1  0.126595 -0.084699 -0.008194 -0.019724 -0.256300      0  \n",
       "2  0.004072 -0.046710 -0.008062  0.001104 -0.075796      0  \n",
       "3 -0.107193  0.209552  0.027887  0.117224  0.104110      0  \n",
       "4  0.192928 -0.012668  0.027907 -0.094648  0.170670      0  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read all data\n",
    "import csv\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv('Dataset\\OversampledVoiceData.csv',index_col=0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJ6zCcRoJBSj",
    "outputId": "8ae45c15-b8ca-49fe-c7b4-7a0f24fa7898"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17200, 105)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtcvPN1l7K8h",
    "outputId": "e1fc4568-6f11-496c-943b-e7e16b98fd1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0     200\n",
       "1     200\n",
       "2     200\n",
       "3     200\n",
       "4     200\n",
       "     ... \n",
       "81    200\n",
       "82    200\n",
       "83    200\n",
       "84    200\n",
       "85    200\n",
       "Name: Label, Length: 86, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace the user ID by class name and count the number of sample in each class\n",
    "#dataset['Label'] = pd.factorize(dataset['Label'])[0]\n",
    "dataset.groupby(['Label'])['Label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffvTQtrH7K8o",
    "outputId": "dbacdf2e-49bb-41ea-efa5-a5ef8970ad9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in training dataset: 68\n",
      "Total user in auxiliary dataset: 18\n"
     ]
    }
   ],
   "source": [
    "#seperate the profile in two groups: (i) Training profile (0-95), and (ii) auxiliary profile (96-193)\n",
    "totalUser= len(pd.unique(dataset['Label']))\n",
    "trainingData = dataset[dataset['Label'] < 68]\n",
    "auxilaryData = dataset[dataset['Label'] >= 68]\n",
    "print(\"Total user in training dataset:\", len(pd.unique(trainingData['Label'])))\n",
    "print(\"Total user in auxiliary dataset:\", len(pd.unique(auxilaryData['Label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Z1iZ-P0c7K8s"
   },
   "outputs": [],
   "source": [
    "#Prepare the traning data for training and testing the model\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X=trainingData.drop(columns=['Label'])\n",
    "y=trainingData['Label']\n",
    "\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size=0.2, random_state=22)\n",
    "#Xtrain, Xval, ytrain, yval = train_test_split(Xtrain, ytrain, test_size=0.2, random_state=22)\n",
    "\n",
    "ytrain = to_categorical(ytrain)\n",
    "yval = to_categorical(yval)\n",
    "#ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RR_H8-nz7K8v",
    "outputId": "1ddf20dc-8ee2-4382-e755-3c9c56f38944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10880, 104)\n",
      "(10880, 68)\n",
      "(2720, 104)\n",
      "(2720, 68)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(Xval.shape)\n",
    "print(yval.shape)\n",
    "#print(Xtest.shape)\n",
    "#print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "mFKXheCd7K9r"
   },
   "outputs": [],
   "source": [
    "# import all necessary package for a neural network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inlineimport keras\n",
    "from keras.layers import Dense, Dropout, Input,Activation,Dropout, Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import mnist\n",
    "#from tqdm import tqdm\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vQeWRxpf7K93"
   },
   "outputs": [],
   "source": [
    "#define optimizers for neural network\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "def adam_optimizer():\n",
    "    return Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "def RMSprop_optimizer():\n",
    "    return RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpyiok1O7K96",
    "outputId": "e2169dd7-8dcb-431f-8e12-24961efa646e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,440</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,772</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m13,440\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_8 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_9 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m)             │         \u001b[38;5;34m8,772\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">356,164</span> (1.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m356,164\u001b[0m (1.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">353,604</span> (1.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m353,604\u001b[0m (1.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> (10.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,560\u001b[0m (10.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neural network architecture for model training\n",
    "\n",
    "def create_classifier(release=False,totalClass=68):\n",
    "  classifier = Sequential()\n",
    "  classifier.add(Dense(128, input_dim=104))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.1))\n",
    "\n",
    "  classifier.add(Dense(256))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.1))\n",
    "  \n",
    "  classifier.add(Dense(512))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.1))\n",
    "\n",
    "  classifier.add(Dense(256))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.1))\n",
    "\n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(128))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  classifier.add(Dropout(0.1))\n",
    "\n",
    "  #if release:\n",
    "  classifier.add(Dense(totalClass, activation='softmax'))\n",
    "  #else:\n",
    "  #   classifier.add(Dense(Tuser))\n",
    "  #np.log_softmax_v2(a, axis=axis)\n",
    "  #classifier.add(F.softmax(a, dim=1))\n",
    "\n",
    "  classifier.compile(loss='categorical_crossentropy', optimizer=RMSprop_optimizer(),metrics=['accuracy'])\n",
    "  return classifier\n",
    "\n",
    "Clasf=create_classifier()\n",
    "Clasf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eg9_dAIv7K-D",
    "outputId": "9991a9fd-6fd2-41ea-deca-b83bbdc0bc7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.3953 - loss: 2.7847 - val_accuracy: 0.4401 - val_loss: 2.5475 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8167 - loss: 0.7593 - val_accuracy: 0.8540 - val_loss: 0.5572 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8744 - loss: 0.4456 - val_accuracy: 0.8996 - val_loss: 0.3106 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9080 - loss: 0.3123 - val_accuracy: 0.9165 - val_loss: 0.2870 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9229 - loss: 0.2546 - val_accuracy: 0.9217 - val_loss: 0.2434 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9290 - loss: 0.2139 - val_accuracy: 0.9364 - val_loss: 0.2065 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9394 - loss: 0.1847 - val_accuracy: 0.9331 - val_loss: 0.2006 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9430 - loss: 0.1686 - val_accuracy: 0.9346 - val_loss: 0.2199 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9418 - loss: 0.1670 - val_accuracy: 0.9382 - val_loss: 0.2068 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9516 - loss: 0.1460 - val_accuracy: 0.9449 - val_loss: 0.1776 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9524 - loss: 0.1416 - val_accuracy: 0.9379 - val_loss: 0.1994 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9505 - loss: 0.1390 - val_accuracy: 0.9338 - val_loss: 0.2265 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9569 - loss: 0.1172 - val_accuracy: 0.9430 - val_loss: 0.1775 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9580 - loss: 0.1195 - val_accuracy: 0.9419 - val_loss: 0.1758 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9572 - loss: 0.1095 - val_accuracy: 0.9518 - val_loss: 0.1534 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9601 - loss: 0.1089 - val_accuracy: 0.9452 - val_loss: 0.1685 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9651 - loss: 0.0997 - val_accuracy: 0.9474 - val_loss: 0.1502 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9642 - loss: 0.0965 - val_accuracy: 0.9408 - val_loss: 0.2233 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9707 - loss: 0.0778 - val_accuracy: 0.9496 - val_loss: 0.1756 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m165/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.9662 - loss: 0.0866\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9662 - loss: 0.0869 - val_accuracy: 0.9449 - val_loss: 0.1913 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9689 - loss: 0.0818 - val_accuracy: 0.9555 - val_loss: 0.1395 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9758 - loss: 0.0679 - val_accuracy: 0.9544 - val_loss: 0.1461 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9762 - loss: 0.0556 - val_accuracy: 0.9551 - val_loss: 0.1458 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9779 - loss: 0.0576 - val_accuracy: 0.9518 - val_loss: 0.1497 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9802 - loss: 0.0521 - val_accuracy: 0.9540 - val_loss: 0.1428 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m166/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9794 - loss: 0.0514\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9794 - loss: 0.0515 - val_accuracy: 0.9548 - val_loss: 0.1388 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9810 - loss: 0.0502 - val_accuracy: 0.9581 - val_loss: 0.1327 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9778 - loss: 0.0467 - val_accuracy: 0.9551 - val_loss: 0.1422 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9822 - loss: 0.0423 - val_accuracy: 0.9544 - val_loss: 0.1407 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9802 - loss: 0.0506 - val_accuracy: 0.9526 - val_loss: 0.1438 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9816 - loss: 0.0417 - val_accuracy: 0.9618 - val_loss: 0.1309 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9833 - loss: 0.0426 - val_accuracy: 0.9577 - val_loss: 0.1324 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9812 - loss: 0.0399 - val_accuracy: 0.9544 - val_loss: 0.1356 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9841 - loss: 0.0392 - val_accuracy: 0.9548 - val_loss: 0.1363 - learning_rate: 2.5000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9819 - loss: 0.0421 - val_accuracy: 0.9540 - val_loss: 0.1403 - learning_rate: 2.5000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m168/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9833 - loss: 0.0384\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9832 - loss: 0.0384 - val_accuracy: 0.9540 - val_loss: 0.1394 - learning_rate: 2.5000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9830 - loss: 0.0407 - val_accuracy: 0.9544 - val_loss: 0.1400 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9839 - loss: 0.0336 - val_accuracy: 0.9548 - val_loss: 0.1460 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9821 - loss: 0.0376 - val_accuracy: 0.9563 - val_loss: 0.1463 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9857 - loss: 0.0399 - val_accuracy: 0.9585 - val_loss: 0.1391 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m165/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9842 - loss: 0.0360\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9842 - loss: 0.0361 - val_accuracy: 0.9555 - val_loss: 0.1419 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9848 - loss: 0.0372 - val_accuracy: 0.9563 - val_loss: 0.1413 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9845 - loss: 0.0325 - val_accuracy: 0.9544 - val_loss: 0.1394 - learning_rate: 1.0000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9834 - loss: 0.0374 - val_accuracy: 0.9563 - val_loss: 0.1382 - learning_rate: 1.0000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9839 - loss: 0.0359 - val_accuracy: 0.9581 - val_loss: 0.1358 - learning_rate: 1.0000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9843 - loss: 0.0357 - val_accuracy: 0.9585 - val_loss: 0.1333 - learning_rate: 1.0000e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9829 - loss: 0.0356 - val_accuracy: 0.9566 - val_loss: 0.1372 - learning_rate: 1.0000e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9822 - loss: 0.0370 - val_accuracy: 0.9577 - val_loss: 0.1349 - learning_rate: 1.0000e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9863 - loss: 0.0338 - val_accuracy: 0.9570 - val_loss: 0.1375 - learning_rate: 1.0000e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m170/170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9863 - loss: 0.0312 - val_accuracy: 0.9551 - val_loss: 0.1420 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "#Train the classifier seperately for black-box attack\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
    "callbacks_list = [learning_rate_reduction]\n",
    "\n",
    "Classfier2= create_classifier(True,68)\n",
    "\n",
    "#------Comment will start from here\n",
    "lossc='categorical_crossentropy'\n",
    "optimizerc=RMSprop(learning_rate=0.001, rho=0.9)\n",
    "Classfier2.compile(loss=lossc, optimizer=optimizerc,metrics=['accuracy'])\n",
    "#------Comments will end from here\n",
    "historyc2 =  Classfier2.fit(Xtrain, ytrain, batch_size=64, epochs=50, validation_data=(Xval, yval),verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "oLCDk2Rl7K-Z",
    "outputId": "90212053-1d85-4820-f746-1b20ad3fed6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epochs')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the classifier loss and accuracy curves for training and validation data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(historyc2.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "plt.plot(historyc2.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>0.488973</td>\n",
       "      <td>0.053560</td>\n",
       "      <td>-0.008640</td>\n",
       "      <td>0.021311</td>\n",
       "      <td>-0.076520</td>\n",
       "      <td>0.082283</td>\n",
       "      <td>-0.089348</td>\n",
       "      <td>-0.014124</td>\n",
       "      <td>0.014635</td>\n",
       "      <td>-0.021004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152212</td>\n",
       "      <td>0.013779</td>\n",
       "      <td>0.047660</td>\n",
       "      <td>0.190799</td>\n",
       "      <td>0.157012</td>\n",
       "      <td>-0.084286</td>\n",
       "      <td>0.045943</td>\n",
       "      <td>-0.012276</td>\n",
       "      <td>0.034138</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3984</th>\n",
       "      <td>0.363459</td>\n",
       "      <td>-0.105725</td>\n",
       "      <td>0.016582</td>\n",
       "      <td>-0.002718</td>\n",
       "      <td>0.005014</td>\n",
       "      <td>-0.003073</td>\n",
       "      <td>-0.032538</td>\n",
       "      <td>0.027676</td>\n",
       "      <td>-0.013223</td>\n",
       "      <td>-0.013705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007987</td>\n",
       "      <td>-0.051168</td>\n",
       "      <td>-0.052688</td>\n",
       "      <td>0.006131</td>\n",
       "      <td>0.070498</td>\n",
       "      <td>-0.058756</td>\n",
       "      <td>0.020532</td>\n",
       "      <td>-0.059942</td>\n",
       "      <td>-0.020443</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6137</th>\n",
       "      <td>0.355303</td>\n",
       "      <td>0.230379</td>\n",
       "      <td>-0.232042</td>\n",
       "      <td>0.092237</td>\n",
       "      <td>0.022894</td>\n",
       "      <td>0.401214</td>\n",
       "      <td>-0.126424</td>\n",
       "      <td>0.071484</td>\n",
       "      <td>0.062484</td>\n",
       "      <td>-0.035887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090618</td>\n",
       "      <td>-0.108350</td>\n",
       "      <td>-0.015494</td>\n",
       "      <td>0.239295</td>\n",
       "      <td>0.447787</td>\n",
       "      <td>0.249986</td>\n",
       "      <td>-0.023513</td>\n",
       "      <td>0.179470</td>\n",
       "      <td>0.031638</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6479</th>\n",
       "      <td>0.446764</td>\n",
       "      <td>-0.188594</td>\n",
       "      <td>0.313923</td>\n",
       "      <td>0.144440</td>\n",
       "      <td>-0.090597</td>\n",
       "      <td>0.162816</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>-0.075382</td>\n",
       "      <td>-0.188972</td>\n",
       "      <td>0.076771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109222</td>\n",
       "      <td>-0.155602</td>\n",
       "      <td>0.052856</td>\n",
       "      <td>-0.093521</td>\n",
       "      <td>0.081228</td>\n",
       "      <td>-0.037728</td>\n",
       "      <td>-0.029078</td>\n",
       "      <td>-0.004140</td>\n",
       "      <td>0.398180</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.493266</td>\n",
       "      <td>0.101619</td>\n",
       "      <td>0.160273</td>\n",
       "      <td>-0.031643</td>\n",
       "      <td>0.028593</td>\n",
       "      <td>-0.140356</td>\n",
       "      <td>-0.192247</td>\n",
       "      <td>-0.090680</td>\n",
       "      <td>-0.190940</td>\n",
       "      <td>0.012892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>-0.229604</td>\n",
       "      <td>-0.344814</td>\n",
       "      <td>-0.420924</td>\n",
       "      <td>-0.257385</td>\n",
       "      <td>-0.309532</td>\n",
       "      <td>-0.147215</td>\n",
       "      <td>-0.063143</td>\n",
       "      <td>0.205770</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7  \\\n",
       "2595  0.488973  0.053560 -0.008640  0.021311 -0.076520  0.082283 -0.089348   \n",
       "3984  0.363459 -0.105725  0.016582 -0.002718  0.005014 -0.003073 -0.032538   \n",
       "6137  0.355303  0.230379 -0.232042  0.092237  0.022894  0.401214 -0.126424   \n",
       "6479  0.446764 -0.188594  0.313923  0.144440 -0.090597  0.162816  0.116800   \n",
       "39    0.493266  0.101619  0.160273 -0.031643  0.028593 -0.140356 -0.192247   \n",
       "\n",
       "             8         9        10  ...        96        97        98  \\\n",
       "2595 -0.014124  0.014635 -0.021004  ...  0.152212  0.013779  0.047660   \n",
       "3984  0.027676 -0.013223 -0.013705  ... -0.007987 -0.051168 -0.052688   \n",
       "6137  0.071484  0.062484 -0.035887  ...  0.090618 -0.108350 -0.015494   \n",
       "6479 -0.075382 -0.188972  0.076771  ... -0.109222 -0.155602  0.052856   \n",
       "39   -0.090680 -0.190940  0.012892  ...  0.035562 -0.229604 -0.344814   \n",
       "\n",
       "            99       100       101       102       103       104  Label  \n",
       "2595  0.190799  0.157012 -0.084286  0.045943 -0.012276  0.034138     21  \n",
       "3984  0.006131  0.070498 -0.058756  0.020532 -0.059942 -0.020443     33  \n",
       "6137  0.239295  0.447787  0.249986 -0.023513  0.179470  0.031638     51  \n",
       "6479 -0.093521  0.081228 -0.037728 -0.029078 -0.004140  0.398180     53  \n",
       "39   -0.420924 -0.257385 -0.309532 -0.147215 -0.063143  0.205770      0  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read test data to validate the model accuracy\n",
    "import csv\n",
    "import pandas as pd\n",
    "testdataset=pd.read_csv('Dataset/VoiceDatatest.csv',index_col=0)\n",
    "testdataset = testdataset[testdataset['Label'] < 68]\n",
    "Xtest=testdataset.drop(columns=['Label'])\n",
    "ytest=testdataset['Label']\n",
    "ytest = to_categorical(ytest)\n",
    "\n",
    "testdataset.head()\n",
    "#Xtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60OnWGGQ7K-b",
    "outputId": "dc6114d8-1f74-4452-ebf5-f9df6b4c1ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9758 - loss: 0.0625\n",
      "Loss: 0.05037437006831169\n",
      "Accuracy: 0.9798657894134521\n"
     ]
    }
   ],
   "source": [
    "#Performance of the classifier\n",
    "Classfier2.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "loss, accuracy = Classfier2.evaluate(Xtest, ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#read test data to validate the model security\n",
    "import csv\n",
    "import pandas as pd\n",
    "testdataset=pd.read_csv('Dataset/VoiceDatatest.csv',index_col=0)\n",
    "testdataset = testdataset[testdataset['Label'] >= 68]\n",
    "Xtest=testdataset.drop(columns=['Label'])\n",
    "ytest = np.random.randint(0, 68, size=Xtest.shape[0])\n",
    "ytest = pd.DataFrame(ytest, columns=['random_numbers'])\n",
    "#ytest=testdataset['Label']\n",
    "print(type(ytest))\n",
    "ytest = to_categorical(ytest)\n",
    "\n",
    "#testdataset.head()\n",
    "#Xtest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 4.7398e-04 - loss: 14.1405  \n",
      "Loss: 14.28099536895752\n",
      "Accuracy: 0.002352941082790494\n"
     ]
    }
   ],
   "source": [
    "#Performance of the classifier for valid test data\n",
    "Classfier2.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "loss, accuracy = Classfier2.evaluate(Xtest, ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

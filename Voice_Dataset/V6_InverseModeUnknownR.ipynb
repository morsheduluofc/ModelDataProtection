{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Info: Estimate the verification accuracy of DAC for project data\n",
    "- Used DAC(RP projected) data to train an NN model\n",
    "- There are 193 different user's profiles and each profiles has 1000 data samples (normalized data)\n",
    "- Devide all profiles in two groups: training  profile (96) and auxilary profiles (96) \n",
    "- Each auxilary data semple has 65 different features and RP prjection moved them to 56 features\n",
    "- Random matrix of RP follow following distributions: Pr(x=+1)= 1/2s; Pr(x=-1)= 1/2s, Pr(x=0)= 1-1/s where s=3\n",
    "- The value of dimension reduction k is calculated by k= [(4+2\\beta)/(\\epsolon^2/2+\\epsolon^3/2)]log (n) where n is total sample in a profile and \\epsolon,\\beta>0\n",
    "- Construct a NN regressor has 4 dense layers along with 'BatchNormalization' and 'relu' activation funcation\n",
    "- Last layer is sigmoid function. Input dimension of model is 65 and output dimension 56.\n",
    "- Trained regressor to recover the plain data from the projected data for the 96 auxilary data classes\n",
    "- This traind regressor will be used to recove the training data of classifer.\n",
    "- Let say attacker has the access of RP data of original data and their corresponding label. Attacker can find it by model inversion attack\n",
    "\n",
    "- Included a summary of the NN architecture\n",
    "- Need shallow as RP make users profile more distinct\n",
    "- For 10 rounds of training training accurach reached to 100.0% and validation accuracy reached to 100.0%\n",
    "- Included a graph that shows change of training and validation acccruacy in different ephocs\n",
    "- Test accruacy 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.582305</td>\n",
       "      <td>-0.091624</td>\n",
       "      <td>-0.113317</td>\n",
       "      <td>0.069735</td>\n",
       "      <td>-0.056116</td>\n",
       "      <td>0.071154</td>\n",
       "      <td>-0.146473</td>\n",
       "      <td>0.049818</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.020655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137189</td>\n",
       "      <td>-0.202803</td>\n",
       "      <td>-0.061708</td>\n",
       "      <td>-0.239653</td>\n",
       "      <td>0.033237</td>\n",
       "      <td>0.097121</td>\n",
       "      <td>0.090061</td>\n",
       "      <td>-0.064359</td>\n",
       "      <td>-0.222110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.403713</td>\n",
       "      <td>-0.178570</td>\n",
       "      <td>0.066751</td>\n",
       "      <td>-0.055265</td>\n",
       "      <td>-0.010389</td>\n",
       "      <td>0.041391</td>\n",
       "      <td>0.013069</td>\n",
       "      <td>0.018638</td>\n",
       "      <td>-0.047548</td>\n",
       "      <td>-0.012591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434401</td>\n",
       "      <td>0.033208</td>\n",
       "      <td>-0.172818</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>0.126595</td>\n",
       "      <td>-0.084699</td>\n",
       "      <td>-0.008194</td>\n",
       "      <td>-0.019724</td>\n",
       "      <td>-0.256300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.400374</td>\n",
       "      <td>-0.185792</td>\n",
       "      <td>0.012088</td>\n",
       "      <td>-0.067337</td>\n",
       "      <td>0.038272</td>\n",
       "      <td>0.049996</td>\n",
       "      <td>0.012814</td>\n",
       "      <td>-0.025244</td>\n",
       "      <td>-0.030817</td>\n",
       "      <td>-0.068128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156473</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>-0.124143</td>\n",
       "      <td>-0.129319</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>-0.046710</td>\n",
       "      <td>-0.008062</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>-0.075796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.477862</td>\n",
       "      <td>-0.018475</td>\n",
       "      <td>0.071872</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>-0.016150</td>\n",
       "      <td>-0.035783</td>\n",
       "      <td>0.070162</td>\n",
       "      <td>-0.061627</td>\n",
       "      <td>-0.107956</td>\n",
       "      <td>-0.082812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127110</td>\n",
       "      <td>0.179423</td>\n",
       "      <td>-0.115302</td>\n",
       "      <td>0.104676</td>\n",
       "      <td>-0.107193</td>\n",
       "      <td>0.209552</td>\n",
       "      <td>0.027887</td>\n",
       "      <td>0.117224</td>\n",
       "      <td>0.104110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.465657</td>\n",
       "      <td>-0.029432</td>\n",
       "      <td>-0.029614</td>\n",
       "      <td>0.028301</td>\n",
       "      <td>0.067341</td>\n",
       "      <td>-0.131359</td>\n",
       "      <td>0.090756</td>\n",
       "      <td>-0.053325</td>\n",
       "      <td>-0.038439</td>\n",
       "      <td>0.009342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122469</td>\n",
       "      <td>-0.361710</td>\n",
       "      <td>-0.096674</td>\n",
       "      <td>0.032555</td>\n",
       "      <td>0.192928</td>\n",
       "      <td>-0.012668</td>\n",
       "      <td>0.027907</td>\n",
       "      <td>-0.094648</td>\n",
       "      <td>0.170670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5         6         7  \\\n",
       "0  0.582305 -0.091624 -0.113317  0.069735 -0.056116  0.071154 -0.146473   \n",
       "1  0.403713 -0.178570  0.066751 -0.055265 -0.010389  0.041391  0.013069   \n",
       "2  0.400374 -0.185792  0.012088 -0.067337  0.038272  0.049996  0.012814   \n",
       "3  0.477862 -0.018475  0.071872 -0.000463 -0.016150 -0.035783  0.070162   \n",
       "4  0.465657 -0.029432 -0.029614  0.028301  0.067341 -0.131359  0.090756   \n",
       "\n",
       "          8         9        10  ...        96        97        98        99  \\\n",
       "0  0.049818  0.002500  0.020655  ...  0.137189 -0.202803 -0.061708 -0.239653   \n",
       "1  0.018638 -0.047548 -0.012591  ...  0.434401  0.033208 -0.172818  0.005580   \n",
       "2 -0.025244 -0.030817 -0.068128  ...  0.156473  0.000591 -0.124143 -0.129319   \n",
       "3 -0.061627 -0.107956 -0.082812  ...  0.127110  0.179423 -0.115302  0.104676   \n",
       "4 -0.053325 -0.038439  0.009342  ...  0.122469 -0.361710 -0.096674  0.032555   \n",
       "\n",
       "        100       101       102       103       104  Label  \n",
       "0  0.033237  0.097121  0.090061 -0.064359 -0.222110      0  \n",
       "1  0.126595 -0.084699 -0.008194 -0.019724 -0.256300      0  \n",
       "2  0.004072 -0.046710 -0.008062  0.001104 -0.075796      0  \n",
       "3 -0.107193  0.209552  0.027887  0.117224  0.104110      0  \n",
       "4  0.192928 -0.012668  0.027907 -0.094648  0.170670      0  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read all data [194 users' oversampled data]\n",
    "import csv\n",
    "import pandas as pd\n",
    "dataset=pd.read_csv('Dataset/OversampledVoiceData.csv',index_col=0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0     200\n",
       "1     200\n",
       "2     200\n",
       "3     200\n",
       "4     200\n",
       "     ... \n",
       "81    200\n",
       "82    200\n",
       "83    200\n",
       "84    200\n",
       "85    200\n",
       "Name: Label, Length: 86, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace the user ID by class name and count the number of sample in each class\n",
    "#dataset['Label'] = pd.factorize(dataset['Label'])[0]\n",
    "dataset.groupby(['Label'])['Label'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in training dataset: 68\n",
      "Total user in auxiliary dataset: 18\n"
     ]
    }
   ],
   "source": [
    "#seperate the profile in two groups: (i) Training profile (0-95), and (ii) auxiliary profile (96-193)\n",
    "totalUser= len(pd.unique(dataset['Label']))\n",
    "trainingData = dataset[dataset['Label'] < 68]\n",
    "attackData = dataset[dataset['Label'] >= 68]\n",
    "print(\"Total user in training dataset:\", len(pd.unique(trainingData['Label'])))\n",
    "print(\"Total user in auxiliary dataset:\", len(pd.unique(attackData['Label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1         0.952207\n",
      "2         1.000000\n",
      "3         0.766847\n",
      "4         0.747409\n",
      "5         1.000000\n",
      "           ...    \n",
      "101       0.745435\n",
      "102       0.526589\n",
      "103       0.586205\n",
      "104       2.246000\n",
      "Label    67.000000\n",
      "Length: 105, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#value range of training data\n",
    "print(trainingData.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#When attacker only knows the distribution of R, attacker will train the attack model by the reandom projected attack data that are train by random generated RP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 105)\n",
      "(3600, 95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_26180\\3236769198.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  attackDataRP = pd.concat([attackDataRP, XRP], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Random project the auxiliary dataset\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94']\n",
    "\n",
    "attackDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(68,86):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = attackData[attackData['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=94, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    attackDataRP = pd.concat([attackDataRP, XRP], ignore_index=True)\n",
    "    #print(auxilaryDataRP)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(attackData.shape)\n",
    "print(attackDataRP.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       68.0\n",
      "1       68.0\n",
      "2       68.0\n",
      "3       68.0\n",
      "4       68.0\n",
      "        ... \n",
      "3595    85.0\n",
      "3596    85.0\n",
      "3597    85.0\n",
      "3598    85.0\n",
      "3599    85.0\n",
      "Name: Label, Length: 3600, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#user id in auxilary data\n",
    "print(attackDataRP['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the traning data for training and testing the attacker's model\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Xdata=attackData.drop(columns=['Label'])\n",
    "XRPdata=attackDataRP.drop(columns=['Label'])\n",
    "\n",
    "\n",
    "Xtrain, Xval, XRPtrain, XRPval = train_test_split(Xdata, XRPdata, test_size=0.2, random_state=22)\n",
    "#Xtrain, Xval, XRPtrain, XRPval = train_test_split(Xtrain, XRPtrain, test_size=0.3, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2880, 104)\n",
      "(2880, 94)\n",
      "(720, 104)\n",
      "(720, 94)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)\n",
    "print(XRPtrain.shape)\n",
    "#print(Xtest.shape)\n",
    "#print(XRPtest.shape)\n",
    "print(Xval.shape)\n",
    "print(XRPval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary package for a neural network\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inlineimport keras\n",
    "from keras.layers import Dense, Dropout, Input,Activation,Dropout, Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import mnist\n",
    "#from tqdm import tqdm\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "#import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizers for neural network\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "def adam_optimizer():\n",
    "    return Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "def RMSprop_optimizer():\n",
    "    return RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_19          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">104</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">13,416</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m12,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_16          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_16 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_17          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_17 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_18          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_18 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_19          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_19 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m104\u001b[0m)            │        \u001b[38;5;34m13,416\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">160,360</span> (626.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m160,360\u001b[0m (626.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">158,824</span> (620.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m158,824\u001b[0m (620.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#neural network architecture for training a regressor\n",
    "\n",
    "def create_Regressor(release=False,outDim=104):\n",
    "  classifier = Sequential()\n",
    "  classifier.add(Dense(128, input_dim=94))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "\n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(256))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "\n",
    "  classifier.add(Dense(256))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "\n",
    "  #classifier.add(Dense(256))\n",
    "  #classifier.add(BatchNormalization())\n",
    "  #classifier.add(Activation('relu'))\n",
    "\n",
    "  classifier.add(Dense(128))\n",
    "  classifier.add(BatchNormalization())\n",
    "  classifier.add(Activation('relu'))\n",
    "  #classifier.add(Dropout(0.2))\n",
    "\n",
    "  #if release:\n",
    "  classifier.add(Dense(outDim, activation='sigmoid'))\n",
    "  #else:\n",
    "  #   classifier.add(Dense(Tuser))\n",
    "  #np.log_softmax_v2(a, axis=axis)\n",
    "  #classifier.add(F.softmax(a, dim=1))\n",
    "\n",
    "  classifier.compile(loss='mean_squared_error', optimizer='SGD',metrics=['mean_squared_error'])\n",
    "  return classifier\n",
    "\n",
    "Clasf=create_Regressor()\n",
    "Clasf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.1184 - mean_squared_error: 0.1184 - val_loss: 0.1200 - val_mean_squared_error: 0.1200\n",
      "Epoch 2/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0654 - val_mean_squared_error: 0.0654\n",
      "Epoch 3/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0226 - mean_squared_error: 0.0226 - val_loss: 0.0402 - val_mean_squared_error: 0.0402\n",
      "Epoch 4/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0213 - mean_squared_error: 0.0213 - val_loss: 0.0291 - val_mean_squared_error: 0.0291\n",
      "Epoch 5/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0236 - val_mean_squared_error: 0.0236\n",
      "Epoch 6/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0193 - mean_squared_error: 0.0193 - val_loss: 0.0209 - val_mean_squared_error: 0.0209\n",
      "Epoch 7/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0195 - val_mean_squared_error: 0.0195\n",
      "Epoch 8/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0187 - val_mean_squared_error: 0.0187\n",
      "Epoch 9/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0183 - val_mean_squared_error: 0.0183\n",
      "Epoch 10/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0180 - val_mean_squared_error: 0.0180\n",
      "Epoch 11/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
      "Epoch 12/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0176 - val_mean_squared_error: 0.0176\n",
      "Epoch 13/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0175 - val_mean_squared_error: 0.0175\n",
      "Epoch 14/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0174 - val_mean_squared_error: 0.0174\n",
      "Epoch 15/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0173 - val_mean_squared_error: 0.0173\n",
      "Epoch 16/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0176 - mean_squared_error: 0.0176 - val_loss: 0.0172 - val_mean_squared_error: 0.0172\n",
      "Epoch 17/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0171 - val_mean_squared_error: 0.0171\n",
      "Epoch 18/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0170 - val_mean_squared_error: 0.0170\n",
      "Epoch 19/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0172 - mean_squared_error: 0.0172 - val_loss: 0.0169 - val_mean_squared_error: 0.0169\n",
      "Epoch 20/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0169 - val_mean_squared_error: 0.0169\n",
      "Epoch 21/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0168 - val_mean_squared_error: 0.0168\n",
      "Epoch 22/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0168 - val_mean_squared_error: 0.0168\n",
      "Epoch 23/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
      "Epoch 24/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
      "Epoch 25/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0166 - val_mean_squared_error: 0.0166\n",
      "Epoch 26/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0166 - val_mean_squared_error: 0.0166\n",
      "Epoch 27/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0165 - val_mean_squared_error: 0.0165\n",
      "Epoch 28/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0165 - val_mean_squared_error: 0.0165\n",
      "Epoch 29/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
      "Epoch 30/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0167 - mean_squared_error: 0.0167 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
      "Epoch 31/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0164 - val_mean_squared_error: 0.0164\n",
      "Epoch 32/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
      "Epoch 33/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
      "Epoch 34/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0163 - val_mean_squared_error: 0.0163\n",
      "Epoch 35/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0162 - val_mean_squared_error: 0.0162\n",
      "Epoch 36/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0161 - mean_squared_error: 0.0161 - val_loss: 0.0162 - val_mean_squared_error: 0.0162\n",
      "Epoch 37/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0162 - val_mean_squared_error: 0.0162\n",
      "Epoch 38/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0166 - mean_squared_error: 0.0166 - val_loss: 0.0161 - val_mean_squared_error: 0.0161\n",
      "Epoch 39/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0161 - val_mean_squared_error: 0.0161\n",
      "Epoch 40/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0161 - val_mean_squared_error: 0.0161\n",
      "Epoch 41/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 42/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 43/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 44/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0160 - val_mean_squared_error: 0.0160\n",
      "Epoch 45/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 46/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 47/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0161 - mean_squared_error: 0.0161 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 48/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0159 - val_mean_squared_error: 0.0159\n",
      "Epoch 49/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
      "Epoch 50/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0159 - mean_squared_error: 0.0159 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n"
     ]
    }
   ],
   "source": [
    "#Train the regressor  by auxilary dataset\n",
    "# Input: Projected data\n",
    "# Output: Plain data\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, UpSampling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=5, verbose=1, factor=0.5,min_lr=0.0001)\n",
    "callbacks_list = [learning_rate_reduction]\n",
    "\n",
    "Regressor= create_Regressor(True,104)\n",
    "\n",
    "#------Comment will start from here\n",
    "lossc='mean_squared_error'\n",
    "optimizerc=RMSprop(learning_rate=0.001, rho=0.9)\n",
    "Regressor.compile(loss=lossc, optimizer=optimizerc,metrics=['mean_squared_error'])\n",
    "#------Comments will end from here\n",
    "Rhistoryc2 =  Regressor.fit(XRPtrain, Xtrain, batch_size=64, epochs=50, validation_data=(XRPval, Xval),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epochs')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the classifier loss and accuracy curves for training and validation data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(Rhistoryc2.history['loss'], color='b', label=\"Training loss\")\n",
    "plt.plot(Rhistoryc2.history['val_loss'], color='r',label=\"Validation loss\")\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model by pre-seperated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the test data and seperate test data\n",
    "import csv\n",
    "import pandas as pd\n",
    "testattackdata=pd.read_csv('Dataset/VoiceDatatest.csv',index_col=0)\n",
    "testattackdata = testattackdata[testattackdata['Label'] >= 68]\n",
    "#testdataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(425, 105)\n",
      "(425, 95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_26180\\780229104.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  testattackDataRP = pd.concat([testattackDataRP, XRP], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94']\n",
    "\n",
    "testattackDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(68,86):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = testattackdata[testattackdata['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=94, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    testattackDataRP = pd.concat([testattackDataRP, XRP], ignore_index=True)\n",
    "    #print(auxilaryDataRP)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(testattackdata.shape)\n",
    "print(testattackDataRP.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testattackdata=testattackdata.drop(columns=['Label'])\n",
    "testattackDataRP=testattackDataRP.drop(columns=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0315 - mean_squared_error: 0.0315 \n",
      "Loss: 0.032184287905693054\n",
      "Accuracy: 0.032184287905693054\n"
     ]
    }
   ],
   "source": [
    "#Performance of the trained attacker regressor\n",
    "loss, accuracy = Regressor.evaluate(testattackDataRP, testattackdata)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let say attacker has the access of Random projected data of the original data profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdmor\\AppData\\Local\\Temp\\ipykernel_26180\\598149980.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  trainingDataRP = pd.concat([trainingDataRP, XRP], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13600, 105)\n",
      "(13600, 95)\n"
     ]
    }
   ],
   "source": [
    "#RP of original trained data. Let say attacker has the access of RP data of original data\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94','Label']\n",
    "column2=column1=['RPF1', 'RPF2', 'RPF3', 'RPF4', 'RPF5', 'RPF6', 'RPF7', 'RPF8', 'RPF9', 'RPF10', 'RPF11', 'RPF12', 'RPF13', 'RPF14', 'RPF15', 'RPF16', 'RPF17', 'RPF18',\n",
    "         'RPF19', 'RPF20', 'RPF21', 'RPF22', 'RPF23', 'RPF24', 'RPF25', 'RPF26', 'RPF27', 'RPF28', 'RPF29', 'RPF30', 'RPF31', 'RPF32', 'RPF33', 'RPF34', 'RPF35',\n",
    "         'RPF36', 'RPF37', 'RPF38', 'RPF39', 'RPF40', 'RPF41', 'RPF42', 'RPF43', 'RPF44', 'RPF45', 'RPF46', 'RPF47', 'RPF48', 'RPF49', 'RPF50', 'RPF51', 'RPF52',\n",
    "         'RPF53', 'RPF54', 'RPF55', 'RPF56', 'RPF57', 'RPF58', 'RPF59', 'RPF60', 'RPF61', 'RPF62', 'RPF63', 'RPF64', 'RPF65', 'RPF66', 'RPF67', 'RPF68', 'RPF69',\n",
    "         'RPF70', 'RPF71', 'RPF72', 'RPF73', 'RPF74', 'RPF75', 'RPF76', 'RPF77', 'RPF78', 'RPF79', 'RPF80', 'RPF81', 'RPF82', 'RPF83', 'RPF84', 'RPF85', 'RPF86',\n",
    "         'RPF87', 'RPF88', 'RPF89', 'RPF90', 'RPF91', 'RPF92', 'RPF93', 'RPF94']\n",
    "\n",
    "trainingDataRP = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,68):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = trainingData[trainingData['Label']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=94, random_state=rng)\n",
    "    Xdata=X.drop(columns=['Label'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['Label']=seed\n",
    "    trainingDataRP = pd.concat([trainingDataRP, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(trainingData.shape)\n",
    "print(trainingDataRP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.0\n",
      "1         0.0\n",
      "2         0.0\n",
      "3         0.0\n",
      "4         0.0\n",
      "         ... \n",
      "13595    67.0\n",
      "13596    67.0\n",
      "13597    67.0\n",
      "13598    67.0\n",
      "13599    67.0\n",
      "Name: Label, Length: 13600, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#user id in original projected data\n",
    "print(trainingDataRP['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m425/425\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "(13600, 104)\n"
     ]
    }
   ],
   "source": [
    "#Prediction of plain data by the attacker mdoel assuming that attacker has access of projected data\n",
    "tDataRP=trainingDataRP.drop(columns=['Label'])\n",
    "tDataReg= Regressor.predict(tDataRP)\n",
    "print(tDataReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#assume that along with project data attacker know the label of the data.\n",
    "# Add id with recovered data\n",
    "print(type(tDataReg))\n",
    "print(type(trainingDataRP['Label'].to_numpy()))\n",
    "traningdataReg = pd.concat([pd.DataFrame(tDataReg), trainingDataRP['Label'].to_frame()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13600, 105)\n"
     ]
    }
   ],
   "source": [
    "# recovered data by the attacker model from projected data\n",
    "print(traningdataReg.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To test the qulity of recover data we did this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1         2         3         4         5         6         7  \\\n",
      "0      0.483372  0.101583  0.044915  0.082492  0.074305  0.055991  0.031684   \n",
      "1      0.565776  0.009141  0.017549  0.023649  0.022665  0.043290  0.005006   \n",
      "2      0.593789  0.014229  0.029338  0.029109  0.025321  0.054296  0.006323   \n",
      "3      0.445346  0.014340  0.067281  0.022884  0.076482  0.034707  0.013667   \n",
      "4      0.463070  0.013551  0.073908  0.023874  0.089284  0.034584  0.030146   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "13595  0.592411  0.009256  0.053468  0.012672  0.047730  0.143501  0.012419   \n",
      "13596  0.580866  0.014491  0.041289  0.046630  0.030735  0.189527  0.009254   \n",
      "13597  0.639455  0.016970  0.027331  0.016332  0.055226  0.040757  0.006401   \n",
      "13598  0.561637  0.010145  0.045779  0.011068  0.041741  0.066896  0.009437   \n",
      "13599  0.636534  0.015082  0.028823  0.012838  0.060210  0.130224  0.009842   \n",
      "\n",
      "              8         9        10  ...        96        97        98  \\\n",
      "0      0.088634  0.061137  0.082434  ...  0.046693  0.032083  0.036050   \n",
      "1      0.015208  0.032501  0.010874  ...  0.010106  0.002517  0.010323   \n",
      "2      0.017158  0.029739  0.010905  ...  0.011398  0.002243  0.011407   \n",
      "3      0.017603  0.041102  0.016268  ...  0.012632  0.009595  0.018794   \n",
      "4      0.038582  0.056712  0.025371  ...  0.012298  0.014228  0.018993   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "13595  0.012973  0.026448  0.027990  ...  0.016852  0.030435  0.007518   \n",
      "13596  0.015448  0.015092  0.060312  ...  0.039089  0.017766  0.011473   \n",
      "13597  0.031191  0.025077  0.040059  ...  0.011797  0.009085  0.007611   \n",
      "13598  0.010882  0.023175  0.037652  ...  0.024607  0.014972  0.004613   \n",
      "13599  0.025748  0.020516  0.031390  ...  0.023518  0.018728  0.010169   \n",
      "\n",
      "             99       100       101       102       103       104  Label  \n",
      "0      0.044123  0.080709  0.078558  0.027785  0.037242  0.048782    0.0  \n",
      "1      0.005625  0.008865  0.011145  0.002747  0.017798  0.048965    0.0  \n",
      "2      0.006892  0.008864  0.015214  0.004302  0.017525  0.036363    0.0  \n",
      "3      0.008182  0.038040  0.017504  0.008387  0.019775  0.049221    0.0  \n",
      "4      0.014107  0.041551  0.024212  0.016901  0.018948  0.040606    0.0  \n",
      "...         ...       ...       ...       ...       ...       ...    ...  \n",
      "13595  0.010777  0.045724  0.019919  0.006143  0.042134  0.032963   67.0  \n",
      "13596  0.014877  0.042388  0.029858  0.008355  0.029296  0.041309   67.0  \n",
      "13597  0.018067  0.029062  0.023948  0.006630  0.024289  0.019973   67.0  \n",
      "13598  0.010690  0.056732  0.022736  0.006475  0.031778  0.018903   67.0  \n",
      "13599  0.013752  0.032321  0.034211  0.008631  0.042402  0.028388   67.0  \n",
      "\n",
      "[13600 rows x 105 columns]\n"
     ]
    }
   ],
   "source": [
    "traningdataReg.columns=list(trainingData.columns)\n",
    "print(traningdataReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "allPvalue=np.zeros((86,104))\n",
    "for id in range(0,68):\n",
    "    dataset1=traningdataReg[traningdataReg['Label']==id]\n",
    "    dataset2=trainingData[trainingData['Label']==id]\n",
    "    for col in range (0,104):\n",
    "        sample1=dataset1.iloc[:,col]\n",
    "        sample2=dataset2.iloc[:,col]\n",
    "        statistics, allPvalue[id,col]=stats.kstest(sample1, sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.05666947e-07 1.54069022e-25 1.44688522e-23 ... 3.79202747e-22\n",
      "  5.02225608e-35 6.56257820e-57]\n",
      " [2.94258341e-91 3.13309544e-21 3.24494723e-33 ... 7.77668630e-41\n",
      "  2.88228319e-36 2.88228319e-36]\n",
      " [1.53520048e-59 5.41679120e-86 1.68858500e-83 ... 4.58581398e-70\n",
      "  4.78321055e-67 3.36087488e-63]\n",
      " ...\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(allPvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "allPvalue = np.where(allPvalue < 0.05, 0, 1)\n",
    "#allPvalue[allPvalue < 0.05] = 0\n",
    "print(allPvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 1 3 1 1 5 1 2 6 2 1 4 1 1 2 4 3 3 2 3 2 2 1 1 4 0 4 5 7 0 6 4 2 4 2 3\n",
      " 4 1 2 1 3 4 5 3 3 2 0 1 4 2 3 0 3 0 3 2 4 4 2 3 5 7 1 0 3 1 5 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(allPvalue, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data=np.sum(allPvalue, axis=1)*100/104\n",
    "index=[i for i in range (1,len(data)+1)]\n",
    "plt.bar(index, data)\n",
    "plt.xlabel('user id')\n",
    "plt.ylabel('percent of features')\n",
    "#plt.title('Total features in a profile (out of 65 features) that passed the similarity test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97999, 66)\n",
      "(97999, 57)\n"
     ]
    }
   ],
   "source": [
    "#Random project the recover data to impersonate the user. Used different seed\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "#traningdataReg = pd.concat([tDataReg, trainingDataRP['ID']], axis=1)\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','ID']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "traningdataReg.columns=dataset.columns\n",
    "trainingDataRPReg = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,96):\n",
    "    rng = np.random.RandomState(seed+10)\n",
    "    X = traningdataReg[traningdataReg['ID']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['ID'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['ID']=seed\n",
    "    trainingDataRPReg = pd.concat([trainingDataRPReg, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(traningdataReg.shape)\n",
    "print(trainingDataRPReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total user in test dataset: 96\n"
     ]
    }
   ],
   "source": [
    "print(\"Total user in test dataset:\", len(pd.unique(trainingDataRPReg['ID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3063/3063 [==============================] - 4s 1ms/step - loss: 19.9066 - accuracy: 0.0000e+00\n",
      "Loss: 19.906648635864258\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Performance of the attacker by using the random projected recover data\n",
    "#UserModel.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "Xtest=trainingDataRPReg.drop(columns=['ID'])\n",
    "ytest=trainingDataRPReg['ID']\n",
    "ytest=to_categorical(ytest)\n",
    "loss, accuracy = TrainedClassifier.evaluate(Xtest,ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97999, 66)\n",
      "(97999, 57)\n"
     ]
    }
   ],
   "source": [
    "#Random project the recover data to impersonate the user. If the attacker know the key\n",
    "import numpy as np\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "#traningdataReg = pd.concat([tDataReg, trainingDataRP['ID']], axis=1)\n",
    "column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56','ID']\n",
    "column2=column1=['RPF1','RPF2','RPF3','RPF4','RPF5','RPF6','RPF7','RPF8','RPF9','RPF10','RPF11','RPF12','RPF13','RPF14','RPF15',\n",
    "         'RPF16','RPF17','RPF18','RPF19','RPF20','RPF21','RPF22','RPF23','RPF24','RPF25','RPF26','RPF27','RPF28','RPF29','RPF30',\n",
    "         'RPF31','RPF32','RPF33','RPF34','RPF35','RPF36','RPF37','RPF38','RPF39','RPF40','RPF41','RPF42','RPF43','RPF44','RPF45',\n",
    "         'RPF46','RPF47','RPF48','RPF49','RPF50','RPF51','RPF52','RPF53','RPF54','RPF55','RPF56']\n",
    "traningdataReg.columns=dataset.columns\n",
    "trainingDataRPReg = pd.DataFrame(columns=column1)\n",
    "for seed in range(0,96):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X = traningdataReg[traningdataReg['ID']==seed]\n",
    "    transformer = SparseRandomProjection(n_components=56, random_state=rng)\n",
    "    Xdata=X.drop(columns=['ID'])\n",
    "    XRP = pd.DataFrame(transformer.fit_transform(Xdata),columns=column2)\n",
    "    XRP['ID']=seed\n",
    "    trainingDataRPReg = pd.concat([trainingDataRPReg, XRP], ignore_index=True)\n",
    "    #print(\"Shape of Actual data:\",Xdata.shape)\n",
    "    #print(\"Shape of Randome Matrix:\", transformer.components_.shape[1],transformer.components_.shape[0])\n",
    "    #print(\"Shape of Projected data:\", X_new.shape)\n",
    "print(traningdataReg.shape)\n",
    "print(trainingDataRPReg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3063/3063 [==============================] - 4s 1ms/step - loss: 0.1201 - accuracy: 0.9698\n",
      "Loss: 0.12010253220796585\n",
      "Accuracy: 0.969836413860321\n"
     ]
    }
   ],
   "source": [
    "#Performance of the attacker by using the random projected recover data when key is known\n",
    "#UserModel.compile(loss='categorical_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
    "Xtest=trainingDataRPReg.drop(columns=['ID'])\n",
    "ytest=trainingDataRPReg['ID']\n",
    "ytest=to_categorical(ytest)\n",
    "loss, accuracy = TrainedClassifier.evaluate(Xtest,ytest)\n",
    "#print('Test score:', score)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
